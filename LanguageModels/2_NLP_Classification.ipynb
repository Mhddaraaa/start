{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d36de5-17a4-4a60-9080-4e3e167e1bea",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "<img align='center' width='600' src=\"https://images.gr-assets.com/misc/1535611813-1535611813_goodreads_misc.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0492c6a5-7e1f-49cf-bd73-4424ad51119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca2c16-291f-45ae-8aa9-0ec1d8fa39c3",
   "metadata": {},
   "source": [
    "## **work with kaggle**\n",
    "\n",
    "- First you have to go to your profile and creat API token which will download kaggle.json to your pc\n",
    "- Now drag and drop json file to your colab files tab\n",
    "- Run the codes below on **google colab**\n",
    "```\n",
    "    !mkdir /root/.kaggle\n",
    "    !mv kaggle.json /root/.kaggle/kaggle.json\n",
    "    !chmod 600 /root/.kaggle/kaggle.json\n",
    "    !kaggle datasets list\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- or if you are on your **local system** put save kaggle.json on desktop and run below command on jupyter notebook:\n",
    "\n",
    "```\n",
    "    !mkdir /Users/mhd/.kaggle/\n",
    "    !cp ~/Desktop/kaggle.json /Users/mhd/.kaggle/\n",
    "    !chmod 600 /Users/mhd/.kaggle/kaggle.json\n",
    "```\n",
    "<br>\n",
    "\n",
    "- To download the datasts to your colab open the desire competition(datasets) and click on `three dot` on the upper-right then `copy API command`\n",
    "\n",
    "-  for more information [click here](https://www.kaggle.com/discussions/general/74235)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706387e-7882-4416-aa8f-35738048c1e5",
   "metadata": {},
   "source": [
    "## **Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6d7cc-7b2c-483f-bf12-35853ab23257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r /root/.kaggle\n",
    "!mkdir /Users/mhd/.kaggle/\n",
    "!cp ~/Desktop/kaggle.json /Users/mhd/.kaggle/\n",
    "!chmod 600 /Users/mhd/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcbb1f-2495-4072-b21d-8c54f984236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d venky73/spam-mails-dataset\n",
    "!unzip spam-mails-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6022b08-9e1d-4777-873d-5ebde7a17e9a",
   "metadata": {},
   "source": [
    "### **Pandas library to read and edit csv files**\n",
    "\n",
    "Our dataset consists of a text column and a label column for \"ham\" or \"spam\". Since Machine Learning libraries use numeric data as input, If labels are in string format they need to be converted.\n",
    "\n",
    "The text column also includes raw text, which needs to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5603538-226b-4f4a-aee8-c61c9ece4fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_num\n",
       "605    ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
       "2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
       "3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
       "4685  spam  Subject: photoshop , windows , office . cheap ...          1\n",
       "2030   ham  Subject: re : indian springs\\r\\nthis deal is t...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('spam_ham_dataset.csv', index_col=0)\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b1645-ded3-4f56-9d2a-d5dffce0952e",
   "metadata": {},
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81aa60-0838-4b66-a91d-78f1d78b1cb6",
   "metadata": {},
   "source": [
    "#### **Handle labels column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b9c3fe-1cae-4e9d-93a0-344b5fcef153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "605   Subject: enron methanol ; meter # : 988291\\r\\n...   ham\n",
       "2349  Subject: hpl nom for january 9 , 2001\\r\\n( see...   ham\n",
       "3624  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   ham"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If label are just in string format use the code below to create new column for int type of that column\n",
    "# This is for demonstration purpose only \n",
    "df_new = spam_df[['text', 'label']]\n",
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c902b101-c02a-480d-8201-4722fc5f2a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label  label_num\n",
       "605   Subject: enron methanol ; meter # : 988291\\r\\n...   ham          0\n",
       "2349  Subject: hpl nom for january 9 , 2001\\r\\n( see...   ham          0\n",
       "3624  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   ham          0\n",
       "4685  Subject: photoshop , windows , office . cheap ...  spam          1\n",
       "2030  Subject: re : indian springs\\r\\nthis deal is t...   ham          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['label_num'] = (df_new['label'] == 'spam').astype(int)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a85e11-b92f-4fbb-9f9a-0d6b0a2da6be",
   "metadata": {},
   "source": [
    "#### **Handle text column**\n",
    "\n",
    "The text column has a lot of stop words and punctuation. First, we remove them. After that, it's time to reduce the words to their roots using lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee115eb-5b5a-4118-a0d3-153414b9b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  5171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv('spam_ham_dataset.csv')\n",
    "print('Number of rows: ', len(spam_df))\n",
    "spam_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82d2702-4393-44d5-ab46-41054fae9659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>text_lemma_without_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject enron methanol meter follow note give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject hpl nom january attached file hplnol x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject neon retreat ho ho ho wonderful time y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_num  \\\n",
       "0  Subject: enron methanol ; meter # : 988291\\r\\n...          0   \n",
       "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0   \n",
       "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0   \n",
       "\n",
       "                            text_lemma_without_stops  \n",
       "0  subject enron methanol meter follow note give ...  \n",
       "1  subject hpl nom january attached file hplnol x...  \n",
       "2  subject neon retreat ho ho ho wonderful time y...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to edit input text data\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_stops(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords, punctuation, and digits from the input text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "    str: The processed text with stopwords, punctuation, and digits removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize whitespace in the text\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    # Process the text using an NLP tool (assumed to be spaCy)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract lemmatized tokens that are not stopwords, punctuation, or digits\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_digit]\n",
    "\n",
    "    # Join the tokens into a single string and return\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# use small fraction of data to check the function\n",
    "df = spam_df.loc[:2, ('text', 'label_num')]\n",
    "df['text_lemma_without_stops'] = df['text'].apply(remove_stops)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5051be45-db82-4946-b597-fb6b45b72e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam_df['text_lemma_without_stops'] = spam_df['text'].apply(remove_stops)\n",
    "# spam_df.to_csv('spam_ham_dataset_lemma.csv')\n",
    "# spam_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75db5663-56ca-46d5-bcdd-493d490e5d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>text_lemma_without_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject enron methanol meter follow note give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject hpl nom january attached file hplnol x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject neon retreat ho ho ho wonderful time y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject indian spring deal book teco pvr reven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num                           text_lemma_without_stops  \n",
       "0          0  subject enron methanol meter follow note give ...  \n",
       "1          0  subject hpl nom january attached file hplnol x...  \n",
       "2          0  subject neon retreat ho ho ho wonderful time y...  \n",
       "3          1  subject photoshop windows office cheap main tr...  \n",
       "4          0  subject indian spring deal book teco pvr reven...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df = pd.read_csv(\"spam_ham_dataset_lemma.csv\", index_col=0)\n",
    "spam_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196f959-5bd0-4eb4-8e29-08f00199b7b0",
   "metadata": {},
   "source": [
    "#### **Bag Of Word(BOG)**\n",
    "\n",
    "The Bag-of-Words (BoW) model is a simple yet powerful technique used for text representation in Natural Language Processing (NLP) tasks. In this model, a document (or a piece of text) is represented as a bag (multiset) of words, disregarding grammar and word order but keeping multiplicity. The basic idea is to create a vocabulary of unique words from the entire corpus, and then represent each document as a vector where each dimension corresponds to a word in the vocabulary, and the value represents the frequency of that word in the document.\n",
    "\n",
    "**CountVectorizer** is a tool for converting a collection of text documents into a matrix of token counts. It essentially converts text data into numerical features that can be used for machine learning algorithms.\n",
    "\n",
    "- **Tokenization**: It breaks down each document into individual words or tokens. It can also handle n-grams, which are sequences of n tokens.\n",
    "- **Vocabulary Building**: It constructs a vocabulary of all unique tokens across the entire corpus of documents. Each unique token becomes a feature.\n",
    "- **Counting**: It counts the occurrences of each token in each document. This count becomes the value of the corresponding feature in the matrix.\n",
    "- **Sparse Matrix**: The output is typically a sparse matrix where each row represents a document, each column represents a token in the vocabulary, and each cell represents the count of the corresponding token in the document. Since most documents will only contain a small subset of all possible tokens, the matrix is sparse, meaning that most of its entries are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f0b18ff-6d27-4602-a7ab-6ef8f3912e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = spam_df['text']\n",
    "X_lemma = spam_df['text_lemma_without_stops']\n",
    "y = spam_df['label_num']\n",
    "\n",
    "transformer = CountVectorizer()\n",
    "X_cv = transformer.fit_transform(X)\n",
    "X_np = X_cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0cf2f7-911f-4c27-91ff-e1465141a62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1034, 50447), (4137, 50447), (1034,), (4137,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size=0.8, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c60d501d-a19a-43b7-a76f-0b58b9922aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ff3cbe5-66b0-41ab-a655-43647aa6cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 96.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "acc = accuracy_score(y_test, pred) * 100\n",
    "print(f'Model accuracy is {acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "111b5971-4019-4136-996f-d2a349a1f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      2963\n",
      "           1       0.99      0.90      0.94      1174\n",
      "\n",
      "    accuracy                           0.97      4137\n",
      "   macro avg       0.97      0.95      0.96      4137\n",
      "weighted avg       0.97      0.97      0.97      4137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4a00f6-e874-451f-801c-58b4a28154bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(model, feature_extraction, X, y):\n",
    "    \"\"\"\n",
    "    Train a text classification model using a pipeline consisting of a feature extraction method and a model for classification.\n",
    "\n",
    "    Args:\n",
    "    model (class): The classifier model to use for classification.\n",
    "    feature_extraction (class): The feature extraction method to use, typically CountVectorizer or TfidfVectorizer.\n",
    "    X (array-like): Input data containing text documents.\n",
    "    y (array-like): Target labels for the input data.\n",
    "\n",
    "    Returns:\n",
    "    str: Classification report showing precision, recall, F1-score, and support.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n",
    "\n",
    "    \n",
    "    # Define a pipeline consisting of a feature extraction method and model classifier\n",
    "    clf = Pipeline([\n",
    "        ('transformer', feature_extraction),\n",
    "        ('Model', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    # Generate a classification report\n",
    "    report = classification_report(y_test, pred)\n",
    "\n",
    "    # Print the name of the model used\n",
    "    print(f'''Your model is: \"{str(model).split('(')[0]}\"''', end='\\n\\n')\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238724a9-74de-448f-b7c6-b3249a84b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is: \"MultinomialNB\"\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2963\n",
      "           1       0.97      0.93      0.95      1174\n",
      "\n",
      "    accuracy                           0.97      4137\n",
      "   macro avg       0.97      0.96      0.96      4137\n",
      "weighted avg       0.97      0.97      0.97      4137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing with less line of code\n",
    "print(train_model(MultinomialNB(), CountVectorizer(), X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d1f5d6-c5b2-4d4d-a357-cd568204c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is: \"MultinomialNB\"\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      2963\n",
      "           1       0.95      0.94      0.94      1174\n",
      "\n",
      "    accuracy                           0.97      4137\n",
      "   macro avg       0.96      0.96      0.96      4137\n",
      "weighted avg       0.97      0.97      0.97      4137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing with lemmatized text\n",
    "print(train_model(MultinomialNB(), CountVectorizer(), X_lemma, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "197eb98c-5c17-4438-b9a7-ed5aea622bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is: \"MultinomialNB\"\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2963\n",
      "           1       0.97      0.92      0.94      1174\n",
      "\n",
      "    accuracy                           0.97      4137\n",
      "   macro avg       0.97      0.95      0.96      4137\n",
      "weighted avg       0.97      0.97      0.97      4137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing with lemmatized text with different n_gram range(1 and 2)\n",
    "print(train_model(MultinomialNB(), CountVectorizer(ngram_range=(1, 2)), X_lemma, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d3fe0-cb11-499c-92af-3b4929e89411",
   "metadata": {},
   "source": [
    "## **NLTK**\n",
    "\n",
    "The `movie_reviews` corpus in NLTK is a dataset commonly used for sentiment analysis and text classification tasks. It consists of reviews of movies categorized into *positive* and *negative* sentiments.\n",
    "\n",
    "The `movie_reviews` corpus is organized in the following way:\n",
    "\n",
    "1. **File Structure**:\n",
    "    - The corpus consists of two directories: pos and neg.\n",
    "    - The `pos` directory contains text files with positive movie reviews.\n",
    "    - The `neg` directory contains text files with negative movie reviews.\n",
    "2. **File Content**:\n",
    "    - Each text file corresponds to a single movie review.\n",
    "    - The content of each file is the text of the movie review.\n",
    "3. **Categorization:**\n",
    "    - The reviews are categorized based on sentiment.\n",
    "    - Positive reviews are stored in the pos directory.\n",
    "    - Negative reviews are stored in the neg directory.\n",
    "4. **Balanced Dataset**:\n",
    "    - The dataset is balanced, meaning that it contains an equal number of positive and negative reviews.\n",
    "    - Each directory (pos and neg) contains an equal number of text files.\n",
    "5. **Usage:**\n",
    "    - Researchers and developers often use this corpus for training and testing machine learning models for sentiment analysis or text classification tasks.\n",
    "    - It provides a standardized dataset for evaluating the performance of different algorithms and techniques in sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "499fe723-1121-4d32-b0d1-8e4e08df8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in movie review corpus:  1583820\n",
      "Categories:  ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Number of words in movie review corpus: \", len(mr.words()))\n",
    "print(\"Categories: \", mr.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699b3a8d-0ec0-4595-b04a-46a242a91a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words and remove 'not' from it\n",
    "stops = stopwords.words('english')\n",
    "stops.remove('not')\n",
    "\n",
    "# Extract each item review and class\n",
    "docs = [(file, category) for category in mr.categories() for file in mr.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f227471-08af-413c-ae17-4eea02f1bccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1583820, 714149)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "all_words = []\n",
    "lemma_words_without_stops = []\n",
    "\n",
    "# remove punctuations and stops words\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # just accept words\n",
    "for w in mr.words():\n",
    "    all_words.append(w.lower())\n",
    "    lemma_words_without_stops.append(lemmatizer.lemmatize(w.lower())) if (w not in stops) and (tokenizer.tokenize(w)) else None\n",
    "\n",
    "\n",
    "len(all_words), len(lemma_words_without_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25a8b46-cfe5-415a-87a4-19faf095a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_words_without_stops = nltk.FreqDist(lemma_words_without_stops)\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8371e09b-7d94-4dbc-8b67-0822629071d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ',' repitition in all_words:  77717\n",
      "number of ',' repitition in words_without_stops:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for existance of different words in list\n",
    "print(\"number of ',' repitition in all_words: \",all_words[','])\n",
    "print(\"number of ',' repitition in words_without_stops: \", lemma_words_without_stops[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eda82d06-2fa2-41bf-8eec-52c992e030c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use portion of all words as feature vectore\n",
    "feature_vector = list(lemma_words_without_stops)[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beae5d-7e34-44f8-a0da-5a1fdf4ee2f2",
   "metadata": {},
   "source": [
    "### **Create feature extraction function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9330eb1c-0872-4eae-93cb-02608ffeea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(words, feature_vector):\n",
    "    \"\"\"\n",
    "    Extract features from a list of words based on a feature vector.\n",
    "\n",
    "    Args:\n",
    "    words (list): List of words to extract features from.\n",
    "    feature_vector (list): Feature vector containing the features to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary indicating whether each feature in the feature vector is present in the words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert words to a set for faster membership checking\n",
    "    words = set(words)\n",
    "    \n",
    "    # Initialize an empty dictionary to store features\n",
    "    feature = {}\n",
    "\n",
    "    # Check if each feature in the feature vector is present in the words\n",
    "    for x in feature_vector:\n",
    "        feature[x] = x in words\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d713bf5-42e6-4626-81da-1d77bcbcec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film\n",
      "movie\n",
      "one\n",
      "not\n",
      "character\n",
      "like\n",
      "get\n",
      "make\n",
      "even\n",
      "good\n",
      "would\n",
      "also\n",
      "well\n",
      "life\n",
      "two\n",
      "see\n",
      "way\n",
      "go\n",
      "plot\n",
      "really\n",
      "little\n",
      "know\n",
      "people\n",
      "bad\n",
      "director\n",
      "new\n",
      "look\n",
      "find\n",
      "audience\n",
      "back\n",
      "give\n",
      "big\n",
      "world\n",
      "still\n",
      "want\n",
      "seems\n",
      "every\n",
      "part\n",
      "going\n",
      "point\n",
      "actually\n",
      "although\n",
      "ever\n",
      "since\n",
      "line\n",
      "problem\n",
      "away\n",
      "watch\n",
      "might\n",
      "start\n",
      "bit\n",
      "making\n",
      "american\n",
      "kind\n",
      "always\n",
      "seem\n",
      "trying\n",
      "sense\n",
      "half\n",
      "need\n",
      "idea\n",
      "pretty\n",
      "sure\n",
      "mind\n",
      "given\n",
      "horror\n",
      "attempt\n",
      "head\n",
      "music\n",
      "got\n",
      "ending\n",
      "10\n",
      "completely\n",
      "2\n",
      "different\n",
      "simply\n",
      "mean\n",
      "dead\n",
      "lost\n",
      "entire\n",
      "someone\n",
      "main\n",
      "review\n",
      "final\n",
      "playing\n",
      "despite\n",
      "video\n",
      "production\n",
      "running\n",
      "entertaining\n",
      "feeling\n",
      "throughout\n",
      "deal\n",
      "genre\n",
      "others\n",
      "five\n",
      "flick\n",
      "member\n",
      "coming\n",
      "break\n",
      "girlfriend\n",
      "guess\n",
      "obviously\n",
      "taken\n",
      "secret\n",
      "3\n",
      "happen\n",
      "oh\n",
      "giving\n",
      "studio\n",
      "seemed\n",
      "street\n",
      "chase\n",
      "apparently\n",
      "cool\n",
      "party\n",
      "ago\n",
      "teen\n",
      "us\n",
      "4\n",
      "strange\n",
      "came\n",
      "mess\n",
      "took\n",
      "decent\n",
      "overall\n",
      "drive\n",
      "showing\n",
      "biggest\n",
      "fantasy\n",
      "8\n",
      "concept\n",
      "beauty\n",
      "whatever\n",
      "somewhere\n",
      "hot\n",
      "witch\n",
      "generation\n",
      "okay\n",
      "write\n",
      "7\n",
      "accident\n",
      "edge\n",
      "normal\n",
      "sad\n",
      "decided\n",
      "generally\n",
      "rarely\n",
      "as\n",
      "weird\n",
      "20\n",
      "stick\n",
      "blair\n",
      "bottom\n",
      "confusing\n",
      "sitting\n",
      "continues\n",
      "slasher\n",
      "nightmare\n",
      "plain\n",
      "explanation\n",
      "turning\n",
      "engaging\n",
      "clue\n",
      "church\n",
      "9\n",
      "explained\n",
      "enter\n",
      "insight\n",
      "exact\n",
      "critique\n",
      "hide\n",
      "terribly\n",
      "understanding\n",
      "crow\n",
      "entertain\n",
      "wes\n",
      "skip\n",
      "executed\n",
      "thrilling\n",
      "drink\n",
      "offering\n",
      "personally\n",
      "chasing\n",
      "wrapped\n",
      "fed\n",
      "package\n",
      "lazy\n",
      "harder\n",
      "neat\n",
      "stir\n",
      "highway\n",
      "arrow\n",
      "joblo\n",
      "dig\n",
      "figured\n"
     ]
    }
   ],
   "source": [
    "# Check if the function works \n",
    "rev = feature_extract(mr.words(docs[0][0]), feature_vector)\n",
    "for k, v in rev.items():\n",
    "    if v is True:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e9906d2-b6b5-4e56-83ff-a707cdf5733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature extraction function on documents\n",
    "features = [(feature_extract(rev, feature_vector), category) for (rev, category) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08c3ee3-f875-4666-9721-98e4b4c88df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset as train and test sets\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Using Sklearn\n",
    "train_set, test_set = model_selection.train_test_split(features, test_size=0.2)\n",
    "\n",
    "# Costum\n",
    "# split = int(len(features) * 0.8)\n",
    "# train_set = features[:split]\n",
    "# test_set = features[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d18cf4e7-03d6-4410-a4c6-48eeb655aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use nltk Naive bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set) * 100\n",
    "print(f\"NaiveBayesClassifier accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56609e2a-ef71-46e5-8fcb-849a60255db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       1 = False             neg : pos    =      1.2 : 1.0\n",
      "                       2 = False             pos : neg    =      1.1 : 1.0\n",
      "                       7 = False             pos : neg    =      1.1 : 1.0\n",
      "                       7 = True              neg : pos    =      1.1 : 1.0\n",
      "                       2 = True              neg : pos    =      1.1 : 1.0\n",
      "                       1 = True              pos : neg    =      1.1 : 1.0\n",
      "                       5 = False             neg : pos    =      1.1 : 1.0\n",
      "                       5 = True              pos : neg    =      1.0 : 1.0\n",
      "                       4 = False             neg : pos    =      1.0 : 1.0\n",
      "                       4 = True              pos : neg    =      1.0 : 1.0\n",
      "                       0 = False             pos : neg    =      1.0 : 1.0\n",
      "                       0 = True              neg : pos    =      1.0 : 1.0\n",
      "                       3 = False             pos : neg    =      1.0 : 1.0\n",
      "                       3 = True              neg : pos    =      1.0 : 1.0\n",
      "                       6 = False             pos : neg    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdec54-9190-492d-9c86-75f9cd3c4b0c",
   "metadata": {},
   "source": [
    "### **Create train sklearn models function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbee1abc-d1ed-4b3e-b487-ada93e4dedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "def train_nltk(my_model, train_set, test_set):\n",
    "    \"\"\"\n",
    "    Train an NLTK classifier using a provided model and evaluate its accuracy on a test set.\n",
    "\n",
    "    Args:\n",
    "    my_model: An NLTK classifier model to be trained.\n",
    "    train_set: Training set for the classifier.\n",
    "    test_set: Test set for evaluating the classifier.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy of the trained model on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the NLTK classifier model to a scikit-learn compatible classifier\n",
    "    model = SklearnClassifier(my_model)\n",
    "\n",
    "    # Train the model using the provided training set\n",
    "    model.train(train_set)\n",
    "\n",
    "    # Extract and print the name of the model (extracted from its string representation)\n",
    "    print(f\"Your model is:  {str(my_model).split('(')[0]}\")\n",
    "\n",
    "    # Calculate accuracy of the trained model on the test set\n",
    "    accuracy = nltk.classify.accuracy(model, test_set) * 100\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "973dd23a-8dcb-40c8-9ee7-769fca809614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  SVC\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "accuracy = train_nltk(SVC(kernel='linear'), train_set, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cbfd28c-1415-4a03-be3f-89996ae5f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  LogisticRegression\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "accuracy = train_nltk(LogisticRegression(), train_set, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f4129d5-b030-499f-8ddf-501d3071bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  SGDClassifier\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_nltk(SGDClassifier(), train_set, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d64f414-8ed5-4102-8f09-66f6865299ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  MultinomialNB\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "accuracy = train_nltk(MultinomialNB(), train_set, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6577458f-f6df-4757-8a51-f66214526a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  BernoulliNB\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_nltk(BernoulliNB(), train_set, test_set)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d786a01-3ad1-4848-b482-1f767adb3639",
   "metadata": {},
   "source": [
    "### **Create preprocess function**\n",
    "\n",
    "which to all the work for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92c34889-8b7c-4798-94ec-810e5688ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from sklearn import model_selection\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def preprocess_nltk(docs, n=2):\n",
    "    \"\"\"\n",
    "    Preprocesses text data using NLTK tools, including tokenization, lemmatization,\n",
    "    removal of stop words and punctuation, and generation of n-grams. Prepares a feature set\n",
    "    for classification tasks using NLTK movie reviews dataset.\n",
    "\n",
    "    Args:\n",
    "    docs (list): List of tuples containing text documents and their corresponding categories.\n",
    "    n (int, optional): Size of n-grams to generate (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the training and testing sets for classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize WordNetLemmatizer for lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Initialize an empty list to store words after preprocessing\n",
    "    lemma_words_without_stops = []\n",
    "\n",
    "    # Tokenize words and preprocess each word\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # Accept only words\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize words from the movie_reviews corpus, remove punctuations and stop words\n",
    "    words = [tokenizer.tokenize(w)[0] for w in mr.words() if tokenizer.tokenize(w)]\n",
    "    for w in tqdm(words):\n",
    "        # Lemmatize words and remove stop words\n",
    "        lemma_words_without_stops.append(lemmatizer.lemmatize(w.lower())) if (w not in stops) else None\n",
    "\n",
    "    # Generate n-grams from the preprocessed words\n",
    "    if n > 1:\n",
    "        ngrams_list = []\n",
    "        feature_ngram = []\n",
    "        for gram in range(2, n + 1):\n",
    "            print(f\"Add ngram={gram} to the data\")\n",
    "            for w in nltk.ngrams(lemma_words_without_stops, gram):\n",
    "                word = ''\n",
    "                for i in range(gram):\n",
    "                    word += w[i] + ' '\n",
    "                ngrams_list.append(word.strip())\n",
    "    \n",
    "            # Compute the frequency distribution of n-grams\n",
    "            ngrams_freq = nltk.FreqDist(ngrams_list)\n",
    "            feature_ngram += list(ngrams_freq)[:1000]\n",
    "\n",
    "    # Compute the frequency distribution of lemmatized words\n",
    "    lemma_words_without_stops = nltk.FreqDist(lemma_words_without_stops)\n",
    "\n",
    "    # Select the top 3000 most common lemmatized words and n-grams as the feature vector\n",
    "    feature_vector = list(lemma_words_without_stops)[:3000]\n",
    "    feature_vector += feature_ngram if n > 1 else []\n",
    "\n",
    "    # Prepare the feature set for classification\n",
    "    features = [(feature_extract(rev, feature_vector), category) for (rev, category) in docs]\n",
    "\n",
    "    # Split the feature set into training and testing sets\n",
    "    train_set, test_set = model_selection.train_test_split(features, test_size=0.2, random_state=1)\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22365a85-cfcc-4122-8e5e-2b9f0cdf6d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b935f0360c2148b3a1512e448a790bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1336782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [(file, category) for category in mr.categories() for file in mr.fileids(category)]\n",
    "train, test = preprocess_nltk(docs, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbb3907f-8317-4cf7-9e3b-5164c1980160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use nltk Naive bayes classifier ngrams equal 1\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, test) * 100\n",
    "print(f\"NaiveBayesClassifier accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83d4c85e-b7a3-456d-815c-ec3145566a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       1 = False             neg : pos    =      1.1 : 1.0\n",
      "                       2 = False             pos : neg    =      1.1 : 1.0\n",
      "                       7 = False             pos : neg    =      1.1 : 1.0\n",
      "                       7 = True              neg : pos    =      1.1 : 1.0\n",
      "                       2 = True              neg : pos    =      1.1 : 1.0\n",
      "                       5 = False             neg : pos    =      1.0 : 1.0\n",
      "                       1 = True              pos : neg    =      1.0 : 1.0\n",
      "                       5 = True              pos : neg    =      1.0 : 1.0\n",
      "                       4 = False             pos : neg    =      1.0 : 1.0\n",
      "                       4 = True              neg : pos    =      1.0 : 1.0\n",
      "                       0 = False             neg : pos    =      1.0 : 1.0\n",
      "                       0 = True              pos : neg    =      1.0 : 1.0\n",
      "                       6 = False             pos : neg    =      1.0 : 1.0\n",
      "                       6 = True              neg : pos    =      1.0 : 1.0\n",
      "                       3 = False             neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "817c88a5-48f2-45d9-ae2d-86896ed164b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57c50511f5847379dbe7c69ce0d1667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1336782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ngram=2 to the data\n",
      "NaiveBayesClassifier accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "# ngrams equal 2\n",
    "train, test = preprocess_nltk(docs, n=2)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, test) * 100\n",
    "print(f\"NaiveBayesClassifier accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f249a4f-8a4f-41dc-b913-6bdd3367d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9a80cb21fb416dabaf4c5b13cf48fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1336782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ngram=2 to the data\n",
      "Add ngram=3 to the data\n",
      "NaiveBayesClassifier accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "# ngrams equal 3\n",
    "train, test = preprocess_nltk(docs, n=3)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, test) * 100\n",
    "print(f\"NaiveBayesClassifier accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f99548b-1f4e-4e6c-aca5-e6a639c4d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is:  MultinomialNB\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn model\n",
    "accuracy = train_nltk(MultinomialNB(), train, test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2feab-268c-4997-83f3-0c919deef609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
