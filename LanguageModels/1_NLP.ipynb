{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6031194c-0545-42ce-a438-5c1906f4251a",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**\n",
    "\n",
    "<img align='center' width='600' src=\"https://images.gr-assets.com/misc/1535611813-1535611813_goodreads_misc.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e00d4a-8c5c-46c7-b6b8-758dac51bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99009e49-e7e3-42d3-acc9-199d16ce1edb",
   "metadata": {},
   "source": [
    "## **Text data**\n",
    "\n",
    "- **Lexicons**\n",
    "  \n",
    "  can be define as the vocabulary of a person, language or branch of knowledge. They can be thought of as a dictionary of terms that are called lexemes.\n",
    "\n",
    "- **Phonemes**\n",
    "  \n",
    "  Speech sounds made by mouth or unit of sound.\n",
    "\n",
    "- **Grapheme**\n",
    "  \n",
    "  Groups of letters of size one or more that can represent these individual sounds or phonemes. The word *spoon* consists of five letters that actually repsresent four phonemes.\n",
    "\n",
    "- **Morphemes**\n",
    "  \n",
    "  The smallets meaningful unit in a language. The word *unstoppable* is composed of three morphemes:\n",
    "  - *un* --> a bound morpheme signifying not\n",
    "  - *stop* --> the root morpheme\n",
    "  - *able* --> a free morpheme signifying can be done\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Tokenization**\n",
    "  \n",
    "  Can be thought as a segmentation(word tokenization, sentence tokenization, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697017a9-a63e-43ea-8ebe-948bd557dac7",
   "metadata": {},
   "source": [
    "## **Spacy**\n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "```python\n",
    "nlp = spacy.blank('en') # is an empty pipeline to add more items or say better pretrain models\n",
    "```\n",
    "First we must download them(pretrain models like `en_core_web_sm`), then to use the model we go like:\n",
    "\n",
    "``` python\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# or we can use another method ---------------------------\n",
    "nlp = en_core_web_sm.load()\n",
    "```\n",
    "\n",
    "Since the pipeline is blank it isn't tokenize the sentences efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899d154b-a8b0-46d4-9b12-9b269505a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "My\n",
      "dear\n",
      "friend\n",
      "------------------------------\n",
      "night\n"
     ]
    }
   ],
   "source": [
    "# spacy.blank \n",
    "# en_core_web_sm need to be download first: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "sentence = 'Hello My dear friend I will be late to night.'\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc[:4]:\n",
    "    print(token)\n",
    "    \n",
    "print('-'*30)\n",
    "# you use slicing method on doc object\n",
    "print(doc[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5c0ac0-e654-41dd-b172-af83f0df5a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'a', 'realy', 'happy', 'since', 'my', 'dad', 'say', '\"', 'Your', 'are', 'the', 'best', 'son', '!', '\"', ',', 'then', 'gave', 'me', '100$.']\n"
     ]
    }
   ],
   "source": [
    "sentence = '''I'm realy happy since my dad say \"Your are the best son!\", then gave me 100$.'''\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0fab56f-0361-4e24-b050-88809b3e7c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'> <class 'spacy.tokens.doc.Doc'> <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nlp), type(doc), type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c04f0-7087-42bf-85dc-3e6554eb3312",
   "metadata": {},
   "source": [
    "### Token attributes\n",
    "\n",
    "attributes are specified using internal IDs in many places \n",
    "\n",
    "for more information check the [link](https://spacy.io/api/attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed90afa-0c1b-41c7-821e-463cbcfc7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  token   |          Attributes          \n",
      "----------------------------------------\n",
      "    I     |           - is alpha          \n",
      "    'm    |                               \n",
      "  realy   |           - is alpha          \n",
      "  happy   |           - is alpha          \n",
      "  since   |           - is alpha          \n",
      "    my    |           - is alpha          \n",
      "   dad    |           - is alpha          \n",
      "   say    |           - is alpha          \n",
      "    \"     |   - is quote - is punctuation \n",
      "   Your   |           - is alpha          \n",
      "   are    |           - is alpha          \n",
      "   the    |           - is alpha          \n",
      "   best   |           - is alpha          \n",
      "   son    |           - is alpha          \n",
      "    !     |        - is punctuation       \n",
      "    \"     |   - is quote - is punctuation \n",
      "    ,     |        - is punctuation       \n",
      "   then   |           - is alpha          \n",
      "   gave   |           - is alpha          \n",
      "    me    |           - is alpha          \n",
      "    $     |          - is curency         \n",
      "   100    |           - is digit          \n",
      "    .     |        - is punctuation       \n"
     ]
    }
   ],
   "source": [
    "sentence = '''I'm realy happy since my dad say \"Your are the best son!\", then gave me $100.'''\n",
    "doc = nlp(sentence)\n",
    "print(f\"{'token':^10}|{' Attributes':^30}\")\n",
    "print('-'*40)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:^10}|\", end=' ')\n",
    "    attribut = ''\n",
    "    if token.is_alpha:\n",
    "        attribut += ' - is alpha'\n",
    "    if token.is_digit:\n",
    "        attribut += ' - is digit'\n",
    "    if token.is_currency:\n",
    "        attribut += ' - is curency'\n",
    "    if token.is_quote:\n",
    "        attribut += ' - is quote'\n",
    "    if token.is_punct:\n",
    "        attribut += ' - is punctuation'\n",
    "        \n",
    "    print(f\"{attribut:^30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a529284-9d1e-4bb9-931b-c1250316c9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My high school, 8th grade students information\\n',\n",
       " '==============================================\\n',\n",
       " '\\n',\n",
       " 'Name        Birth day       Email\\n',\n",
       " '----        --------        ------\\n',\n",
       " 'Virat       5 June, 1882    virat@gmail.com\\n',\n",
       " 'ali         10 August 1884  ali_new@yahoo.com\\n',\n",
       " 'joe         3 February 1883 joe.1883@gmail.com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('students.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ba1144-b758-4cd6-8fac-7305789db92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My high school, 8th grade students information\n",
      "==============================================\n",
      "\n",
      "Name        Birth day       Email\n",
      "----        --------        ------\n",
      "Virat       5 June, 1882    virat@gmail.com\n",
      "ali         10 August 1884  ali_new@yahoo.com\n",
      "joe         3 February 1883 joe.1883@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(''.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4a1443-9d59-48dd-8e65-53f7ab1cde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d621707-7c4a-43eb-ab5d-ce3b3ef1ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat@gmail.com\n",
      "ali_new@yahoo.com\n",
      "joe.1883@gmail.com\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentences)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a640594-1b75-4b2b-936d-34f63f56a2e7",
   "metadata": {},
   "source": [
    "### REGEX\n",
    "Now do the same task using regular expression\n",
    "check the [link](https://github.com/CoreyMSchafer/code_snippets/blob/master/Python-Regular-Expressions/snippets.txt) to see regular expression, Even you see the [video on youtube](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwijj8r9kr-EAxUT7QIHHTOzD8IQtwJ6BAgTEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DK8L6KVGG-7o&usg=AOvVaw3OMhMUs8L3x615-f-V44_7&opi=89978449)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Special character  |  Action  |\n",
    "|--------------------|----------|\n",
    "|     .              | Any Character Except New Line |\n",
    "|    \\d              | Digit (0-9) |\n",
    "|    \\D              | Not a Digit (0-9) |\n",
    "|    \\w              | Word Character (a-z, A-Z, 0-9, _) |\n",
    "|    \\W              | Not a Word Character |\n",
    "|    \\s              | Whitespace (space, tab, newline) |\n",
    "|    \\S              | Not Whitespace (space, tab, newline) |\n",
    "|**----------------**|**----------------------------**|\n",
    "|    \\b              | Word Boundary |\n",
    "|    \\B              | Not a Word Boundary |\n",
    "|    ^               | Beginning of a String |\n",
    "|    $               | End of a String |\n",
    "|**----------------**|**----------------------------**|\n",
    "|    [ ]             | Matches Characters in brackets |\n",
    "|    [^ ]            | Matches Characters NOT in brackets |\n",
    "|                    | Either Or |\n",
    "|    ( )             | Group |\n",
    "|  **Quantifiers:**  |    ---    |\n",
    "|    *               | 0 or More |\n",
    "|    +               | 1 or More |\n",
    "|    ?               | 0 or One |\n",
    "|   {3}              | Exact Number |\n",
    "|   {3,4}           | Range of Numbers (Minimum, Maximum) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1735e199-3b47-4f3a-8805-f81033383635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat@gmail.com\n",
      "new@yahoo.com\n",
      "joe.1883@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'[a-zA-Z0-9-.]+@[a-zA-Z0-9-.]+\\.\\w+'\n",
    "pattern = re.compile(pattern)\n",
    "\n",
    "matches = re.finditer(pattern, sentences)\n",
    "\n",
    "for match in matches:\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404376f3-23f2-463a-a321-562d04ded41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', \"'s\", 'go', 'play', 'game']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"let's go play game\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f59484-971f-4ed4-a8b1-275fa1bbfb3f",
   "metadata": {},
   "source": [
    "<img align='center' width='600' src=\"https://spacy.io/images/pipeline-design.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131c0407-4d78-42cf-b04f-077e8b3e0167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x293d28f50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cceed1a-cedd-481c-b627-52ae88b1b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's go play game.\n",
      "Do you like video games?\n",
      "Yes, of course\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"let's go play game. Do you like video games? Yes, of course\")\n",
    "for token in doc.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f1cb0-6c47-48f3-b005-c1af408d22b8",
   "metadata": {},
   "source": [
    "### Pipe lines\n",
    "now check the `nlp` object pipelines befor and after load pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1b3086c-4860-4368-9355-a1cd8ce4c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1260ad-6d06-4da7-9041-abe69cd9fc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d47df432-7822-4bfe-9a22-ab18fc283563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--token--- ---POS---- --lemma---\n",
      "   let        VERB       let    \n",
      "    's        PRON        us    \n",
      "    go        VERB        go    \n",
      "   play       VERB       play   \n",
      "   game       NOUN       game   \n",
      "    .        PUNCT        .     \n",
      "    Do        AUX         do    \n",
      "   you        PRON       you    \n",
      "   like       VERB       like   \n",
      "  video       NOUN      video   \n",
      "  games       NOUN       game   \n",
      "    ?        PUNCT        ?     \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"let's go play game. Do you like video games?\")\n",
    "\n",
    "print(f\"{'token':-^10} {'POS':-^10} {'lemma':-^10}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:^10} {token.pos_:^10} {token.lemma_:^10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec440a-e921-4d8f-b912-a4dc2dc6e941",
   "metadata": {},
   "source": [
    "**Stopword**\n",
    "\n",
    "Stopword are words such as *a, an, the, in, at, ...* that accure frequently in text corpora and do not carry a lot of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36093d19-4b39-40e3-ade5-23f67082e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['per', 'fifty', 'everyone', 'whether', 'quite', 'it', 'did', 'once', 'each', 'former', 'something', 'nowhere', 'therein', 'were', 'whom']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Check for stop words\n",
    "print(list(STOP_WORDS)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7179bfb-6d6e-490a-bc64-5f1530e833c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regarding\n",
      "am\n",
      "her\n",
      "if\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "items = ['regarding', 'am', 'her', 'if', 'hello']\n",
    "stops = list(STOP_WORDS)[:15]\n",
    "\n",
    "for w in items:\n",
    "    if w not in stops:\n",
    "        print(w)\n",
    "# [w for w in items if w not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9167537-b6ff-4e44-8db6-b024021cf14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with Stopword:\n",
      " let | 's | go | play | game | . | Do | you | like | video | games | ? \n",
      "\n",
      "Tokens without Stopword:\n",
      " let | play | game | . | like | video | games | ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens with Stopword:\\n\", ' | '.join([token.text for token in doc]), \"\\n\")\n",
    "print(\"Tokens without Stopword:\\n\", ' | '.join([token.text for token in doc if not token.is_stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7fabdb-4b39-4573-b497-f7f7966b54a2",
   "metadata": {},
   "source": [
    "### Not as a STOP_WORD \n",
    "It's really important remove the word \"not\", which is exist in STOP_WORD list, in order to preserve the negative aspect of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f3d3a4-89b8-4626-a601-e12b1c89de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Sentence           |    Stops removal    |     Custom Stops    \n",
      "---------------------------------------------------------------------------\n",
      "      This movie is good      |      movie good     |      movie good     \n",
      "    This movie is not good    |      movie good     |    movie not good   \n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDSn\n",
    "\n",
    "pos = \"This movie is good\"\n",
    "neg = \"This movie is not good\"\n",
    "\n",
    "pos_doc = nlp(pos.lower())\n",
    "neg_doc = nlp(neg.lower())\n",
    "\n",
    "stops = list(STOP_WORDS)\n",
    "stops.remove('not')\n",
    "\n",
    "pos_tokens = [token.text for token in pos_doc if not token.is_stop]\n",
    "pos_tokens2 = [token.text for token in pos_doc if token.text not in stops]\n",
    "\n",
    "neg_tokens = [token.text for token in neg_doc if not token.is_stop]\n",
    "neg_tokens2 = [token.text for token in neg_doc if token.text not in stops]\n",
    "\n",
    "\n",
    "print(f\"{'Sentence':^30}| {'Stops removal':^20}| {'Custom Stops':^20}\", end='\\n'+'-'*75+'\\n')\n",
    "\n",
    "print(f\"{pos:^30}| {' '.join(pos_tokens):^20}| {' '.join(pos_tokens2):^20}\")\n",
    "print(f\"{neg:^30}| {' '.join(neg_tokens):^20}| {' '.join(neg_tokens2):^20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a855bd-a24e-490e-853e-cfd5d16b7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       entity             ent.label_                 explain            \n",
      "       Apple                 ORG          Companies, agencies, institutions, etc.\n",
      "    $1 trillion             MONEY         Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple worth $1 trillion\")\n",
    "\n",
    "print(f\"{'entity':^20} {'ent.label_':^20} {'explain':^30}\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:^20} {ent.label_:^20} {spacy.explain(ent.label_):^10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873d688-969d-47bc-b045-1f0d900b35ad",
   "metadata": {},
   "source": [
    "## **NLTK**\n",
    "\n",
    "First part of preprocessing is tokenize the sentences. NLTK have various type of tokenizer which are listed [here](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96ed6605-ec80-45c4-b8c7-62bb2f9d893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    sent_tokenize, RegexpTokenizer,\n",
    "    TreebankWordTokenizer,\n",
    "    SExprTokenizer,\n",
    "    TweetTokenizer\n",
    "    )\n",
    "\n",
    "sentence = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551bccd0-0a4a-4bc9-80b2-e90f6beaee98",
   "metadata": {},
   "source": [
    "### `nltk.tokenize.word_tokenize`\n",
    "\n",
    "Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f28dfe-586b-4d7a-a0e9-c1b2c5529b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000.0', '-', '$', '8000.0', 'in', 'USA', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2cf25-9a46-482a-b8a5-be5ba3af9d8a",
   "metadata": {},
   "source": [
    "### `nltk.tokenize.wordpunct_tokenize`\n",
    "\n",
    "  NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation\n",
    "\n",
    "**Warning**: sometimes we need to decode the input string. `sentence.decode(\"utf8\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "523e651e-2c06-4e86-87de-4ca4c5a64056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000', '.', '0', '-', '$', '8000', '.', '0', 'in', 'USA', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69ae6d-44ce-4d54-93e4-8d909b4f90dc",
   "metadata": {},
   "source": [
    "### `nltk.tokenizer.sent_tokenize`\n",
    "\n",
    "  We can also operate at the level of sentences, using the sentence tokenizer directly as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cabe9fa0-136f-4b18-9186-c962729a2063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Rolex watch costs in the range of $3000.0 - $8000.0 in USA. I think It's really expensive.\n",
      "\n",
      "Tokens: \n",
      "A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\n",
      "I think It's really expensive.\n"
     ]
    }
   ],
   "source": [
    "sentence2 = sentence + \" I think It's really expensive.\"\n",
    "print(sentence2, end ='\\n\\n')\n",
    "\n",
    "print(\"Tokens: \")\n",
    "for token in sent_tokenize(sentence2):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903274f-d103-42b1-a9d1-63f3430bc916",
   "metadata": {},
   "source": [
    "### `nltk.tokenize.regexp`\n",
    "\n",
    "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of: <br>\n",
    "- alphabetic sequences: `\\w+` <br>\n",
    "- money expressions: `\\s[0-9\\.]+`, and <br>\n",
    "- any other non-whitespace sequences: `\\S+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5e61da4-60ab-4eb2-b095-01ef6a256d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$3000.0', '-', '$8000.0', 'in', 'USA', '.']\n"
     ]
    }
   ],
   "source": [
    "reg_tokenizer = RegexpTokenizer('\\w+|\\s[0-9\\.]+|\\S+')\n",
    "print(reg_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f842fb-f876-4987-9bc0-a2936bc027cc",
   "metadata": {},
   "source": [
    "### `nltk.tokenize.treebank`\n",
    "\n",
    "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This implementation is a port of the tokenizer sed script written by Robert McIntyre and available at [here](http://www.cis.upenn.edu/~treebank/tokenizer.sed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44c8db16-91e3-4ab7-88db-999323492f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000.0', '-', '$', '8000.0', 'in', 'USA', '.']\n"
     ]
    }
   ],
   "source": [
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "print(tree_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24179c1a-2016-4b30-adb8-b65759a5bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize.stanford import StanfordTokenizer\n",
    "# stan_tokenizer = StanfordTokenizer(options={\"americanize\": True})\n",
    "# print(stan_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7519d-e585-4ceb-9fbf-d53065e6ca79",
   "metadata": {},
   "source": [
    "### `nltk.tokenize.sexpr`\n",
    "SExprTokenizer is used to find *parenthesized expressions* in a string. In particular, it divides a string into a sequence of substrings that are either parenthesized expressions (including any nested parenthesized expressions), or other whitespace-separated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48a3a63f-f92b-418a-a584-9ba1225b4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SExpr output: \n",
      " ['A', 'Rolex', 'watch', '(which is a luxury brand)', 'costs', 'about', '$5000', '(sometimes even higher)', '.']\n",
      "\n",
      "TreeBank output:\n",
      " ['A', 'Rolex', 'watch', '(', 'which', 'is', 'a', 'luxury', 'brand', ')', 'costs', 'about', '$', '5000', '(', 'sometimes', 'even', 'higher', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence3 = \"A Rolex watch (which is a luxury brand) costs about $5000 (sometimes even higher).\"\n",
    "# used to find parenthesized expressions in a string.\n",
    "se_tokenizer = SExprTokenizer()\n",
    "\n",
    "print(\"SExpr output: \\n\",se_tokenizer.tokenize(sentence3), end=\"\\n\\n\")\n",
    "print(\"TreeBank output:\\n\", tree_tokenizer.tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0041f-b134-4644-bf58-c1b9475557cb",
   "metadata": {},
   "source": [
    "### `nltk.tokenizer.TweetTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "078678b8-46b8-4164-a758-df1f6f2d890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Mhddaraaa', 'I', 'just', 'bought', 'a', 'newwwww', 'car', '!', ':D', '#happiness', '#audi', '<3']\n",
      "['I', 'just', 'bought', 'a', 'newww', 'car', '!', ':D', '#happiness', '#audi', '<3']\n"
     ]
    }
   ],
   "source": [
    "sentence4 = \"@Mhddaraaa I just bought a newwwww car! :D #happiness #audi <3\"\n",
    "t_tokenizer = TweetTokenizer()\n",
    "print(t_tokenizer.tokenize(sentence4))\n",
    "\n",
    "# if you want get rid of @ and excesive letters\n",
    "t_tokenizer = TweetTokenizer(reduce_len=True, strip_handles=True)\n",
    "print(t_tokenizer.tokenize(sentence4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1e297-94f9-4105-819a-cf5ac0f4bf79",
   "metadata": {},
   "source": [
    "### ```nltk.tokenize.punkt``` \n",
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9836582e-0bb4-4a65-91de-13ad1853a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = tokenizer.tokenize(test_text)\n",
    "\n",
    "tagged = []\n",
    "for w in tokenized:\n",
    "    words = nltk.word_tokenize(w)\n",
    "    tagged.append(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "251cc304-699b-4241-a61f-83d94c511262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tokens       |    POS    \n",
      "------------------------------\n",
      "        Mr.         |    NNP    \n",
      "      Speaker       |    NNP    \n",
      "         ,          |     ,     \n",
      "        Vice        |    NNP    \n",
      "     President      |    NNP    \n",
      "       Cheney       |    NNP    \n",
      "         ,          |     ,     \n",
      "      members       |    NNS    \n",
      "         of         |     IN    \n",
      "      Congress      |    NNP    \n",
      "         ,          |     ,     \n",
      "      members       |    NNS    \n",
      "         of         |     IN    \n",
      "        the         |     DT    \n",
      "      Supreme       |    NNP    \n",
      "       Court        |    NNP    \n",
      "        and         |     CC    \n",
      "     diplomatic     |     JJ    \n",
      "       corps        |     NN    \n",
      "         ,          |     ,     \n",
      "   distinguished    |     JJ    \n",
      "       guests       |    NNS    \n",
      "         ,          |     ,     \n",
      "        and         |     CC    \n",
      "       fellow       |     JJ    \n",
      "      citizens      |    NNS    \n",
      "         :          |     :     \n",
      "       Today        |     VB    \n",
      "        our         |    PRP$   \n",
      "       nation       |     NN    \n",
      "        lost        |    VBD    \n",
      "         a          |     DT    \n",
      "      beloved       |    VBN    \n",
      "         ,          |     ,     \n",
      "      graceful      |     JJ    \n",
      "         ,          |     ,     \n",
      "     courageous     |     JJ    \n",
      "       woman        |     NN    \n",
      "        who         |     WP    \n",
      "       called       |    VBD    \n",
      "      America       |    NNP    \n",
      "         to         |     TO    \n",
      "        its         |    PRP$   \n",
      "      founding      |     NN    \n",
      "       ideals       |    NNS    \n",
      "        and         |     CC    \n",
      "      carried       |    VBD    \n",
      "         on         |     IN    \n",
      "         a          |     DT    \n",
      "       noble        |     JJ    \n",
      "       dream        |     NN    \n",
      "         .          |     .     \n"
     ]
    }
   ],
   "source": [
    "# Let's check the first sentence\n",
    "print(f\"{'tokens':^20}| {'POS':^10}\")\n",
    "print('-'*30)\n",
    "for item in tagged[1]:\n",
    "    print(f\"{item[0]:^20}| {item[1]:^10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236ec50-1a70-48a7-b6db-0b1e4b3e40b9",
   "metadata": {},
   "source": [
    "### **Word Normalization**\n",
    "\n",
    "most of the time we don't want to have every individual word fragment that we have ever encountered in our vocabulary. There are several reason for this like:\n",
    "\n",
    "- **Distinguish the phrases or words**\n",
    "\n",
    "  for example: UN with U.N\n",
    "\n",
    "- **Bring words to their root**\n",
    "\n",
    "  am , is , are --> be\n",
    "\n",
    "- **Remove inflections**\n",
    "\n",
    "  car, cars, car's --> car\n",
    "\n",
    "- **Words wich do not convey much meaning**\n",
    "\n",
    "  the articles: *a*, *an* and *the* - Wh words: *when*, *where*, ... . However, all these highly depend on the use cases. All of them are removes as a part of technique called **Stopword removal**.\n",
    "\n",
    "**Note**: **Case Folding** is strategy to convert all of the letter in corpus to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7de2b7-da28-493e-89e7-9dfbeaf3cdbb",
   "metadata": {},
   "source": [
    "#### 1.Stemming\n",
    "A crude attempt is made to remove the inflectional form of a word and bring them to a a base form called **stem**. The chopped-off pieces refred to as **affixes**. \n",
    "\n",
    "\n",
    "- **Over-stemming**\n",
    "\n",
    "  A situation may arise when words that are stemmed to the same root should have been stemmed to different roots\n",
    "\n",
    "- **under-stemmin**\n",
    " \n",
    "  In contrast, words that should have been stemmed to the same root aren't stemmed to it\n",
    "  \n",
    "[NLTK stem package](https://www.nltk.org/api/nltk.stem.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bf4c9-9c01-4ed2-b9b7-321b0f61af65",
   "metadata": {},
   "source": [
    "##### `nltk.stem.porter.PorterStemmer`\n",
    "\n",
    "- Supports the *English* language\n",
    "- Works only with *strings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "998715be-9978-43dd-b48b-e38df3c7a72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  words   |  stems   \n",
      "--------------------\n",
      "   cars   |   car    \n",
      " caresses |  caress  \n",
      "  flies   |   fli    \n",
      "   dies   |   die    \n",
      "  mules   |   mule   \n",
      "   died   |   die    \n",
      " stating  |  state   \n",
      "  sized   |   size   \n",
      "   men    |   men    \n",
      "  women   |  women   \n",
      "   oxen   |   oxen   \n",
      "  geese   |   gees   \n",
      " children | children \n",
      "  teeth   |  teeth   \n",
      "   feet   |   feet   \n",
      "   mice   |   mice   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "words = ['cars', 'caresses', 'flies', 'dies', 'mules', 'died', 'stating',\n",
    "         'sized', 'men', 'women', 'oxen', 'geese', 'children', 'teeth', 'feet', 'mice']\n",
    "p_stemmer = PorterStemmer()\n",
    "stems = [p_stemmer.stem(w) for w in words]\n",
    "\n",
    "for i, (w, l) in enumerate(zip(['words'] + words, ['stems'] + stems)):\n",
    "    print(f\"{w:^10}|{l:^10}\")\n",
    "    print('-' * 20) if i == 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e2bb4-1720-4cdb-b4bf-113f6b5551ab",
   "metadata": {},
   "source": [
    "##### `nltk.stem.snowball.SnowballStemmer`\n",
    "\n",
    "- SnowballStemmer is an improvement of PorterStemmer, supports different languages\n",
    "- Work with both *strings* and *Unicodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42abeff-58e5-4796-aedc-0504935d50b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the supported languages:\n",
      " ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "\n",
      "  words   |  stems   \n",
      "--------------------\n",
      "   cars   |   car    \n",
      " caresses |  caress  \n",
      "  flies   |   fli    \n",
      "   dies   |   die    \n",
      "  mules   |   mule   \n",
      "   died   |   die    \n",
      " stating  |  state   \n",
      "  sized   |   size   \n",
      "   men    |   men    \n",
      "  women   |  women   \n",
      "   oxen   |   oxen   \n",
      "  geese   |   gees   \n",
      " children | children \n",
      "  teeth   |  teeth   \n",
      "   feet   |   feet   \n",
      "   mice   |   mice   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print('Here is the supported languages:\\n',SnowballStemmer.languages, end='\\n\\n')\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "stems = [s_stemmer.stem(w) for w in words]\n",
    "\n",
    "for i, (w, l) in enumerate(zip(['words'] + words, ['stems'] + stems)):\n",
    "    print(f\"{w:^10}|{l:^10}\")\n",
    "    print('-' * 20) if i == 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021451d-d442-491f-a956-f2d1541188ed",
   "metadata": {},
   "source": [
    "#### 2.Lemmatizer\n",
    "Unlike stemming, where in a few characters are removed from words using crude methods, **lemmatization** is a process wherein the context is used to convert a word to its meaningful base form. Lemmatization algorithms try to indentify the lemma form of a word by taking into account the neighborhood conext of the word, **Part-Of-Speech(POS)** tags, the meaning of the word, ... .\n",
    "\n",
    "<br>\n",
    "\n",
    "##### `nltk.stem.wordnet.WordNetLemmatizer`\n",
    "**wordnet** is a lexical database of english.\n",
    "\n",
    "**Note**: wordnet lemmatizer work well if **POS** also included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fd0a32c-b4d5-4e71-91b4-f0aa9ede5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  words   |  lemmas  \n",
      "--------------------\n",
      "   cars   |   car    \n",
      " caresses |  caress  \n",
      "  flies   |   fly    \n",
      "   dies   |    dy    \n",
      "  mules   |   mule   \n",
      "   died   |   died   \n",
      " stating  | stating  \n",
      "  sized   |  sized   \n",
      "   men    |   men    \n",
      "  women   |  woman   \n",
      "   oxen   |    ox    \n",
      "  geese   |  goose   \n",
      " children |  child   \n",
      "  teeth   |  teeth   \n",
      "   feet   |   foot   \n",
      "   mice   |  mouse   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "for i, (w, l) in enumerate(zip(['words'] + words, ['lemmas'] + lemmas)):\n",
    "    print(f\"{w:^10}|{l:^10}\")\n",
    "    print('-' * 20) if i == 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b63a6c-9451-47fe-94b7-858928618e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('putting', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('enhance', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('understanding', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Lemmatization', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "nltk.pos_tag(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77e8303d-ceb9-49fd-bc25-d0ba12c0a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos(token):\n",
    "    \"\"\"\n",
    "    Map POS to first character lemmatize() accepts\n",
    "    if ther is no match for POS of a word it will return NOUN\n",
    "    \"\"\"\n",
    "    \n",
    "    tags_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "    }\n",
    "\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "\n",
    "    return tags_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "052993a2-6ab8-48ac-a7ba-f76f57639a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     words     |    lemmas     |-> lemmas with pos <-\n",
      "--------------------------------------------------\n",
      "      we       |      we       |      we       \n",
      "      are      |      are      |   -> be <-    \n",
      "    putting    |    putting    |   -> put <-   \n",
      "      in       |      in       |      in       \n",
      "    efforts    |    effort     |    effort     \n",
      "      to       |      to       |      to       \n",
      "    enhance    |    enhance    |    enhance    \n",
      "      our      |      our      |      our      \n",
      " understanding | understanding |-> understand <-\n",
      "      of       |      of       |      of       \n",
      " lemmatization | lemmatization | lemmatization \n"
     ]
    }
   ],
   "source": [
    "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "# Case Folding\n",
    "sentence = sentence.lower()\n",
    "\n",
    "pos_tags = nltk.pos_tag(sentence.split())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in sentence.split()]\n",
    "lemmas_with_pos = [lemmatizer.lemmatize(w, get_pos(w)) for w in sentence.split()]\n",
    "\n",
    "items = zip(['words'] + sentence.split(), ['lemmas'] + lemmas, ['lemmas with pos'] + lemmas_with_pos)\n",
    "for i, (w, l, l_p) in enumerate(items):\n",
    "    if l != l_p:\n",
    "        l_p = '-> ' + l_p + ' <-'\n",
    "    print(f\"{w:^15}|{l:^15}|{l_p:^15}\")\n",
    "    print('-' * 50) if i == 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82d6b4-d705-43ba-b5ff-54e2c6438087",
   "metadata": {},
   "source": [
    "#### 3. Stopword removal\n",
    "Stopword are words such as *a, an, the, in, at, ...* that accure frequently in text corpora and do not carry a lot of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bb683d5-f332-4f04-8147-8fa045d96161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 stopword:\n",
      "\n",
      " ['so', 'shan', 'no', 'yours', 'those', 'couldn', \"shan't\", 'not', 'the', 'should', 'before', 'have', 'won', 'about', 'and', 'will', 'during', 'haven', 'to', 'some']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(\"First 20 stopword:\\n\\n\",list(stop)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbfdd85c-01fb-461b-ab5c-15aa4367e5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Stopwords befor wh_word removing: 179\n",
      "len Stopwords after wh_word removing: 171\n"
     ]
    }
   ],
   "source": [
    "# if want classify questions so Wh are important, so we need to remove them from stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "wh_words = ['who', 'what', 'where', 'when', 'why', 'how', 'which', 'whom']\n",
    "\n",
    "print(f\"len Stopwords befor wh_word removing: {len(stop)}\")\n",
    "for w in wh_words:\n",
    "    stop.remove(w)\n",
    "\n",
    "print(f\"len Stopwords after wh_word removing: {len(stop)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb109d6e-8e19-4231-ba35-ed48a2175833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     words     #    lemmas     #    no stopword     \n",
      "##################################################\n",
      "      we       #      we       #        ---         \n",
      "      are      #      are      #        ---         \n",
      "    putting    #    putting    #      putting       \n",
      "      in       #      in       #        ---         \n",
      "    efforts    #    effort     #       effort       \n",
      "      to       #      to       #        ---         \n",
      "    enhance    #    enhance    #      enhance       \n",
      "      our      #      our      #        ---         \n",
      " understanding # understanding #   understanding    \n",
      "      of       #      of       #        ---         \n",
      " lemmatization # lemmatization #   lemmatization    \n"
     ]
    }
   ],
   "source": [
    "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "# Case Folding\n",
    "sentence = sentence.lower()\n",
    "\n",
    "pos_tags = nltk.pos_tag(sentence.split())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in sentence.split()]\n",
    "\n",
    "lemmas_without_stop = [lemmatizer.lemmatize(w) if w not in stop else \"---\" for w in sentence.split()]\n",
    "\n",
    "items = zip(['words'] + sentence.split(), ['lemmas'] + lemmas, ['no stopword'] + lemmas_without_stop)\n",
    "for i, (w, l, s) in enumerate(items):\n",
    "    print(f\"{w:^15}#{l:^15}#{s:^20}\")\n",
    "    print('#' * 50) if i == 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e04cd-b93d-4d49-9f87-ddf38d91b73e",
   "metadata": {},
   "source": [
    "### **N-grams**\n",
    "Every thing we cover until now were tokens of size 1, which means only one word. Sentences generally contain names or phrases or other compound terms such as *living room, coffe mug, look up, ...*. These phrases convey a specific meaning when two or more words are used together. When used individually, they carry a differnt meaning  altogether and the inherent meaning behind the compound term is somewhat lost.\n",
    "\n",
    "There is a **Unigram, Bigram, Trigram, ...**, the naming system can be extended to larger n-grams, but mos **NLP** tasks use only trigram or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "594b0805-6ed0-45bf-9a45-6fcaf9700c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language',\n",
       " 'language processing',\n",
       " 'processing is',\n",
       " 'is developing',\n",
       " 'developing right',\n",
       " 'right now',\n",
       " 'now and',\n",
       " 'and there',\n",
       " 'there will',\n",
       " 'will be',\n",
       " 'be a',\n",
       " 'a lot',\n",
       " 'lot of',\n",
       " 'of new',\n",
       " 'new job.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"Natural language processing is developing right now and there will be a lot of new job.\"\n",
    "\n",
    "tokens = sentence.lower().split()\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "[' '.join(token) for token in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18d952f4-54c8-4f68-8563-59c9df3fd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680c372-fd07-41ea-95ee-ba1d75a07258",
   "metadata": {},
   "source": [
    "### **Wordnet**\n",
    "WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations\n",
    "\n",
    "```wordnet.synset``` : a set of synonyms that share a common meaning.\n",
    "\n",
    "In NLTK docs, Synset string is compose of 3 three different part `lemma`, `pos`, `number`:\n",
    "\n",
    "- <lemma> is the word’s morphological stem\n",
    "- <pos> is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
    "    - n:    NOUN\n",
    "    - v:    VERB\n",
    "    - a:    ADJECTIVE\n",
    "    - s:    ADJECTIVE SATELLITE\n",
    "    - r:    ADVERB \n",
    "- <number> is the sense number, used to disambiguate word meanings, counting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb9c8f74-ffe2-4ae2-9708-ef6876c6bc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plan.n.01',\n",
       " 'program.n.02',\n",
       " 'broadcast.n.02',\n",
       " 'platform.n.02',\n",
       " 'program.n.05',\n",
       " 'course_of_study.n.01',\n",
       " 'program.n.07',\n",
       " 'program.n.08',\n",
       " 'program.v.01',\n",
       " 'program.v.02']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets('program')\n",
    "[syn.name() for syn in syns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2bd6ccd-d7f0-485e-ae94-82b5247fd543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms for program|  POS |                     Definition                    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "        plan        |   n  | a series of steps to be carried out or goals to be accomplished\n",
      "      program       |   n  | a system of projects or services intended to meet a public need\n",
      "     broadcast      |   n  | a radio or television show\n",
      "      platform      |   n  | a document stating the aims and principles of a political party\n",
      "      program       |   n  | an announcement of the events that will occur as part of a theatrical or sporting event\n",
      "  course_of_study   |   n  | an integrated course of academic studies\n",
      "      program       |   n  | (computer science) a sequence of instructions that a computer can interpret and execute\n",
      "      program       |   n  | a performance (or series of performances) at a public presentation\n",
      "      program       |   v  | arrange a program of or for\n",
      "      program       |   v  | write a computer program\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'synonyms for program':^20}| {'POS':^5}| {'Definition':^50}\")\n",
    "print('-'*100)\n",
    "for syn in syns:\n",
    "    lemma, pos, _ = syn.name().split('.')\n",
    "    print(f\"{lemma:^20}| {pos:^5}| {syn.definition():^20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2246c1f5-4285-458f-be7b-8ba64eff485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms of program |                                 lemmas                                \n",
      "#########################################################################################################################\n",
      "        plan        |        plan        ,       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "     broadcast      |      broadcast     ,       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      platform      |      platform      , political_platform ,  political_program ,       program      , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "  course_of_study   |   course_of_study  ,       program      ,      programme     ,     curriculum     ,      syllabus      , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     ,  computer_program  , computer_programme , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n",
      "      program       |       program      ,      programme     , \n",
      " ------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# we want to find synonyms lemmas in nice format, costume format of course\n",
    "print(f\"{'synonyms of program':^20}| {'lemmas':^70}\")\n",
    "print(\"#\"*121)\n",
    "for syn in syns:\n",
    "    print(f\"{syn.name().split('.')[0]:^20}|\", end=' ')\n",
    "    for lemma in syn.lemmas():\n",
    "        print(f\"{lemma.name():^19},\", end=' ')\n",
    "        \n",
    "    print(\"\\n\", \"-\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a37e38b-fca5-4969-88d4-f33570f137b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they drew up a six-step plan\n",
      "they discussed plans for a new bond issue\n"
     ]
    }
   ],
   "source": [
    "# What if we want some example of specific word!?\n",
    "for example in syns[0].examples():\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60fe622b-0c6c-4afc-a39c-757831042bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to find antonym for different synonyms\n",
    "def syn_ant(word, print_=True):\n",
    "    \n",
    "    # Create sysnset object for word\n",
    "    syns = wordnet.synsets(word)\n",
    "    synonyms = []\n",
    "    antonyms = {}\n",
    "    \n",
    "    # append sysnonyms word with out their POS and number in one list\n",
    "    for syn in syns:\n",
    "        synonyms.append(syn.name().split('.')[0])\n",
    "        ants = []\n",
    "        # Iterate for lemma of different synonyms and find their antonyms\n",
    "        for lemma in syn.lemmas():\n",
    "            # Check if there is a antonyms for specific lemma or not\n",
    "            if ant := lemma.antonyms():\n",
    "                ants.append(ant[0].name())\n",
    "                \n",
    "        # If antonym added to a dictionary of antonyms befor hand new antonyms will append to that\n",
    "        lemma = syn.name().split('.')[0]\n",
    "        if lemma in antonyms.keys() and ants:\n",
    "            antonyms[lemma] += ants\n",
    "        # If antonyms doesn't exist in the dictionary it will added to it\n",
    "        elif lemma not in antonyms.keys():\n",
    "            antonyms[lemma] = ants\n",
    "\n",
    "    synonyms = list(set(synonyms))\n",
    "    if print_:\n",
    "        num = len(max(antonyms.values(), key=len)) * 15\n",
    "        print(f\"{'Synonyms of '+word:^20}| {'Antonyms':^{int(num)}}\")\n",
    "        num += 25\n",
    "        print('-' * num)\n",
    "        \n",
    "        for k, v in antonyms.items():\n",
    "            print(f\"{k:^20}|\", end='')\n",
    "            if values := [value for value in v]:\n",
    "                for v in values:\n",
    "                    print(f\"{v:^15}\", end='')\n",
    "            else:\n",
    "                print(f\"{'--':^15}\", end='')\n",
    "            print()\n",
    "            \n",
    "    return synonyms, antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69b3a6a0-cafd-470d-9603-6a9b42c7774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Synonyms of good  |                                          Antonyms                                         \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "        good        |     evil         evilness          bad          badness          bad           evil      \n",
      "     commodity      |      --       \n",
      "        full        |      --       \n",
      "     estimable      |      --       \n",
      "     beneficial     |      --       \n",
      "       adept        |      --       \n",
      "        dear        |      --       \n",
      "     dependable     |      --       \n",
      "     effective      |      --       \n",
      "        well        |      ill      \n",
      "     thoroughly     |      --       \n"
     ]
    }
   ],
   "source": [
    "_, antonyms = syn_ant('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40fad217-b71c-4ebf-8328-5b3d219b66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the similarity between words\n",
    "def similarity(w1, w2):\n",
    "    # first we must creat right format for wordnet and convert them to synset objects\n",
    "    w1_s = wordnet.synset( w1 + '.n.01')\n",
    "    w2_s = wordnet.synset(w2 + '.n.01')\n",
    "\n",
    "    # use wup.similarity to count similarity between words\n",
    "    similarity = w1_s.wup_similarity(w2_s)\n",
    "    print(f\"{w1} and {w2} similarity is: {similarity}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a8050dc-1e59-4ac3-b563-8afe9d63fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship and boat similarity is: 0.9090909090909091\n",
      "ship and car similarity is: 0.6956521739130435\n",
      "ship and goat similarity is: 0.2962962962962963\n"
     ]
    }
   ],
   "source": [
    "similarity('ship', 'boat')\n",
    "similarity('ship', 'car')\n",
    "similarity('ship', 'goat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea87d55-224f-435e-954d-89034ee0f1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
