{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kY0wgW84-DvM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzv3M7G7-IYj",
        "outputId": "eb1f0e3e-cbba-4083-fde1-0abf7123fb67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-14 18:05:11--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-05-14 18:05:11 (12.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/input.txt', mode='r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "ZVBenahi-J9b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length of data: ', len(text))\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKKdBdJE-Lj3",
        "outputId": "b248368c-4689-45b8-f21d-931c340559c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizers**\n",
        "\n",
        "Here are different **SUBWORD** tokenizers:\n",
        "\n",
        "<br>\n",
        "\n",
        "Tokenizer Type | Library/Example | Strength | Weakness | Used in\n",
        "---------------|-----------------|----------|----------|---------\n",
        "BPE | HuggingFace, GPT-2 | Fast, simple | Unstructured merges | GPT-2, RoBERTa |\n",
        "Unigram LM | SentencePiece | Probabilistic, flexible | More complex to train | T5, mT5 |\n",
        "WordPiece | TensorFlow/BERT | Good with rare words | Slower training than BPE | BERT, ALBERT |\n",
        "SentencePiece Framework | SentencePiece | Handles whitespace + full text | Adds special chars | T5, XLNet |\n",
        "Character-level | Custom / simple RNNs | No OOV, trivial implementation | Long sequences | Old RNNs, toy tasks |\n",
        "Byte-level BPE | GPT-2 tokenizer | Handles any input data | Harder to read tokens | GPT-2, GPT-3\n",
        "\n",
        "<br>\n",
        "\n",
        "In previous notebook we implemented **BPE** tokenizer, since we wanted it for GPT models. Now we are going to implement **WordPiece** tokenizer for BERT implementation.\n",
        "\n",
        "<br>\n",
        "\n",
        "1.  **Byte Pair Encoding (BPE)**\n",
        "\n",
        "Start with characters → iteratively merge most frequent adjacent pairs into new tokens.\n",
        "\n",
        "- **Example:**\n",
        "```python\n",
        "\"low lower lowest\"\n",
        "# Split to character level\n",
        "→ ['l', 'o', 'w', 'l', 'o', 'w','e', 'r' ,'l', 'o', 'w', 'e', 's', 't']\n",
        "# → Merge ('l', 'o') → 'lo'\n",
        "→ ['lo', 'w', 'lo', 'w','e', 'r' ,'lo', 'w', 'e', 's', 't']\n",
        "# Merge ('lo', 'w') → 'low'\n",
        "→ ['low', 'low','e', 'r' ,'low', 'e', 's', 't']\n",
        "→ ...\n",
        "```\n",
        "\n",
        "- **Pros:**\n",
        "\t- Efficient\n",
        "    - Deterministic\n",
        "    - Easy to implement (you already did!)\n",
        "    - Good balance between vocab size and sequence length\n",
        "\n",
        "- **Cons:**\n",
        "    - Doesn’t model word boundaries explicitly\n",
        "    - Can create strange merges (e.g., merging across morpheme boundaries)\n",
        "\n",
        "> GPT-2, RoBERTa, CLIP. <br> When you want fast, trainable tokenization with high performance\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **WordPiece:**\n",
        "\n",
        "Similar to BPE but slightly different merge criterion:\n",
        "WordPiece merges tokens to maximize the likelihood of a language model, not just frequency.\n",
        "\n",
        "- **Example:**\n",
        "\n",
        "\"playing\" might become [\"play\", \"##ing\"]\n",
        "(## indicates subword continuation)\n",
        "\n",
        "- **Pros:**\n",
        "\t- Handles rare words and OOV (out-of-vocabulary) tokens well\n",
        "\t- Popular and effective\n",
        "\n",
        "- **Cons:**\n",
        "\t- Slower to train than BPE\n",
        "\t- May break words in unnatural places (like BPE)\n",
        "\n",
        "> BERT, ALBERT, DistilBERT. <br> Best when used with bidirectional Transformer models\n",
        "\n",
        "<br>\n",
        "\n",
        "- GPT recommendations:\n",
        "\n",
        "<br>\n",
        "\n",
        "Use case | Recommended Tokenizer\n",
        "---------|-----------------------\n",
        "Large transformer decoder | BPE or Byte-level BPE |\n",
        "Encoder-decoder models | Unigram (SentencePiece) |\n",
        "Multilingual input | SentencePiece |\n",
        "Highest compression | WordPiece or Unigram |\n",
        "Fastest to implement | BPE\n",
        "\n",
        "<br>\n",
        "\n",
        "> Here We are going to tokenize *tiny Shakespeare*, which is poetic."
      ],
      "metadata": {
        "id": "Kg6T5wNI-VZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BPE Tokenizer**"
      ],
      "metadata": {
        "id": "J5FvC0aO-dou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(words, pair):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        pairs = [[word[i] + ' ' + word[i+1]] for i in range(len(word) - 1)]\n",
        "        if pair in [''.join(pair) for pair in pairs]:\n",
        "            item = ' '.join(word)\n",
        "            item = item.replace(' '.join(pair.split()), ''.join(pair.split()))\n",
        "            new_words.append(item.split())\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return new_words\n",
        "\n",
        "def PBE(text, num_iteration=1000, freq_limit=5, pattern=r\"\\s*\\S+\"):\n",
        "    # Tokenize input text into list of character lists per word\n",
        "    words = [[c for c in item] for item in re.findall(pattern, text)]\n",
        "\n",
        "    pbar = tqdm(range(num_iteration))  # Show progress bar\n",
        "\n",
        "    for _ in pbar:\n",
        "        pairs = []\n",
        "        for item in words:\n",
        "            # Collect all adjacent character pairs from words\n",
        "            pairs += [item[i] + ' ' + item[i+1] for i in range(len(item) - 1)]\n",
        "\n",
        "        print(pairs[:10])\n",
        "        # Find most common pair\n",
        "        max = Counter(pairs).most_common(1)[0]\n",
        "\n",
        "        if max[1] > freq_limit:\n",
        "            pair = max[0]\n",
        "            words = merge(words, pair)  # Merge the pair in all words\n",
        "        else:\n",
        "            break  # Stop if no pair is frequent enough\n",
        "\n",
        "        pbar.set_description(f\"Max frequency: {max[1]}, \")\n",
        "\n",
        "    # Flatten the list of words into a single list of subwords/tokens\n",
        "    return [w for word in words for w in word]"
      ],
      "metadata": {
        "id": "XDdOkeR--Q7T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = PBE(text, num_iteration=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "b381b8e6ac1e465d8310f2f2bf09ea0d",
            "d357a19992094c6880a72f2c3fd8964e",
            "7888361472f74aeabb51f8e4fcf2cc5d",
            "93a9fb5535c34ba8972cc49d34406dec",
            "0dd401abd1f04b80a6cd14247c3a559a",
            "c2a55659dc0040a1bf3faf6db105c88c",
            "0dc233c583ab456da9886fb6ceb4a0f2",
            "2f79bdda166e4b288d549b217f067437",
            "8a465223cc9240a69933af0827ef0b66",
            "4748478aa1a04318a615a6b8c44de098",
            "a6cef49cbae84538a1efbaef2430ff25"
          ]
        },
        "id": "LGHIZ6ep-iEo",
        "outputId": "60ba5ac4-7697-49e6-d2df-5cdfb8234603"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b381b8e6ac1e465d8310f2f2bf09ea0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F i', 'i r', 'r s', 's t', '  C', 'C i', 'i t', 't i', 'i z', 'z e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WordPiece**"
      ],
      "metadata": {
        "id": "VN-urOAG-x-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "napVxm6J-m2U",
        "outputId": "eabf4c4a-f68f-4dd2-a892-c3f55fa419cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ^ → start of line\n",
        "#  .*?:.* → any content that includes : (non-greedy before the :) --> (*?: zero or more characters, non-greedily)\n",
        "#  $ → end of line\n",
        "#  \\n? → optionally match the newline\n",
        "#  flags=re.MULTILINE → makes ^ and $ apply to each line\n",
        "\n",
        "pattern = r'^.*?:.*$\\n?'  # matches any line that contains a colon\n",
        "cleaned = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
        "print(cleaned[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsn8UJzX_NIe",
        "outputId": "8d592830-f320-4bab-e218-dfeaab5234a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "Speak, speak.\n",
            "\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "Resolved. resolved.\n",
            "\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "We know't, we know't.\n",
            "\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "\n",
            "One word, good citizens.\n",
            "\n",
            "We are accounted poor citizens, the patricians good.\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "afflicts us, the object of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'[.?!]\\s*'  # Split on ., ?, ! followed by any space\n",
        "sentences = re.split(pattern, cleaned)\n",
        "words = [s.split() for s in sentences if len(s.split()) > 3]\n",
        "words[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tmdVtiC_Q3v",
        "outputId": "350e68ac-7cbf-40ff-a510-5a812ce40070"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['You',\n",
              " 'are',\n",
              " 'all',\n",
              " 'resolved',\n",
              " 'rather',\n",
              " 'to',\n",
              " 'die',\n",
              " 'than',\n",
              " 'to',\n",
              " 'famish']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'^.*?:.*$\\n?'\n",
        "cleaned = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
        "\n",
        "# Split sentences\n",
        "pattern = r'[.?!]\\s*'\n",
        "sentences = re.split(pattern, cleaned)\n",
        "\n",
        "# Only keep sentences with at least 4 words\n",
        "words = [s.split() for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "# Flatten the list of word lists\n",
        "flatten_words = [word for sentence in words for word in sentence]\n",
        "\n",
        "# Initialize vocabulary with characters\n",
        "chars = list(set(''.join(flatten_words)))\n",
        "chars.extend(['[UNK]', '[CLS]', '[SEP]'])\n",
        "\n",
        "# Convert each word to a list of its characters\n",
        "character_lvl_words = [[ch for ch in word] for word in flatten_words]"
      ],
      "metadata": {
        "id": "_BDrV42I_Sks"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge adjacent character pairs in tokenized words\n",
        "def merge(words, pair_str):\n",
        "    a, b = pair_str.split()  # Split the pair string into two parts\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        i = 0\n",
        "        new_word = []\n",
        "        while i < len(word):\n",
        "            # Merge the pair (a, b) if found\n",
        "            if i < len(word) - 1 and word[i] == a and word[i + 1] == b:\n",
        "                merged_token = a + b\n",
        "                new_word.append(merged_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        new_words.append(new_word)\n",
        "\n",
        "    return new_words\n",
        "\n",
        "# Add WordPiece-style prefixes (##) to non-initial subword tokens\n",
        "def add_wordpiece_prefixes(token_list):\n",
        "    result = []\n",
        "    for word in token_list:\n",
        "        if not word:\n",
        "            continue\n",
        "        result.append([word[0]] + ['##' + t for t in word[1:]])\n",
        "    return result"
      ],
      "metadata": {
        "id": "E4TSbc0a_U4i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Likelihood**\n",
        "\n",
        "1. **Use entropy(counts):**\n",
        "\n",
        "    If you want to reduce uncertainty and encourage compact token distributions — closer to BERT’s goal in WordPiece.\n",
        "2. **Use sum(counts * log(probs)):**\n",
        "    \n",
        "    If you want a likelihood-based heuristic that reflects how much total “confidence” the model has in the token set.\n",
        "\n",
        "3. **frequency-based heuristic**\n",
        "\n",
        "    When we want a fast, lightweight, and non-probabilistic training process.\n",
        "\n",
        "$$\n",
        "\\text{score}(a, b) = \\frac{\\text{freq}(a \\, b)}{\\text{freq}(a) \\times \\text{freq}(b)}\n",
        "$$\n",
        "\n",
        "> freq(a b) = frequency of the pair (a, b) appearing together (adjacent).<br>\n",
        "freq(a) = how often token a appears in the corpus.<br>\n",
        "freq(b) = how often token b appears in the corpus."
      ],
      "metadata": {
        "id": "hNkwhAjw_Ycf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood_sum(before, after):\n",
        "    def sum_counts(counts):\n",
        "        # Normalize to get probabilities\n",
        "        probs = counts / counts.sum()\n",
        "        return np.sum(counts * np.log(probs + 1e-12))  # small epsilon for stability\n",
        "\n",
        "    # Flatten and count tokens using Counter\n",
        "    flat_before = [token for word in before for token in word]\n",
        "    flat_after = [token for word in after for token in word]\n",
        "\n",
        "    before_counts = np.array(list(Counter(flat_before).values()), dtype=np.float64)\n",
        "    after_counts = np.array(list(Counter(flat_after).values()), dtype=np.float64)\n",
        "\n",
        "    return sum_counts(after_counts) - sum_counts(before_counts)\n",
        "\n",
        "def log_likelihood_entropy(before, after):\n",
        "    def entropy(counts):\n",
        "        # Normalize to get probabilities\n",
        "        probs = counts / counts.sum()\n",
        "        return -np.sum(probs * np.log(probs + 1e-12))  # small epsilon for stability\n",
        "\n",
        "    # Flatten and count tokens using Counter\n",
        "    flat_before = [token for word in before for token in word]\n",
        "    flat_after = [token for word in after for token in word]\n",
        "\n",
        "    before_counts = np.array(list(Counter(flat_before).values()), dtype=np.float64)\n",
        "    after_counts = np.array(list(Counter(flat_after).values()), dtype=np.float64)\n",
        "\n",
        "    return entropy(before_counts) - entropy(after_counts)  # Δ entropy: positive means after-merge distribution is more compact (lower entropy)"
      ],
      "metadata": {
        "id": "UgXLc-J2_Wg2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove metadata lines (e.g., \"speaker: text\")\n",
        "    pattern = r'^.*?:.*$\\n?'\n",
        "    cleaned = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Split sentences\n",
        "    pattern = r'[.?!]\\s*'\n",
        "    sentences = re.split(pattern, cleaned)\n",
        "\n",
        "    # Only keep sentences with at least 4 words\n",
        "    words = [s.split() for s in sentences if len(s.split()) > 3]\n",
        "    sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    return words, sentences"
      ],
      "metadata": {
        "id": "7E--X3RY_aaP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordPiece(text, num_iteration=1000, freq_limit=5, likeli_iteration=30, load_char_lvl=False):\n",
        "\n",
        "    if load_char_lvl:\n",
        "        character_lvl_words = np.load(load_char_lvl, allow_pickle=True).tolist()\n",
        "\n",
        "    else:\n",
        "        words, _ = clean_text(text)\n",
        "\n",
        "        # Flatten the list of word lists\n",
        "        flatten_words = [word for sentence in words for word in sentence]\n",
        "\n",
        "        # Convert each word to a list of its characters\n",
        "        character_lvl_words = [[ch for ch in word] for word in flatten_words]\n",
        "\n",
        "    # Progress bar for the outer loop\n",
        "    pbar = tqdm(range(num_iteration), desc=\"Outer loop progress\")\n",
        "\n",
        "    for _ in pbar:\n",
        "        # Count character pair frequencies\n",
        "        pairs = {}\n",
        "        for word in character_lvl_words:\n",
        "            for pair in zip(word[:-1], word[1:]):\n",
        "                pairs[pair] = pairs.get(pair, 0) + 1\n",
        "\n",
        "        best_prob = float('-inf')\n",
        "        best_pair = None\n",
        "        best_merged = None\n",
        "\n",
        "        # Get the most common pairs and limit iterations (e.g., 100 or fewer)\n",
        "        common_pairs = Counter(pairs).most_common(likeli_iteration)\n",
        "\n",
        "        # Create the inner progress bar only once, outside the outer loop\n",
        "        inner_pbar = tqdm(common_pairs, desc=\"Evaluating pairs\", leave=False)\n",
        "\n",
        "        for pair, freq in inner_pbar:\n",
        "            if freq > freq_limit:\n",
        "                pair_str = ' '.join(pair)\n",
        "                merged = merge(character_lvl_words, pair_str)\n",
        "                prob_dif = log_likelihood_entropy(character_lvl_words, merged) # We can change the function to `log_likelihood_sum`\n",
        "                if prob_dif > best_prob:\n",
        "                    best_prob = prob_dif\n",
        "                    best_pair = pair\n",
        "                    best_merged = merged\n",
        "\n",
        "            # Update the inner progress bar description with current progress\n",
        "            inner_pbar.set_postfix({\"Best pair\": best_pair, \"Δ log-likelihood\": f\"{best_prob:.4f}\"})\n",
        "\n",
        "        if best_pair is None:\n",
        "            break  # Stop if no good merges are found\n",
        "\n",
        "        character_lvl_words = best_merged\n",
        "        np.save('/content/WordPiececharacter_lvl_words.npy', np.array(character_lvl_words, dtype=object))\n",
        "        # !cp /content/WordPiececharacter_lvl_words.npy /content/drive/MyDrive/LLM/\n",
        "        pbar.set_description(f\"Best pair: {best_pair}, Δ log-likelihood: {best_prob:.4f}\")\n",
        "\n",
        "    # Apply WordPiece prefixes and return unique tokens\n",
        "    tokens = add_wordpiece_prefixes(character_lvl_words)\n",
        "    flat_tokens = [tok for word in tokens for tok in word]\n",
        "    # Add special characters to tokens\n",
        "    flat_tokens += ['[UNK]', '[CLS]', '[SEP]', '.'] # '[CLS]', '[SEP]' are not necessary, since I want train MLM (Maked language model)\n",
        "    return sorted(set(flat_tokens))"
      ],
      "metadata": {
        "id": "nURUb97w_cA4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = WordPiece(\n",
        "    text, num_iteration=1000-999,\n",
        "    likeli_iteration=30,\n",
        "    load_char_lvl='/content/drive/MyDrive/LLM/WordPiececharacter_lvl_words.npy'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d649b47aad68430e9ca6a7921b7da39b",
            "6eb320f21ef64a658000e8a060ba6a6d",
            "084dd31f8f18486caf534f37093e4550",
            "efffe083254249fd885d5120b5d7262c",
            "845b0a85dff2481493de73b6f1287503",
            "d73e2d5433db45d281a34737d87690f5",
            "c5ad8ee820db4cce8b85bcbb3ccd37cd",
            "30ecbc3dbb2c4a9094fc126be3a84435",
            "a7c88c8298814dfeb0ec33d21a472f70",
            "b903871bdae2452aa90e8eb6a20cb657",
            "b908df7b8a9c402f850f1931ce82cdc1",
            "d573e747bda34b6390f49f2d9b9f62da",
            "f211ab57e1aa4237ba9688d43fa23ae5",
            "3296a59b61db4ee1b9cc0b845ac5c910",
            "b1675158770f4e698653775fba335db9",
            "728470fb44ed430caf184149e7c8b913",
            "8ca425eda62c403b8aec80bef27beed1",
            "cb34591f5154480bb5c1696485960cf9",
            "f2ebd6bc2467488c9ad4ddef114443bb",
            "540cac17aa4b4314b12efad6c7a162da",
            "61e3506574e84ca290c379c5a9cf14f7",
            "707af1d1b0454ed3acb13df7f607fcf7"
          ]
        },
        "id": "GAfh-2SO_djb",
        "outputId": "2aca8e54-42ba-4d75-8c19-f79129c4fc2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Outer loop progress:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d649b47aad68430e9ca6a7921b7da39b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating pairs:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d573e747bda34b6390f49f2d9b9f62da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ix095fe_hZW",
        "outputId": "d76c300c-c0e5-4141-df9e-cf8e4080ea22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1757"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordpiece_tokenize(word, tokens_list, unk_token='[UNK]'):\n",
        "    # List to store the resulting tokens\n",
        "    tokeninzed = []\n",
        "    start = 0\n",
        "\n",
        "    # Continue until the entire word is processed\n",
        "    while start < len(word):\n",
        "        end = len(word)\n",
        "        matched = None\n",
        "\n",
        "        # Try to find the longest substring starting at 'start' that exists in the vocabulary\n",
        "        while start < end:\n",
        "            substr = word[start:end]\n",
        "\n",
        "            # For non-initial tokens, add WordPiece continuation prefix\n",
        "            if start > 0:\n",
        "                substr = \"##\" + substr\n",
        "\n",
        "            # Check if this substring is in the vocabulary\n",
        "            if substr in tokens_list:\n",
        "                matched = substr\n",
        "                break  # Found the longest match\n",
        "\n",
        "            # Shorten the substring by one character from the right\n",
        "            end -= 1\n",
        "\n",
        "        # If no match found, treat the entire word (or remaining part) as unknown\n",
        "        if matched is None:\n",
        "            tokeninzed.append(unk_token)\n",
        "            break\n",
        "\n",
        "        # Add matched token to result\n",
        "        tokeninzed.append(matched)\n",
        "\n",
        "        # Update the start position:\n",
        "        # If it's a continuation token (starts with ##), move forward by the number of characters matched (excluding ##)\n",
        "        # If it's the initial token, move by the full length of the token\n",
        "        start = end if matched.startswith(\"##\") else len(matched)\n",
        "\n",
        "    return tokeninzed"
      ],
      "metadata": {
        "id": "h8gmidgs_j5K"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the tokenizer\n",
        "for item in ['resolve', 'Resolve', 'speak', 'relieved', 'object', 'objection']:\n",
        "    tokenized = wordpiece_tokenize(item, tokens)\n",
        "    print(f\"{item:.^12} tokens are: {tokenized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAXKCsCL_lI1",
        "outputId": "26289d4a-7e49-4736-859c-dd3e58daffa6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..resolve... tokens are: ['r', '##e', '##so', '##l', '##ve']\n",
            "..Resolve... tokens are: ['Re', '##so', '##l', '##ve']\n",
            "...speak.... tokens are: ['speak']\n",
            "..relieved.. tokens are: ['r', '##el', '##ie', '##ve', '##d']\n",
            "...object... tokens are: ['o', '##b', '##ject']\n",
            ".objection.. tokens are: ['o', '##b', '##ject', '##ion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure that '[PAD]' has 0 index\n",
        "tokens.remove('[UNK]')\n",
        "for special in ['[UNK]', '[MASK]', '[PAD]']:\n",
        "        tokens.insert(0, special)"
      ],
      "metadata": {
        "id": "ORrWf3oX_sGc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pY0OHwU_9nS",
        "outputId": "e7510155-5fa2-41b6-e2b0-161ec5bdad94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]', '[MASK]', '[UNK]', '##$', \"##'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(tokens)} # String to int\n",
        "itos = {i:ch for i, ch in enumerate(tokens)} # Int to string\n",
        "\n",
        "decode_word_piece = lambda string, tokens: [stoi[item] if item in stoi else stoi['[UNK]']\n",
        "                                            for item in wordpiece_tokenize(string, tokens)]\n",
        "encode_word_piece = lambda ids: ''.join([tok if not tok.startswith('##') else tok[2:]\n",
        "                                         for tok in [itos[item] for item in ids]])"
      ],
      "metadata": {
        "id": "bqd7I8nsADLV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the decoder-encoder\n",
        "ids = decode_word_piece('hello!', tokens)\n",
        "print(ids, '-->', [itos[id] for id in ids])\n",
        "\n",
        "encode_word_piece(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "yVZks0d4AE5V",
        "outputId": "c9b43224-d9fb-4fe7-a235-202840a024e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1317, 466, 2] --> ['hel', '##lo', '[UNK]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello[UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, cleaned_text = clean_text(text)\n",
        "print(f'Length of data: {len(cleaned_text):^20}')\n",
        "print(f'Maxmum number of words in a sentence: {len(max(cleaned_text, key=len).split()):^20}')\n",
        "print(f'Minimum number of words in a sentence: {len(min(cleaned_text, key=len).split()):^20}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lx1K9N3AHMZ",
        "outputId": "e57dc886-a92d-4fd5-b96d-74bdb95daaba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:        10312        \n",
            "Maxmum number of words in a sentence:         192         \n",
            "Minimum number of words in a sentence:          4          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_poetic_text(text, max_words=40, min_words=10):\n",
        "    # Split text by strong punctuation first\n",
        "    clauses = re.split(r'(?<=[,;])\\s+', text)\n",
        "\n",
        "    segments = []\n",
        "    current = []\n",
        "\n",
        "    for clause in clauses:\n",
        "        current.append(clause.strip())\n",
        "        word_count = sum(len(part.split()) for part in current)\n",
        "\n",
        "        if word_count >= max_words or (\n",
        "            word_count >= min_words and clause.strip().endswith((',', ';'))\n",
        "        ):\n",
        "            segments.append(' '.join(current).strip())\n",
        "            current = []\n",
        "\n",
        "    # Add remaining\n",
        "    if current:\n",
        "        segments.append(' '.join(current).strip())\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "Q3c2zUtmAJpg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = []\n",
        "for item in cleaned_text:\n",
        "    item = item.replace('\\n', ' ')\n",
        "    if len(item.split()) > 50:\n",
        "        new_text.append(split_poetic_text(item)[0])\n",
        "    else:\n",
        "        new_text.append(item)"
      ],
      "metadata": {
        "id": "OMPeXpEZALkO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 90\n",
        "i = 0\n",
        "while True:\n",
        "    if len(new_text[i].split()) > max_len:\n",
        "        new_text[i] = new_text[i][:max_len]\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    merged  = new_text[i] + ' . ' + new_text[i+1]\n",
        "    if len(merged.split()) > max_len:\n",
        "        i += 1\n",
        "        continue\n",
        "\n",
        "    new_text[i] = merged\n",
        "    del new_text[i+1]\n",
        "\n",
        "    if i >= len(new_text)-2:\n",
        "        break\n",
        "\n",
        "new_text = new_text[:-1]"
      ],
      "metadata": {
        "id": "nSVPWpLlANXr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of data: {len(new_text):^20}')\n",
        "print(f'Maxmum number of words in a sentence: {len(max(new_text, key=len).split()):^20}')\n",
        "print(f'Minimum number of words in a sentence: {len(min(new_text, key=len).split()):^20}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIUz1blFAO1C",
        "outputId": "190365e9-ed62-46b2-e2fc-8a70b9fbea0f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:         1933        \n",
            "Maxmum number of words in a sentence:          87         \n",
            "Minimum number of words in a sentence:          40         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = []\n",
        "for i, item in (enumerate(new_text)):\n",
        "    item = [wordpiece_tokenize(i, tokens) for i in item.split()]\n",
        "    tokenized_text.append([i for i in item for i in i])"
      ],
      "metadata": {
        "id": "jbAD4VdTAQ0c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of data: {len(tokenized_text):^20}')\n",
        "print(f'Maxmum number of tokens in a sentence: {len(max(tokenized_text, key=len)):^20}')\n",
        "print(f'Minimum number of tokens in a sentence: {len(min(tokenized_text, key=len)):^20}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhqGDW0YATTt",
        "outputId": "c5581f8f-d97d-4744-e6a7-e527a246b044"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:         1933        \n",
            "Maxmum number of tokens in a sentence:         218         \n",
            "Minimum number of tokens in a sentence:          73         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do2fzrETAV-Q",
        "outputId": "53d8ec49-d1f6-4712-cb7a-b7037cdd34ff"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Be', '##fore', 'we', 'pro', '##ce', '##ed', 'any', 'fur', '##t', '##her,']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wRNL4yPAX4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}