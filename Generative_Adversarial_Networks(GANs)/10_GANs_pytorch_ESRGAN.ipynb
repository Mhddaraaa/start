{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33Wc74yu3Lwd"
   },
   "source": [
    "# **Generative Adversarial Networks(GANs)**\n",
    "<img align='right' width='800' src=\"https://cdn-images-1.medium.com/v2/resize:fit:851/0*pPEL7ryJR51VpnDO.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jauDzu0V3I6S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **work with kaggle**\n",
    "- First you have to go to your profile and creat API token which will download kaggle.json to your pc\n",
    "- Now drag and drop json file to your colab files tab\n",
    "- Run the codes below\n",
    "    - ```\n",
    "    !mkdir /root/.kaggle\n",
    "    !mv kaggle.json /root/.kaggle/kaggle.json\n",
    "    !chmod 600 /root/.kaggle/kaggle.json\n",
    "    !kaggle datasets list\n",
    "    ```\n",
    "- To download the datasts to your colab open the desire competition(datasets) and click on `three dot` on the upper-right then `copy API command`\n",
    "\n",
    "-  for more information [click here](https://www.kaggle.com/discussions/general/74235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8UrJEK4jP6o",
    "outputId": "ea766591-1793-4950-a339-6f3891ca4167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n"
     ]
    }
   ],
   "source": [
    "# !rm -r /root/.kaggle\n",
    "!mkdir /root/.kaggle\n",
    "!mv kaggle.json /root/.kaggle/kaggle.json\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHZcFiSYjimu",
    "outputId": "45c48270-d7f1-404d-907b-8b9a7dab68c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading celeba-dataset.zip to /content\n",
      "100% 1.33G/1.33G [00:35<00:00, 42.7MB/s]\n",
      "100% 1.33G/1.33G [00:35<00:00, 39.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G8ttEGCj9ga"
   },
   "outputs": [],
   "source": [
    "!unzip /content/celeba-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLdhm6bG3cSU",
    "outputId": "3109613a-0520-43ef-aecb-3b7fa1a88d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Availabe device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCH = 20\n",
    "LR = 2e-4\n",
    "BS = 32\n",
    "C, H, W = 3, 24, 24\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Availabe device is: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "99HkBvJI3c8v"
   },
   "outputs": [],
   "source": [
    "# Visioalize the data\n",
    "def show(tensor, ch=C, size=(H, W), num_to_display=16):\n",
    "    \"\"\"\n",
    "    Inputs would be tensor with (batch_size, channel, height, weight) dimention\n",
    "    First we detach() tensor so because it's not require grade any more,\n",
    "    Then send it to cpu() to make sure the tensor doesn't on different device\n",
    "    Matplotlib show images in (height, width, channel) dimention so the images permute to match the criteria\n",
    "    \"\"\"\n",
    "    images = tensor.detach().cpu().view(-1, ch, *size)\n",
    "    grid = make_grid(images[:num_to_display], nrow=4, normalize=True).permute(1, 2, 0)\n",
    "    plt.axis(False)\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DALjL_4s41WZ"
   },
   "source": [
    "##**ESRGAN architecture**\n",
    "\n",
    "<img align='center' width='1200' src=\"https://esrgan.readthedocs.io/en/latest/_images/architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_5aVbMDH4d2y"
   },
   "outputs": [],
   "source": [
    "class convBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, use_act=True, use_bn=False, discriminator=False, **kw):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, **kw, bias= not discriminator)\n",
    "        self.bn = nn.BatchNorm2d(out_channel) if discriminator else nn.Identity()\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True) if use_act else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "class denseBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, channels=32, use_act=True, beta=0.2, **kw):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.conv = nn.ModuleList()\n",
    "        for i in range(5):\n",
    "            use_act, ch_out = (False, in_channel) if i == 4 else (True, channels)\n",
    "            self.conv.append(convBlock(\n",
    "                in_channel + i*channels, ch_out, use_act, kernel_size=3, stride=1, padding=1\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        new = x\n",
    "        for conv in self.conv:\n",
    "            out = conv(new)\n",
    "            new = torch.cat([new, out], dim=1)\n",
    "        # In basicBlock residual layers output multiply by 0.2\n",
    "        out = self.beta * out + x\n",
    "        return out\n",
    "\n",
    "class basicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, beta=0.2):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.conv =nn.Sequential(*[denseBlock(in_channel) for _ in range(3)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.beta * self.conv(x) + x\n",
    "\n",
    "\n",
    "class upSample(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, up_factor=2):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=up_factor, mode='nearest') # 'nearest', 'linear', 'bilinear', 'bicubic' ,'trilinear'. Default: 'nearest'\n",
    "        self.conv = nn.Conv2d(in_channel, in_channel, 3, 1, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(self.upsample(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "izeTirkJJo3G",
    "outputId": "5e5dcff4-ddf5-4851-9ccf-32b4486ff679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> input shape: torch.Size([10, 3, 24, 24])\n",
      " --> Generator output size: torch.Size([10, 3, 96, 96])\n",
      " --> Discriminator output size: torch.Size([10, 1])\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Every thing is O.K'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel=3, h_channel=64, n_block=23):\n",
    "        super().__init__()\n",
    "        self.initial = convBlock(in_channel, h_channel, kernel_size=3, stride=1, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *[basicBlock(h_channel) for _ in range(n_block)]\n",
    "        )\n",
    "        self.conv = nn.Conv2d(h_channel, h_channel, kernel_size=3, stride=1, padding=1)\n",
    "        self.up_sample = nn.Sequential(\n",
    "            upSample(h_channel),\n",
    "            upSample(h_channel)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            convBlock(h_channel, h_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(h_channel, in_channel, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        initial = self.initial(x)\n",
    "        x = self.resblocks(initial)\n",
    "        x = self.conv(x) + initial\n",
    "        x = self.up_sample(x)\n",
    "\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_c=3, h_feature=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
    "        super().__init__()\n",
    "        conv_blocks = []\n",
    "        for i, feature in enumerate(h_feature):\n",
    "            layer = convBlock(\n",
    "                in_c,\n",
    "                feature,\n",
    "                use_bn=False if i == 0 else True,\n",
    "                discriminator=True,\n",
    "                kernel_size=3,\n",
    "                stride=1 if i%2 == 0 else 2,\n",
    "                padding=1\n",
    "            )\n",
    "\n",
    "            conv_blocks.append(layer)\n",
    "            in_c = feature\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            *conv_blocks,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*6*6, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "\n",
    "def test_models(device=device):\n",
    "    x = torch.rand(10, 3, 24, 24).to(device)\n",
    "    out = Generator().to(device)(x)\n",
    "    pred = Discriminator().to(device)(out)\n",
    "    print(f\" --> input shape: {x.shape}\\n --> Generator output size: {out.shape}\\n --> Discriminator output size: {pred.shape}\\n\")\n",
    "    return \"Every thing is O.K\"\n",
    "\n",
    "test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rYBu6x423koK"
   },
   "outputs": [],
   "source": [
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "#     elif classname.find('Line') != -1:\n",
    "#         nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "#         nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def weights_init(m, scale=0.1):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "        m.weight.data *= scale\n",
    "\n",
    "    elif classname.find('Line') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "        m.weight.data *= scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tqMjgcbQAjq"
   },
   "source": [
    "## **Perceptual Loss**\n",
    "\n",
    "They also develop a more effective perceptual loss $ \\mathcal L_{percep}$ by constraining on fea- tures before activation rather than after activation as practiced in SRGAN..\n",
    "\n",
    "$$\n",
    "    \\large \\mathcal {L}_{G} = \\mathcal {L}_{percep} + \\lambda \\mathcal {L}^{Ra}_{Gen} + \\eta \\mathcal {L}_{1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\hspace{20pt} \\lambda = 5*10^{-3}$\n",
    "\n",
    "$\\hspace{20pt} \\eta = 10^{-2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Perceptual Loss:**\n",
    "the paper‚Äôs authors decide to go with the loss of several VGG layers. The ReLU activation layers of the pre-trained 19-layer VGG network act as the foundation for this VGG loss which is the Euclidean distance between feature representations.\n",
    "\n",
    "$$\n",
    "    \\large \\mathcal {L}_{percep} = \\frac{1}{W_{i,j}H_{i,j}} \\sum^{W_{i,j}}_{x} \\sum^{H_{i,j}}_{y} \\left(\\phi_{i,j}(I^{HR}_{x,y}) - \\phi_{i,j}(I^{LR}_{x,y}) \\right)^2\n",
    "$$\n",
    "\n",
    "With $\\phi_{i,j}$ we indicate the feature map obtained by the j-th convolution\n",
    "(after activation) before the i-th maxpooling layer within the\n",
    "VGG19 network\n",
    "\n",
    "Here $W_{i,j}$ and $H_{i,j}$ describe the dimensions of the respective feature maps within the VGG network.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Adversarial Loss(Relativistic):**\n",
    "\n",
    "- When label is 1 (for real ones):\n",
    "\n",
    "$$\n",
    "    \\large D_{Ra}(x_r, x_f)  =\\ frac{1}{n}\\sum_{i=1}^{n}{(\\log(\\hat{ùö¢_i}))} - [\\mathbb{E}(D(I^{HR}_{x,y})]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- When label is 0 (for generated images):\n",
    "\n",
    "$$\n",
    "    \\large D_{Ra}(x_f, x_r)  = \\frac{1}{n}\\sum_{i=1}^{n}{(\\log(1- \\hat{ùö¢_i}))} - [\\mathbb{E}(D(G(I^{LR}_{x,y}))]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "    \\hspace{20pt} \\mathbb{E}(D(G(I^{LR}_{x,y})) &=  \\overline{D(G(I^{LR}_{x,y}))} \\\\\n",
    "    \\hspace{20pt} \\mathbb{E}(D(I^{HR}_{x,y}) &=  \\overline{D(I^{HR}_{x,y})}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\large \\mathcal {L}^{Ra}_{Gen} = D_{Ra}(x_r, x_f) + D_{Ra}(x_f, x_r)\n",
    "$$\n",
    "\n",
    "- **content loss:**\n",
    " evaluate the 1-norm distance between recovered image $G(I^{LR}_{x,y})$ and the ground-truth $I^{HR}_{x,y}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\large mathcal {L}_{1} = \\mathbb{E}_{x_i} \\|G(I^{LR}_{x,y}) - I^{HR}_{x,y}\\|\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "[paper](https://arxiv.org/abs/1609.04802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0lF5FhjLlAu",
    "outputId": "83b20906-9f75-4aef-faf0-ded582d0d326"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:09<00:00, 61.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1114, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class vggPartial(nn.Module):\n",
    "\n",
    "    def __init__(self, device=device):\n",
    "        super().__init__()\n",
    "        self.vgg = torchvision.models.vgg19(weights='VGG19_Weights.IMAGENET1K_V1').to(device)\n",
    "        self.vgg = self.vgg.eval()\n",
    "        self.vgg.features[34].register_forward_hook(self._hook)\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        self.out = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.vgg(x)\n",
    "        return self.out\n",
    "\n",
    "class vggLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, device=device):\n",
    "        super().__init__()\n",
    "        self.vgg = vggPartial()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.vgg(x)\n",
    "        y = self.vgg(y)\n",
    "        return F.mse_loss(x, y)\n",
    "\n",
    "\n",
    "def test_vggLoss(device=device):\n",
    "    loss = vggLoss()\n",
    "    x = torch.rand(1, 3, 24, 24).to(device)\n",
    "    y = torch.rand(1, 3, 96, 96).to(device)\n",
    "    out = Generator().to(device)(x)\n",
    "    return loss(out, y)\n",
    "\n",
    "test_vggLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lNsKkdJPLsaJ"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\"\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=512'\n",
    "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rqptprz5Rqf1"
   },
   "outputs": [],
   "source": [
    "def gen_loss_func(gen_net, disc_net, loss_func, l_res, h_res, mode=None):\n",
    "\n",
    "    fake = gen_net(l_res)\n",
    "\n",
    "    if mode == 'First_run':\n",
    "        content_loss = nn.MSELoss()\n",
    "        return content_loss(fake, h_res)\n",
    "\n",
    "    pred = disc_net(fake)\n",
    "    regular_loss = loss_func(pred, torch.ones_like(pred))\n",
    "    perceptual_loss = vggLoss()\n",
    "    vgg_loss = perceptual_loss(fake, h_res)\n",
    "    content_loss = nn.L1Loss()\n",
    "    # 1e-3 * regular_loss + 6e-3 * vgg_loss\n",
    "    return vgg_loss + 5e-3 * regular_loss + 1e-2 * content_loss(fake, h_res)\n",
    "\n",
    "\n",
    "\n",
    "def disc_loss_func(gen_net, disc_net, loss_func, l_res, h_res, mode='relativistic'):\n",
    "    fake = gen_net(l_res)\n",
    "    fake_pred = disc_net(fake.detach())  # detach() the generator output so it won't participate in gen_net learning\n",
    "    real_pred = disc_net(h_res)\n",
    "\n",
    "\n",
    "    if mode == 'relativistic':\n",
    "        loss_real = loss_func(real_pred - fake_pred.mean(0, keepdim=True), torch.ones_like(real_pred))\n",
    "        loss_fake = loss_func(fake_pred - real_pred.mean(0, keepdim=True), torch.zeros_like(fake_pred))\n",
    "    else:\n",
    "        loss_real = loss_func(real_pred, torch.ones_like(real_pred))\n",
    "        loss_fake = loss_func(fake_pred, torch.zeros_like(fake_pred))\n",
    "\n",
    "    return (loss_real + loss_fake) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ba2mJjGOcFI7"
   },
   "outputs": [],
   "source": [
    "class dataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, high_res=96):\n",
    "        super().__init__()\n",
    "        low_res = high_res // 4\n",
    "        self.high_transform = transforms.Compose([\n",
    "            transforms.Resize((high_res, high_res)),\n",
    "            transforms.Normalize([0, 0, 0],[1, 1, 1]),\n",
    "            # transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.low_transform = transforms.Compose([\n",
    "            transforms.Resize((low_res, low_res)),\n",
    "            transforms.Normalize([0, 0, 0],[1, 1, 1]),\n",
    "            # transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.both_transform = transforms.Compose([\n",
    "            transforms.RandomCrop((high_res, high_res)),\n",
    "            # transforms.RandomHorizontalFlip(p=0.5),\n",
    "            # transforms.RandomRotation(90),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.data = data(root='data',\n",
    "                         download=True,\n",
    "                         transform=self.both_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, _ = self.data[idx]\n",
    "        high = self.high_transform(item)\n",
    "        low = self.low_transform(item)\n",
    "\n",
    "        return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3SZNgarjxoP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class KaggleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, high_res=96):\n",
    "        super().__init__()\n",
    "        low_res = high_res // 4\n",
    "        self.dir_path = dir_path\n",
    "\n",
    "        self.high_transform = transforms.Compose([\n",
    "            transforms.Resize((high_res, high_res)),\n",
    "            transforms.Normalize([0, 0, 0],[1, 1, 1])\n",
    "        ])\n",
    "\n",
    "        self.low_transform = transforms.Compose([\n",
    "            transforms.Resize((low_res, low_res)),\n",
    "            transforms.Normalize([0, 0, 0],[1, 1, 1])\n",
    "        ])\n",
    "\n",
    "        self.both_transform = transforms.Compose([\n",
    "            transforms.RandomCrop((high_res, high_res)),\n",
    "            # transforms.RandomHorizontalFlip(p=0.5),\n",
    "            # transforms.RandomRotation(90),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.images = []\n",
    "\n",
    "        for item in tqdm(os.listdir(self.dir_path)[:12800]):\n",
    "            img = Image.open(os.path.join(self.dir_path, item))\n",
    "            img = self.both_transform(img)\n",
    "            high = self.high_transform(img)\n",
    "            low = self.low_transform(img)\n",
    "            self.images.append((low, high))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low, high = self.images[idx]\n",
    "\n",
    "        # img_name = os.listdir(self.dir_path)[idx]\n",
    "        # img = Image.open(os.path.join(self.dir_path, img_name))\n",
    "\n",
    "        # img = self.both_transform(img)\n",
    "        # high = self.high_transform(img)\n",
    "        # low = self.low_transform(img)\n",
    "\n",
    "        return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "7cc86851cb5145a29d296a924f7f4c27",
      "b75ddb39f6494ef29a5bf869ae557f87",
      "7978e5c8657643d8a69870899fe223c9",
      "d8c0a353505842869abe7a7daaf0892e",
      "8435bb4d814b43fdb2622eeb45f10bbb",
      "ad85e296c5f64303abee306feb30e796",
      "72a1a20ba8694316a1c32e77c0ee1c1d",
      "1cfee4eb61f7453a82a8ccb3e18434f3",
      "2cee215b0b314ba1a3bc01c4e7a0fb4f",
      "e5dc8b057ad94da3aa4a57c61a340cd9",
      "269e3b546679404b88edd25619177e05"
     ]
    },
    "id": "5qPGN5Ukc_ut",
    "outputId": "63687b15-b405-4b95-b224-e44264adcc1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc86851cb5145a29d296a924f7f4c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# data = dataSet(torchvision.datasets.LFWPeople)\n",
    "path = \"img_align_celeba/img_align_celeba\"\n",
    "data = KaggleDataset(path)\n",
    "loader = torch.utils.data.DataLoader(data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wrwvJTjefB0"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))\n",
    "show(x, size=(24, 24))\n",
    "show(y, size=(96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvcU09u9fnWa",
    "outputId": "fa4ca96f-0139-4874-877c-bd640fcc7332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/runs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -r /content/runs\n",
    "writer = SummaryWriter(\"/content/runs\")\n",
    "writer_fake = SummaryWriter(\"/content/runs/fake\")\n",
    "writer_l = SummaryWriter(\"/content/runs/l_res\")\n",
    "writer_h = SummaryWriter(\"/content/runs/h_res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlTD2BO63-Qs"
   },
   "outputs": [],
   "source": [
    "# !kill 5081\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BepyEh2W3_T0"
   },
   "outputs": [],
   "source": [
    "# !rm -r $PATH\n",
    "PATH = \"/content/model/\"\n",
    "!mkdir $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-UUAmT6j4A2k"
   },
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "gen.apply(weights_init)\n",
    "# gen.load_state_dict(torch.load('/content/ESRGAN_Gen_1', map_location=device))\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "gen_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(gen_opt, step_size=1, gamma=0.8)\n",
    "\n",
    "disc = Discriminator().to(device)\n",
    "disc.apply(weights_init)\n",
    "# critic.load_state_dict(torch.load('/content/disc_20'))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=4e-4, betas=(0.5, 0.999))\n",
    "disc_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(disc_opt, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCtWfz8q4BPn"
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    discLoss, genLoss = 0, 0\n",
    "    print(f\"\\nEpoch: {epoch + 1}\")\n",
    "\n",
    "    for batch,  (l_res, h_res) in enumerate(tqdm(loader)):\n",
    "        l_res = l_res.to(device)\n",
    "        h_res = h_res.to(device)\n",
    "\n",
    "        # disc_opt.zero_grad()\n",
    "        # disc_loss = disc_loss_func(gen, disc, loss_func, l_res, h_res, mode='relativistic')\n",
    "        # disc_loss.backward() # If False, the graph used to compute the grad will be freed, Actually It isnt necessary\n",
    "        # disc_opt.step()\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        # At first use MSE loss then use paper loss\n",
    "        gen_loss = gen_loss_func(gen, disc, loss_func, l_res, h_res, mode='First_run')\n",
    "        # gen_loss = gen_loss_func(gen, disc, loss_func, content_loss, l_res, h_res)\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # discLoss += disc_loss /len(data)\n",
    "        genLoss += gen_loss /len(data)\n",
    "        if batch % 30 == 0 and batch != 0:\n",
    "            with torch.no_grad():\n",
    "                step += 1\n",
    "                fake = gen(l_res)\n",
    "                image = h_res.view(-1, 3, H, W)\n",
    "                h_res_grid = make_grid(h_res[:32], normalize=True)\n",
    "                l_res_grid = make_grid(l_res[:32], normalize=True)\n",
    "                fake_grid = make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"fake image\", fake_grid, global_step=step\n",
    "                )\n",
    "                writer_l.add_image(\n",
    "                    \"Low_res image\", l_res_grid, global_step=step\n",
    "                )\n",
    "                writer_h.add_image(\n",
    "                    \"High_res image\", h_res_grid, global_step=step\n",
    "                )\n",
    "\n",
    "        writer.add_scalars(\"Loss\", {\n",
    "                    \"Critic\": 0,\n",
    "                    \"Generator\": gen_loss\n",
    "                }, (epoch+1)*batch)\n",
    "\n",
    "    print(f'  Discriminator Loss: {discLoss:.4f} -- Generator Loss: {genLoss:.4f}')\n",
    "    gen_exp_lr_scheduler.step()\n",
    "    disc_exp_lr_scheduler.step()\n",
    "\n",
    "    #Save model\n",
    "    torch.save(gen.state_dict(), f\"{PATH}Gen_{epoch+1}\")\n",
    "    torch.save(disc.state_dict(), f\"{PATH}disc_{epoch+1}\")\n",
    "\n",
    "    if (epoch + 1) % 2 == 0 and epoch > 0:\n",
    "        print(f\"  >>> Discriminator Learning Rate: {disc_opt.param_groups[0]['lr']}\")\n",
    "        print(f\"  >>> Generator Learning Rate: {gen_opt.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_1da6nfWYah"
   },
   "outputs": [],
   "source": [
    "# !cp /content/model/Gen_3\n",
    "# ! cp /content/ESRGAN_Gen_1 /content/ESRGAN_Gen_1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "In17anZ34MPi"
   },
   "outputs": [],
   "source": [
    "# gen.load_state_dict(torch.load('/content/ESRGAN_Gen_1'))\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "for i, (l, h) in enumerate(loader):\n",
    "    generated = gen(l.to(device))\n",
    "    generated = (generated - generated.min()) / generated.max()\n",
    "    fig.add_subplot(3, 5, i+1)\n",
    "    plt.imshow(generated[0].detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('Generated')\n",
    "    fig.add_subplot(3, 5, i+6)\n",
    "    plt.imshow(l[0].detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('Low res')\n",
    "    fig.add_subplot(3, 5, i+11)\n",
    "    plt.imshow(h[0].detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('HIgh res')\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwxlZ1cSp1DV"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "for i, (l, h) in enumerate(loader):\n",
    "    generated = gen(l.to(device))\n",
    "    fig.add_subplot(3, 5, i+1)\n",
    "    plt.imshow(((generated[0]+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('Generated')\n",
    "    fig.add_subplot(3, 5, i+6)\n",
    "    plt.imshow(((l[0]+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('Low res')\n",
    "    fig.add_subplot(3, 5, i+11)\n",
    "    plt.imshow(((h[0]+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "    plt.axis(False)\n",
    "    plt.title('HIgh res')\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuXAwKperX0p",
    "outputId": "bea767f5-200b-43c9-90ae-75c90247c79f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((l[0]+1)/2).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3Ye6zclo8Yq"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = np.array(Image.open('/content/img.jpg'))\n",
    "img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "l = (torchvision.transforms.Resize((24, 24), antialias=True)(img) - 122.5) / 122.5\n",
    "h = (torchvision.transforms.Resize((96, 96), antialias=True)(img) - 122.5) / 122.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EB7Pv7O15G5"
   },
   "outputs": [],
   "source": [
    "gen.load_state_dict(torch.load('/content/drive/MyDrive/Gans_models/SRGAN_Gen_MSE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d62TSSgN0Bbz"
   },
   "outputs": [],
   "source": [
    "l_res = l.unsqueeze(0).to(device)\n",
    "h_res = h.unsqueeze(0).to(device)\n",
    "\n",
    "i = 0\n",
    "while i <100:\n",
    "    i += 1\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss = gen_loss_func(gen, disc, loss_func, content_loss, l_res, h_res)\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6864D9aCsFBh"
   },
   "outputs": [],
   "source": [
    "generated = gen(l.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5asxH35Q1xWw"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(((generated[0]+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('Generated')\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(((l+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('Low res')\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(((h+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('HIgh res')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftrCkPGXtnOp"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(((generated[0]+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('Generated')\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(((l+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('Low res')\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(((h+1)/2).detach().cpu().permute(1, 2 ,0))\n",
    "plt.axis(False)\n",
    "plt.title('HIgh res')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GOXJvg_vTkw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1cfee4eb61f7453a82a8ccb3e18434f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "269e3b546679404b88edd25619177e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2cee215b0b314ba1a3bc01c4e7a0fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72a1a20ba8694316a1c32e77c0ee1c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7978e5c8657643d8a69870899fe223c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cfee4eb61f7453a82a8ccb3e18434f3",
      "max": 12800,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cee215b0b314ba1a3bc01c4e7a0fb4f",
      "value": 12800
     }
    },
    "7cc86851cb5145a29d296a924f7f4c27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b75ddb39f6494ef29a5bf869ae557f87",
       "IPY_MODEL_7978e5c8657643d8a69870899fe223c9",
       "IPY_MODEL_d8c0a353505842869abe7a7daaf0892e"
      ],
      "layout": "IPY_MODEL_8435bb4d814b43fdb2622eeb45f10bbb"
     }
    },
    "8435bb4d814b43fdb2622eeb45f10bbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad85e296c5f64303abee306feb30e796": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b75ddb39f6494ef29a5bf869ae557f87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad85e296c5f64303abee306feb30e796",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_72a1a20ba8694316a1c32e77c0ee1c1d",
      "value": "100%"
     }
    },
    "d8c0a353505842869abe7a7daaf0892e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5dc8b057ad94da3aa4a57c61a340cd9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_269e3b546679404b88edd25619177e05",
      "value": "‚Äá12800/12800‚Äá[00:27&lt;00:00,‚Äá398.40it/s]"
     }
    },
    "e5dc8b057ad94da3aa4a57c61a340cd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
