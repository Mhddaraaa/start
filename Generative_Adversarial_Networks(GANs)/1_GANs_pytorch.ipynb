{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BimOnXEkudn"
      },
      "source": [
        "# **Generative Adversarial Networks(GANs)**\n",
        "<img align='right' width='800' src=\"https://cdn-images-1.medium.com/v2/resize:fit:851/0*pPEL7ryJR51VpnDO.jpg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LxX-JnAN9nb0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e0sKi5Ub-W6m"
      },
      "outputs": [],
      "source": [
        "# Visioalize the data\n",
        "def show(tensor, ch=1, size=(28, 28), num_to_display=16):\n",
        "    \"\"\"\n",
        "    Inputs would be tensor with (batch_size, channel, height, weight) dimention\n",
        "    First we detach() tensor so because it's not require grade any more,\n",
        "    Then send it to cpu() to make sure the tensor doesn't on different device\n",
        "    Matplotlib show images in (height, width, channel) dimention so the images permute to match the criteria\n",
        "    \"\"\"\n",
        "    images = tensor.detach().cpu().view(-1, ch, *size)\n",
        "    grid = make_grid(images[:num_to_display], nrow=4).permute(1, 2, 0)\n",
        "    plt.axis(False)\n",
        "    plt.imshow(grid)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OoAz0BTm9p9V"
      },
      "outputs": [],
      "source": [
        "def get_data(data=MNIST, bs=128):\n",
        "    \"\"\"\n",
        "    From torchvision.datasets we can get different dataset\n",
        "    for training GANs we don't need test datasets just trian sets will be enough\n",
        "    \"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    train_set = MNIST('.',\n",
        "                      train=True,\n",
        "                      transform=ToTensor(),\n",
        "                      download=True)\n",
        "\n",
        "    # group the data in different batch size\n",
        "    data_loader = DataLoader(train_set, bs, shuffle=True)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-vz_-87Ag7",
        "outputId": "fb6a52b1-dca6-435a-c96f-68573c698fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Availabe device is:  cuda\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "EPOCH = 50\n",
        "Z_DIM = 100\n",
        "LR = 1e-4\n",
        "BS = 128\n",
        "\n",
        "data = get_data(MNIST, BS)\n",
        "C, H, W = next(iter(data))[0][0].shape\n",
        "loss_func = nn.BCELoss()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Availabe device is: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFJJMbF8nJ4B"
      },
      "source": [
        "## **Discriminator and Generator Networkrs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8pC2OWzi6n81"
      },
      "outputs": [],
      "source": [
        "#Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=100, hidden_dim=128, out_dim=28*28):\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            self._gen_block(z_dim, hidden_dim),\n",
        "            self._gen_block(hidden_dim, hidden_dim*2),\n",
        "            self._gen_block(hidden_dim*2, hidden_dim*4),\n",
        "            self._gen_block(hidden_dim*4, hidden_dim*8),\n",
        "            self._gen_block(hidden_dim*8, out_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, nois):\n",
        "        return self.gen(nois)\n",
        "\n",
        "    def _gen_block(self, in_dim, out):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_dim, out),\n",
        "            nn.BatchNorm1d(out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "#Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim=28*28, hidden_dim=128, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self._disc_block(in_dim, hidden_dim*4),\n",
        "            self._disc_block(hidden_dim*4, hidden_dim*2),\n",
        "            self._disc_block(hidden_dim*2, hidden_dim),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1*28*28) # Flatten input data to a Linear layer\n",
        "        return self.disc(x)\n",
        "\n",
        "    def _disc_block(self, in_dim, out):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_dim, out),\n",
        "            nn.BatchNorm1d(out),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "n166tyLBAtgY"
      },
      "outputs": [],
      "source": [
        "gen_noise = lambda number, z_dim: torch.randn(number, z_dim).to(device)\n",
        "\n",
        "gen = Generator().to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=LR)\n",
        "#If we need to change learning rate during training we can use lr_scheduler function in torch.optim\n",
        "gen_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(gen_opt, step_size=10, gamma=0.5)\n",
        "\n",
        "disc = Discriminator().to(device)\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=LR)\n",
        "disc_exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(disc_opt, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic1f8katBEbX"
      },
      "outputs": [],
      "source": [
        "#Check the data\n",
        "x, y = next(iter(data))\n",
        "print(\"Shape of dataset images: \", x.shape)\n",
        "print(\"label of images\", y[:16])\n",
        "\n",
        "noise = gen_noise(BS, Z_DIM)\n",
        "fake = gen(noise)\n",
        "show(x, ch=C, size=(H, W))\n",
        "show(fake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNTBiqHBZ1XZ"
      },
      "source": [
        "## **BCELoss**\n",
        "$$\n",
        "    \\large -\\frac{1}{n}\\sum_{i=1}^{n}{-[y_i\\log(ùö¢^{ÃÇ}_i) + (1-y_i)\\log(1-y^{ÃÇ}_i)]}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Generator Loss**\\\n",
        "<br>\n",
        "Generator wants to fool the Discriminator so in loss calculation we consider **one** matrix to calculate the loss so the **BCE** formular will be:\n",
        "\n",
        "$$\n",
        "    \\frac{1}{n}\\sum_{i=1}^{n}{(\\log(ùö¢^{ÃÇ}_i))}\n",
        "$$\n",
        "\n",
        "where the $y^ÃÇ$ is the last output of the Discriminator:\n",
        "\n",
        "$$\n",
        "   \\large ùíÅ ‚ü∂ Generator \\xrightarrow[\\text{}]{\\text{G(z)}} Discriminator \\xrightarrow[\\text{}]{\\text{D(G(z))}} -\\frac{1}{n}\\sum_{i=1}^{n}{(\\log(D(G(z))))}\n",
        "$$\n",
        "\n",
        "**Discriminator Loss**\n",
        "<br>\n",
        "**one** matrix to calculate the loss for **real** input and **zeors** matrix to calculate the loss for **fake** input which is the output of generator so the **BCE** formular will be:\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\text{When label is 1 (for real ones)}:$\n",
        "$$\n",
        "    \\frac{1}{n}\\sum_{i=1}^{n}{(\\log(ùö¢^{ÃÇ}_i))}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\text{When label is 0 (for generated images)}:$\n",
        "$$\n",
        "    \\frac{1}{n}\\sum_{i=1}^{n}{(\\log(1- ùö¢^{ÃÇ}_i))}\n",
        "$$\n",
        "\n",
        "where the $y^ÃÇ$ is the last output of the Discriminator:\n",
        "\n",
        "$$\n",
        "    \\large -\\frac{1}{n}\\sum_{i=1}^{n}{-[\\log(D(x)) + \\log(1 - D(G(z)))]}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2esxTiPWDQSt"
      },
      "outputs": [],
      "source": [
        "#This the the most important part of the training\n",
        "def gen_loss_func(gen_net, disc_net, loss_func, num, z_dim):\n",
        "    noise = gen_noise(num, z_dim)\n",
        "    fake = gen_net(noise)\n",
        "    pred = disc_net(fake)\n",
        "    real = torch.ones_like(pred)\n",
        "    return loss_func(pred, real)\n",
        "\n",
        "\n",
        "def disc_loss_func(gen_net, disc_net, loss_func, image, num, z_dim):\n",
        "    noise = gen_noise(num, z_dim)\n",
        "    fake = gen_net(noise)\n",
        "    fake_pred = disc_net(fake.detach())  # detach() the generator output so it won't participate in gen_net learning\n",
        "    real_pred = disc_net(image)\n",
        "\n",
        "    loss_real = loss_func(real_pred, torch.ones_like(real_pred))\n",
        "    loss_fake = loss_func(fake_pred, torch.zeros_like(fake_pred))\n",
        "\n",
        "    return (loss_real + loss_fake) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-_yNA6v3TXE9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "!rm -r /content/runs\n",
        "writer = SummaryWriter(\"/content/runs\")\n",
        "writer_fake = SummaryWriter(\"/content/runs/fake\")\n",
        "writer_real = SummaryWriter(\"/content/runs/real\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCFEe-UwUDS-"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs\n",
        "# %reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMoMG6MvLSWx"
      },
      "outputs": [],
      "source": [
        "step = 0\n",
        "for epoch in range(EPOCH):\n",
        "    mean_gen_loss = 0\n",
        "    mean_disc_loss = 0\n",
        "    print(f\"\\nEpoch: {epoch + 1}\")\n",
        "\n",
        "    for batch, (real, _) in enumerate(tqdm(data)):\n",
        "        real = real.to(device)\n",
        "\n",
        "        disc_loss = disc_loss_func(gen, disc, loss_func, real, BS, Z_DIM)\n",
        "        disc_opt.zero_grad()\n",
        "        disc_loss.backward(retain_graph=True) # If False, the graph used to compute the grad will be freed, Actually It isnt necessary\n",
        "        disc_opt.step()\n",
        "\n",
        "        gen_loss = gen_loss_func(gen, disc, loss_func, BS, Z_DIM)\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "        if batch % 150 == 0 and batch != 0:\n",
        "            # print(f'  batch: {batch} -- {batch*32} of {len(data.dataset)} sample')\n",
        "            with torch.no_grad():\n",
        "                step += 1\n",
        "                fake = gen(gen_noise(BS, Z_DIM)).view(-1, C, H, W)\n",
        "                image = real.view(-1, C, H, W)\n",
        "                real_grid = make_grid(image[:24], normalize=True)\n",
        "                fake_grid = make_grid(fake[:24], normalize=True)\n",
        "\n",
        "                writer_fake.add_image(\n",
        "                    \"MNIST fake image\", fake_grid, global_step=step\n",
        "                )\n",
        "                writer_real.add_image(\n",
        "                    \"MNIST real image\", real_grid, global_step=step\n",
        "                )\n",
        "\n",
        "\n",
        "    disc_loss /= len(data)\n",
        "    gen_loss /= len(data)\n",
        "    print(f'  Discriminator Loss: {disc_loss:.4f} -- Generator Loss: {gen_loss:.4f}')\n",
        "    gen_exp_lr_scheduler.step()\n",
        "    disc_exp_lr_scheduler.step()\n",
        "\n",
        "    writer.add_scalar(\"Loss/Disc\", disc_loss, epoch)\n",
        "    writer.add_scalar(\"Loss/Gen\", gen_loss, epoch)\n",
        "    if epoch % 10 == 0 and epoch > 0:\n",
        "        print(f\"  >>> Discriminator Learning Rate: {disc_opt.param_groups[0]['lr']}\")\n",
        "        print(f\"  >>> Generator Learning Rate: {gen_opt.param_groups[0]['lr']}\")\n",
        "        # show(gen(gen_noise((BS, Z_DIM))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRVD-zsLQYvT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
