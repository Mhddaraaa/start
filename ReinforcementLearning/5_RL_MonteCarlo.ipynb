{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning**\n",
        "<img align=\"right\" src=\"https://vitalflux.com/wp-content/uploads/2020/12/Reinforcement-learning-real-world-example.png\">\n",
        "\n",
        "- In reinforcement learning, your system learns how to interact intuitively with the environment by basically doing stuff and watching what happens.\n",
        "\n",
        "if you need the last version of gym use block of code below:\n",
        "```\n",
        "!pip uninstall gym -y\n",
        "!pip install gym\n",
        "```"
      ],
      "metadata": {
        "id": "-zm7cpKOfwiP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMm-R68qfqwz"
      },
      "outputs": [],
      "source": [
        "# !pip install -U gym==0.25.2\n",
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from IPython.core.display import HTML\n",
        "from base64 import b64encode\n",
        "from gym.wrappers import record_video, record_episode_statistics\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MJI0LUTUf0di"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(episode=0, video_width=600, video_dir= \"/content/video\"):\n",
        "    video_path = os.path.join(video_dir, f\"rl-video-episode-{episode}.mp4\")\n",
        "    video_file = open(video_path, \"rb\").read()\n",
        "    decoded = b64encode(video_file).decode()\n",
        "    video_url = f\"data:video/mp4;base64,{decoded}\"\n",
        "    return HTML(f\"\"\"<video width=\"{video_width}\"\" controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "def create_env(name, render_mode=None, video_folder='/content/video'):\n",
        "    # render mode: \"human\", \"rgb_array\", \"ansi\")\n",
        "    env = gym.make(name, new_step_api=True, render_mode=render_mode)\n",
        "    env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: x % 50 == 0)\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "    return env\n",
        "\n",
        "def show_reward(total_rewards):\n",
        "    plt.plot(total_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d_YOHy86f4Fj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Monte Carlo Learning**\n",
        "\n",
        "In the previous Notebook, we evaluated and solved a **Markov Decision Process (MDP)** using **dynamic programming (DP)**. **Model-based** methods such as DP have some drawbacks. They require the environment to be fully known, including the transition matrix and reward matrix. They also have limited scalability, especially for environments with plenty of states\n",
        "\n",
        "**model-free approach**, the Monte Carlo (MC) methods, which have no requirement of prior knowledge of the environment and are much more scalable than DP.\n",
        "\n",
        "The term **Monte Carlo** is often used more broadly for any estimation method. Monte Carlo methods require only experience, sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.\n",
        "\n",
        "It is a method for estimating Value-action(Value|State, Action) or Value function(Value|State) using some sample runs from the environment for which we are estimating the Value function\n",
        "\n",
        "<br>\n",
        "\n",
        "- **types of Monte Carlo learning:**\n",
        "1. $\\textit{First Visit Monte Carlo}$: First visit estimates (Value|State: S1) as the average of the returns following the first visit to the state S1\n",
        "2. $\\textit{Every Visit Monte Carlo}$: It estimates (Value|State: S1) as the average of returns for every visit to the State S1.\n",
        "- **Example:**\n",
        "\n",
        "$$\n",
        "\\textit{First iteration: }A+3 \\rightarrow A+2 \\rightarrow B-4 \\rightarrow A+4 \\rightarrow\n",
        "B-3 \\rightarrow terminated \\\\\n",
        "\\textit{SEcond iteration: }B-2 \\rightarrow A+3 \\rightarrow B-3 \\rightarrow terminated\n",
        "$$\n",
        "<br>\n",
        "\n",
        "<table width='1200'>\n",
        "    <tr>\n",
        "        <td><font color=\"Red\" size='4'>$$\\textit{First Visit}$$<td>\n",
        "        <td><font color=\"Red\" size='4'>$$\\textit{V(a)}$$<td>\n",
        "        <td><font color=\"Red\" size='4'>$$\\textit{V(b)}$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"Olive\" size='4'> $$\\textit{First iteraion}$$<td>\n",
        "        <td><font color=\"Olive\" size='4'>$$3+2-4+4-3=2$$<td>\n",
        "        <td><font color=\"Olive\" size='4'>$$-4+4–3=-3$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"Blue\" size='4'>$$\\textit{Second iteraion}$$<td>\n",
        "        <td><font color=\"Blue\" size='4'>$$3-3=0$$<td>\n",
        "        <td><font color=\"Blue\" size='4'>$$-2+3+-3=-2$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"black\" size='4'>$$\\textit{Sum}$$<td>\n",
        "        <td><font color=\"black\" size='4'>$$\\frac{2+0}{2}=1$$<td>\n",
        "        <td><font color=\"black\" size='4'>$$\\frac{-3-2}{2}=-2.5$$<td>\n",
        "    <tr>\n",
        "<table>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<table width='1200'>\n",
        "    <tr>\n",
        "        <td><font color=\"red\" size='4'>$$\\textit{Every Visit}$$<td>\n",
        "        <td><font color=\"red\" size='4'>$$\\textit{V(a)}$$<td>\n",
        "        <td><font color=\"red\" size='4'>$$\\textit{V(b)}$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"olive\" size='4'> $$\\textit{First iteraion}$$<td>\n",
        "        <td><font color=\"olive\" size='4'>$$(3+2-4+4-3)\\\\+(2-4+4-3)\\\\+(4-3)\\\\=2-1+1$$<td>\n",
        "        <td><font color=\"olive\" size='4'>$$(-4+4-3)+(-3)=-3+-3$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"blue\" size='4'>$$\\textit{Second iteraion}$$<td>\n",
        "        <td><font color=\"blue\" size='4'>$$3-3=0$$<td>\n",
        "        <td><font color=\"blue\" size='4'>$$(-2+3–3)+(-3)=-2+-3$$<td>\n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td><font color=\"black\" size='4'>$$\\textit{Sum}$$<td>\n",
        "        <td><font color=\"black\" size='4'>$$\\frac{2+-1+1+0}{4}=0.5$$<td>\n",
        "        <td><font color=\"black\"size='4'>$$\\frac{-3+-3+-2-3}{4}=-2.75$$<td>\n",
        "    <tr>\n",
        "<table>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note**:As we have been given 2 different iterations, we will be summing all the rewards coming after A (including that of A) after the first visit to ‘A’. It must be noted that if an episode doesn’t have an occurence of ‘A’, it won’t be considered in the average.\n"
      ],
      "metadata": {
        "id": "AYaKD2MWPn06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performing Monte Carlo policy evaluation**\n",
        "\n",
        "- A reinforcement learning algorithm that needs a known MDP is categorized as a **model-based algorithm**.\n",
        "- On the other hand, one with no requirement of prior knowledge of transitions and rewards is called a **model-free algorithm**. Monte Carlo-based reinforcement learning is a model-free approach.\n",
        "\n",
        "we will evaluate the value function using the Monte Carlo method. assuming we don't have access to both of environment transition and reward matrices. You will recall that the returns of a process, which are the total rewards over the long run, are as follows:\n",
        "\n",
        "$$\n",
        "\\large G_t=\\sum_{k}^{\\infty}γ^kR_{t+k+1}\n",
        "$$\n",
        "\n",
        "MC policy evaluation uses **empirical mean return** instead of **expected return** (as in DP) to estimate the value function.\n",
        "\n",
        "- **Note**: in the Monte Carlo setting, we need to keep track of the states and rewards for all steps, since we don't have access to the full environment, including the transition probabilities and reward matrix.\n",
        "\n",
        "<center><img width=\"500\" src=\"https://i.stack.imgur.com/Q8YCg.png\">"
      ],
      "metadata": {
        "id": "GWtWNQy6dZ0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "id": "zXF0Wh-OPnQz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, policy):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        action = policy[state].item()\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "    states = torch.tensor(states)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    return states, rewards"
      ],
      "metadata": {
        "id": "7EZZwLHhe1ek"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_first_visit(env, policy, gamma, n_episode):\n",
        "    n_state = env.observation_space.n\n",
        "    V = torch.zeros(n_state)\n",
        "    N = torch.zeros(n_state)\n",
        "    for episode in range(n_episode):\n",
        "        states, rewards = run_episode(env, policy)\n",
        "        G = torch.zeros(n_state)\n",
        "        first_visit = torch.zeros(n_state)\n",
        "        return_t = 0\n",
        "        for state_t, reward_t in zip(reversed(states[1:]), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t] = return_t\n",
        "            first_visit[state_t] = 1\n",
        "        for state in range(n_state):\n",
        "            if first_visit[state] > 0:\n",
        "                V[state] += G[state]\n",
        "                N[state] += 1\n",
        "\n",
        "    for state in range(n_state):\n",
        "        if N[state] > 0:\n",
        "            V[state] = V[state] / N[state]\n",
        "    return V\n",
        "\n",
        "\n",
        "def mc_every_visit(env, policy, gamma, n_episode):\n",
        "    n_state = env.observation_space.n\n",
        "    V = torch.zeros(n_state)\n",
        "    N = torch.zeros(n_state)\n",
        "    G = torch.zeros(n_state)\n",
        "    for episode in range(n_episode):\n",
        "        states, rewards = run_episode(env, policy)\n",
        "        return_t = 0\n",
        "        for state_t, reward_t in zip(reversed(states[1:]), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t] = return_t\n",
        "            N[state_t] += 1\n",
        "\n",
        "    for state in range(n_state):\n",
        "        if N[state] > 0:\n",
        "            V[state] = G[state] / N[state]\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "jqBdSrCbfPF3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1\n",
        "# use policy from previous notebook\n",
        "policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.]).long()\n",
        "n_episode = 1000\n",
        "\n",
        "first_visit = mc_first_visit(env, policy, gamma, n_episode)\n",
        "print(f\"The value function calculated by first visit MC:\\n{first_visit}\\n \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHrIphHSltA-",
        "outputId": "9a08f43f-425b-4632-db56-a4a3db0d286f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value function calculated by first visit MC:\n",
            "tensor([0.7110, 0.4236, 0.4150, 0.3333, 0.7280, 0.0000, 0.3483, 0.0000, 0.7280,\n",
            "        0.7295, 0.6433, 0.0000, 0.0000, 0.7789, 0.8761, 1.0000])\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "every_visit = mc_every_visit(env, policy, gamma, n_episode)\n",
        "print(f\"The value function calculated by every visit MC:\\n{first_visit}\\n \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xna34oRmnd42",
        "outputId": "e8fef1b0-80fc-44bc-a3a4-4d2687f6e087"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value function calculated by every visit MC:\n",
            "tensor([0.7110, 0.4236, 0.4150, 0.3333, 0.7280, 0.0000, 0.3483, 0.0000, 0.7280,\n",
            "        0.7295, 0.6433, 0.0000, 0.0000, 0.7789, 0.8761, 1.0000])\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BlackJack**\n",
        "<img align='right' width='400' src=\"https://www.gymlibrary.dev/_images/blackjack.gif\">\n",
        "\n",
        "Card Values:\n",
        "\n",
        "- Face cards (Jack, Queen, King) have a point value of 10.\n",
        "- Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
        "- Numerical cards (2-9) have a value equal to their number.\n",
        "\n",
        "Action Space:\n",
        "\n",
        "- There are two actions: stick (0), and hit (1).\n",
        "\n",
        "Observation Space:\n",
        "\n",
        "- The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1)."
      ],
      "metadata": {
        "id": "g8ICzxLVuOZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"Blackjack-v1\")"
      ],
      "metadata": {
        "id": "MxExVnWEtY8A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def run_episode(env, hold_score):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        action = 1 if state[0] < hold_score else 0\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "    states = torch.tensor(states)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    return states, rewards\n",
        "\n",
        "def mc_first_visit_blackjack(env, hold_score, gamma, n_episode):\n",
        "    V = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    for episode in range(n_episode):\n",
        "        states, rewards = run_episode(env, hold_score)\n",
        "        G = {}\n",
        "        return_t = 0\n",
        "        for state_t, reward_t in zip(reversed(states[1:]), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t] = return_t\n",
        "\n",
        "        for state, return_t in G.items():\n",
        "            if state[0] <= 21:\n",
        "                V[state] += return_t\n",
        "                N[state] += 1\n",
        "\n",
        "    for state in V:\n",
        "        V[state] = V[state] / N[state]\n",
        "    return V"
      ],
      "metadata": {
        "id": "fFn66GSPt9Hc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"Blackjack-v1\", render_mode = \"human\")\n",
        "hold_score = 18\n",
        "gamma = 1\n",
        "n_episode = 500\n",
        "value = mc_first_visit_blackjack(env, hold_score, gamma, n_episode)"
      ],
      "metadata": {
        "id": "XklmJAoquGFs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciiYFhQavxcO",
        "outputId": "7affeea1-fc74-43ed-a29f-80539401a1e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "647"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performing on-policy Monte Carlo control**\n",
        "\n",
        "Monte Carlo prediction is used to evaluate the value for a given policy, while Monte Carlo control (MC control) is for finding the optimal policy when such a policy is not given. There are basically categories of MC control: **on-policy** and **off-policy**.\n",
        "- On-policy methods learn about the optimal policy by executing the policy and evaluating and improving it\n",
        "- off-policy methods learn about the optimal policy using data generated by another policy.\n",
        "\n",
        "**Note:** The way on-policy MC control works is quite similar to policy iteration in dynamic programming, which has two phases, evaluation and improvement:\n",
        "\n",
        "- **In the evaluation phase**, instead of evaluating the value function (also called the state value, or utility), it evaluates the action-value. The action-value is more frequently called the **Q-function**, which is the utility of a state-action pair (s, a) by taking action a in state s under a given policy. Again, the evaluation can be conducted in a first-visit manner or an every-visit manner.\n",
        "\n",
        "- **In the improvement phase**, the policy is updated by assigning the optimal action to each state:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\large\\pi(s) = argmax_(a)Q(s, a)\n",
        "$$"
      ],
      "metadata": {
        "id": "uIOnX54h8s-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def run_episode(env, Q, n_action):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions =[]\n",
        "    states = []\n",
        "    is_done = False\n",
        "    action = torch.randint(0, n_action, [1]).item()\n",
        "    while not is_done:\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, info= env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "        action = torch.argmax(Q[state]).item()\n",
        "    return states, actions, rewards\n",
        "\n",
        "def mc_on_policy(env, gamma, n_episode):\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
        "    for episode in range(n_episode):\n",
        "        if episode % 100 == 0 and episode > 0:\n",
        "            print(f\"-- Episode: {episode}\")\n",
        "\n",
        "        states, actions, rewards = run_episode(env, Q, n_action)\n",
        "        G = {}\n",
        "        return_t = 0\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t, action_t] = return_t\n",
        "\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:\n",
        "                G_sum[state_action] += return_t\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        policy[state] = torch.argmax(actions).item()\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "F8g-vYMT4x9C"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"Blackjack-v1\", render_mode = \"human\")\n",
        "gamma = 1\n",
        "n_episode = 500\n",
        "optimal_Q, optimal_policy = mc_on_policy(env, gamma, n_episode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpwbzRXUAyVL",
        "outputId": "491142de-f771-4d4a-e4ed-a970d3c8f448"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Episode: 100\n",
            "-- Episode: 200\n",
            "-- Episode: 300\n",
            "-- Episode: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_value = defaultdict(float)\n",
        "\n",
        "for state, action_values in optimal_Q.items():\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "print(optimal_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LhRfthkBGHI",
        "outputId": "d1eb2d24-a5c5-4ca7-eb69-252703815770"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'float'>, {(19, 10, False): 0.0, (18, 10, False): 0.375, (18, 3, False): 0.0, (17, 4, False): 0.4000000059604645, (12, 4, False): 0.5, (16, 6, False): 1.0, (18, 4, True): 0.0, (15, 5, False): 0.5, (15, 5, True): 1.0, (12, 10, False): 0.0, (16, 7, False): 1.0, (17, 10, False): -1.0, (5, 1, False): 4.4373517171309657e-41, (17, 2, True): 0.0, (6, 2, False): -1.1774649297931685e-24, (14, 10, False): -0.3333333432674408, (21, 10, True): 1.0, (17, 7, False): -1.0, (12, 7, False): 1.0, (20, 10, False): 0.4285714328289032, (16, 3, False): -1.0, (16, 3, True): 1.0, (20, 2, False): 1.0, (13, 2, False): 0.0, (16, 2, False): -1.0, (18, 8, False): -1.0, (12, 6, False): 0.0, (20, 5, False): 0.5, (21, 5, True): 0.5, (5, 10, False): 0.0, (16, 5, False): 1.0, (19, 5, True): -1.0, (9, 3, False): 4.4373517171309657e-41, (6, 3, False): 3.6552925943597346e+21, (17, 1, False): -1.0, (19, 2, False): 1.0, (14, 2, False): 1.0, (20, 3, False): 0.3333333432674408, (17, 3, False): -1.0, (21, 8, True): 1.0, (19, 6, False): 0.0, (20, 10, True): 0.0, (16, 10, False): -0.5, (15, 2, False): 0.0, (12, 2, False): 0.0, (8, 10, False): -0.3333333432674408, (13, 8, False): -1.0, (14, 3, False): -0.3333333432674408, (20, 3, True): 1.0, (19, 1, True): 0.0, (14, 1, True): 0.0, (17, 9, False): 0.0, (10, 10, False): 0.5, (14, 7, False): -1.0, (19, 9, False): 0.0, (17, 10, True): 0.25, (16, 2, True): 0.0, (8, 6, False): 1.0, (20, 7, False): 1.0, (13, 7, False): 0.0, (13, 10, False): -0.6000000238418579, (14, 6, False): 1.0, (18, 5, False): 0.5, (18, 6, True): 1.0, (13, 6, True): 1.0, (11, 10, False): -0.3333333432674408, (15, 1, False): 0.0, (12, 9, False): -1.0, (8, 9, False): 0.0, (20, 8, True): 1.0, (21, 10, False): 1.0, (20, 6, False): 1.0, (19, 9, True): 1.0, (11, 2, False): 0.0, (11, 1, False): 0.0, (14, 8, False): -1.0, (12, 8, False): -0.6666666865348816, (18, 1, False): -1.0, (7, 2, False): 0.0, (18, 6, False): 0.0, (8, 1, False): 7.006492321624085e-45, (15, 10, False): -0.7142857313156128, (21, 6, False): 0.6666666865348816, (21, 6, True): 0.0, (9, 2, False): 4.4373517171309657e-41, (9, 9, False): 3.210374781768156e-41, (14, 4, False): 0.0, (18, 4, False): 1.0, (21, 4, True): 1.0, (21, 1, True): 0.0, (18, 1, True): 0.0, (13, 5, False): 0.0, (20, 4, False): 1.0, (19, 7, False): 1.0, (21, 7, False): 1.0, (11, 7, False): 0.3333333432674408, (6, 10, False): -1.0, (13, 6, False): 0.3333333432674408, (19, 8, False): -0.3333333432674408, (16, 8, False): 1.0, (9, 6, False): 1.0, (20, 9, False): 1.0, (20, 9, True): 1.0, (19, 5, False): 0.5, (11, 4, False): 0.0, (21, 2, True): 1.0, (14, 9, True): 0.0, (13, 1, False): 1.0, (18, 7, False): 1.0, (21, 9, True): 1.0, (16, 4, False): -1.0, (21, 5, False): 0.6000000238418579, (9, 4, False): 1.0, (18, 9, False): -0.3333333432674408, (14, 5, False): 1.0, (14, 6, True): 1.0, (7, 10, False): -1.0, (14, 9, False): -1.0, (9, 5, False): 1.0, (19, 3, False): 0.0, (11, 3, False): 8.110539459687732e+34, (20, 1, False): 0.0, (7, 7, False): 1.0, (12, 5, False): -1.0, (16, 1, False): -1.0, (19, 10, True): 1.0, (9, 10, False): 0.3333333432674408, (11, 6, False): 1.0, (20, 8, False): 0.6666666865348816, (9, 8, False): 8.110539459687732e+34, (10, 1, False): 0.0, (18, 2, False): 0.0, (7, 3, False): 1.0, (16, 5, True): -1.0, (15, 7, False): 0.0, (18, 7, True): 0.0, (14, 1, False): -0.3333333432674408, (12, 1, False): 0.0, (20, 7, True): 8.110539459687732e+34, (13, 4, False): -1.0, (17, 2, False): 1.0, (16, 4, True): 4.4373517171309657e-41, (15, 4, True): -1.0, (11, 9, False): 1.0, (6, 6, False): 1.0, (10, 2, False): 1.0, (19, 1, False): 1.0, (21, 2, False): 0.0, (8, 7, False): -1.0, (21, 4, False): 1.0, (15, 8, False): 0.0, (21, 8, False): 0.5, (17, 9, True): 1.0, (15, 8, True): 0.0, (11, 8, False): 1.0, (17, 8, False): -1.0, (13, 9, False): -0.3333333432674408, (18, 2, True): 1.0, (13, 2, True): 1.0, (10, 3, False): 0.0, (16, 9, False): 4.4373517171309657e-41, (15, 9, True): 1.0, (11, 5, False): 0.5, (15, 2, True): 0.0, (5, 2, False): 0.0, (14, 10, True): 4.4373517171309657e-41, (15, 6, False): 0.5, (13, 4, True): 4.4373517171309657e-41, (7, 5, False): 4.4373517171309657e-41, (12, 3, False): 4.4373517171309657e-41, (12, 3, True): 0.0, (7, 1, False): 0.0, (4, 5, False): 4.4373517171309657e-41, (15, 3, False): 0.0, (17, 7, True): 8.110539459687732e+34, (20, 5, True): 1.0, (19, 2, True): 1.0, (7, 4, False): 0.0, (17, 5, True): 1.0, (15, 4, False): 0.0, (16, 10, True): 0.0, (15, 9, False): 1.0, (20, 1, True): 1.0, (13, 1, True): 1.0, (15, 3, True): 0.0, (4, 3, False): 0.0, (6, 4, False): 8.110539459687732e+34, (19, 4, False): 4.4373517171309657e-41, (19, 4, True): 0.0, (21, 9, False): 1.0, (7, 9, False): 8.1106107650339945e+34, (13, 10, True): -1.0, (21, 7, True): 1.0, (9, 7, False): 0.0, (13, 7, True): 0.0, (17, 6, False): 1.0, (20, 4, True): 1.0, (8, 8, False): 0.0, (21, 1, False): 1.0, (18, 3, True): 0.0, (6, 8, False): 0.0, (8, 3, False): 0.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_episode(env, policy):\n",
        "     state = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "        if state in policy:\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            action = torch.randint(2, [1]).item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            return reward"
      ],
      "metadata": {
        "id": "SlmCjlLgDRYq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win = 0\n",
        "for i in range(100):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"-- {i}\")\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)"
      ],
      "metadata": {
        "id": "fVBuWkl9IX8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z79PMVESNLbc",
        "outputId": "222248e5-01a3-4442-b99f-2bcf5157d708"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35 times win which means 35.00 % winning chanse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Developing MC control with epsilon-greedy policy**\n",
        "\n",
        "In MC control with **epsilon-greedy** policy, we no longer exploit the best action all the time, but choose an action randomly under certain probabilities. As the name implies, the algorithm has two folds:\n",
        "\n",
        "Epsilon: given a parameter, $ε$, with a value from 0 to 1, each action is taken with a probability calculated as follows:\n",
        "\n",
        "$$\n",
        "\\large \\pi(s, a) = \\frac{ε}{|A|}\n",
        "$$\n",
        "- Here, |A| is the number of possible actions.\n",
        "\n",
        "Greedy: the action with the highest state-action value is favored, and its probability of being chosen is increased by $1-ε$:\n",
        "\n",
        "$$\n",
        "\\large \\pi(s, a) = 1 - ε + \\frac{ε}{|A|}\n",
        "$$\n",
        "\n",
        "Epsilon-greedy policy exploits the best action most of the time and also keeps exploring different actions from time to time.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img width=\"600\" src=\"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-5b10393cf0c6395ae5fb22260220c574_l3.svg\">"
      ],
      "metadata": {
        "id": "PjYDmV-pMCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def take_action(state, Q, epsilon, n_action):\n",
        "    p = np.random.random()\n",
        "    if p < epsilon:\n",
        "        return torch.randint(0, n_action, (1,)).item()\n",
        "    else:\n",
        "        return torch.argmax(Q[state]).item()\n",
        "\n",
        "\n",
        "def take_action2(state, Q, epsilon, n_action):\n",
        "    probs = torch.ones(n_action) * epsilon / n_action\n",
        "    best_action = np.argmax(Q[state])\n",
        "    probs[best_action] += (1.0 - epsilon)\n",
        "    action = torch.multinomial(probs, 1).item()\n",
        "    return action\n",
        "\n",
        "def run_episode(env, Q, epsilon, n_action):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions =[]\n",
        "    states = []\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        action = take_action2(state, Q, epsilon, n_action)\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, info= env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "        action = torch.argmax(Q[state]).item()\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "uSopckQvMD7f"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_epsilon_greedy(env, gamma, n_episode, epsilon):\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
        "    for episode in range(n_episode):\n",
        "        if episode % 100 == 0 and episode > 0:\n",
        "            print(f\"-- Episode: {episode}\")\n",
        "\n",
        "        states, actions, rewards = run_episode(env, Q, epsilon, n_action)\n",
        "        G = {}\n",
        "        return_t = 0\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t, action_t] = return_t\n",
        "\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:\n",
        "                G_sum[state_action] += return_t\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        policy[state] = torch.argmax(actions).item()\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "t0PHmRRToGl_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"Blackjack-v1\", render_mode = \"human\")\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n_episode = 500\n",
        "optimal_Q, optimal_policy = mc_epsilon_greedy(env, gamma, n_episode, epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Vjy-UTos7F",
        "outputId": "e365ce82-8fc8-4ec1-f681-3376c7790685"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Episode: 100\n",
            "-- Episode: 200\n",
            "-- Episode: 300\n",
            "-- Episode: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_episode(env, policy):\n",
        "     state = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "        if state in policy:\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            action = torch.randint(2, [1]).item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            return reward\n",
        "\n",
        "win = 0\n",
        "for i in range(100):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"-- {i}\")\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)"
      ],
      "metadata": {
        "id": "pyZ0Cky8ooQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d9JkLvCqt1s",
        "outputId": "5ac2c970-1f65-414b-ce88-b5c22bdf3ba9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39 times win which means 39.00 % winning chanse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performing off-policy Monte Carlo control**\n",
        "\n",
        "The off-policy method optimizes the **target policy**, $\\pi$, using data generated by another policy, called the **behavior policy**, $b$. The target policy performs **exploitation** all the time while the behavior policy is for **exploration** purposes. This means that the target policy is greedy with respect to its current Q-function, and the behavior policy generates behavior so that the target policy has data to learn from.\n",
        "\n",
        "We start with the latest step whose action taken under the behavior policy is different from the action taken under the greedy policy. And to learn about the target policy with another policy, we use a technique called **importance sampling**, which is commonly used to estimate the expected value under a distribution, given samples generated from a different distribution. The weighted importance for a state-action pair is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\omega_t= \\sum_{k=t}\\frac{\\pi(a_k|s_k)}{b{a_k|s_k}}$$\n",
        "\n",
        "- Here, $π(a_k | s_k)$ is the probability of taking action $a_k$ in state $s_k$ under the target policy; $b(a_k | s_k)$ is the probability under the behavior policy; and the weight, $w_t$, is the multiplication of ratios between those two probabilities from step $t$ to the end of the episode. The weight, $w_t$, is applied to the return at step $t$."
      ],
      "metadata": {
        "id": "L_vvvsb1qzhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def creat_random_policy(n_action):\n",
        "    probs = torch.ones(n_action) / n_action\n",
        "    def policy_fn(observation):\n",
        "        return probs\n",
        "    return policy_fn\n",
        "\n",
        "def run_episode(env, random_policy):\n",
        "    state = env.reset()\n",
        "    rewards = []\n",
        "    actions =[]\n",
        "    states = []\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        probs = random_policy(state)\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "        state, reward, is_done, info= env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "qG6C-Ph7zkJs"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_off_policy(env, gamma, n_episode, epsilon, behavior_policy):\n",
        "    n_action = env.action_space.n\n",
        "    G_sum = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
        "    for episode in range(n_episode):\n",
        "        w = 1\n",
        "        if episode % 100 == 0 and episode > 0:\n",
        "            print(f\"-- Episode: {episode}\")\n",
        "\n",
        "        states, actions, rewards = run_episode(env, behavior_policy)\n",
        "        G = {}\n",
        "        return_t = 0\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t, action_t] = return_t\n",
        "            if action_t != torch.argmax(Q[state_t]).item():\n",
        "                break\n",
        "\n",
        "            w *= 1. / behavior_policy(state_t)[action_t]\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:\n",
        "                G_sum[state_action] += return_t * w\n",
        "                N[state_action] += 1\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        policy[state] = torch.argmax(actions).item()\n",
        "    return Q, policy"
      ],
      "metadata": {
        "id": "eplUPMxHq1Ra"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env(\"Blackjack-v1\", render_mode = \"human\")\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n_episode = 500\n",
        "random_policy = creat_random_policy(env.action_space.n)\n",
        "optimal_Q, optimal_policy = mc_off_policy(env, gamma, n_episode, epsilon, random_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riOlcFho3yI0",
        "outputId": "36ace1c4-da4c-4473-e00c-6a294dcca3c2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Episode: 100\n",
            "-- Episode: 200\n",
            "-- Episode: 300\n",
            "-- Episode: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_episode(env, policy):\n",
        "     state = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "        if state in policy:\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            action = torch.randint(2, [1]).item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            return reward\n",
        "\n",
        "win = 0\n",
        "for i in range(100):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"-- {i}\")\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)"
      ],
      "metadata": {
        "id": "BKJiz1_43-Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmTc3PiC5pY0",
        "outputId": "ce476489-90ae-4838-f707-b5a2df12c2af"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41 times win which means 41.00 % winning chanse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wa_UNvpN69hd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}