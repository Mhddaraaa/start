{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning**\n",
        "<img align=\"right\" src=\"https://vitalflux.com/wp-content/uploads/2020/12/Reinforcement-learning-real-world-example.png\">\n",
        "\n",
        "- In reinforcement learning, your system learns how to interact intuitively with the environment by basically doing stuff and watching what happens.\n",
        "\n",
        "if you need the last version of gym use block of code below:\n",
        "```\n",
        "!pip uninstall gym -y\n",
        "!pip install gym\n",
        "```"
      ],
      "metadata": {
        "id": "S3pPXb372sNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP82hohi2bRT"
      },
      "outputs": [],
      "source": [
        "# !pip install -U gym==0.25.2\n",
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from IPython.core.display import HTML\n",
        "from base64 import b64encode\n",
        "from gym.wrappers import record_video, record_episode_statistics\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "import torch\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "VC8vzsao2rmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(episode=0, video_width=600, video_dir= \"/content/video\"):\n",
        "\n",
        "    video_path = os.path.join(video_dir, f\"rl-video-episode-{episode}.mp4\")\n",
        "    video_file = open(video_path, \"rb\").read()\n",
        "    decoded = b64encode(video_file).decode()\n",
        "    video_url = f\"data:video/mp4;base64,{decoded}\"\n",
        "    return HTML(f\"\"\"<video width=\"{video_width}\"\" controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "def create_env(name, render_mode=None, video_folder='/content/video'):\n",
        "    # render mode: \"human\", \"rgb_array\", \"ansi\")\n",
        "    env = gym.make(name, new_step_api=True, render_mode=render_mode)\n",
        "    env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda x: x % 50 == 0)\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "    return env\n",
        "\n",
        "def show_reward(total_rewards):\n",
        "    plt.plot(total_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t0ptWCSA2zrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the Markov Decision Process (MDP)**\n",
        "\n",
        "\n",
        "1. **Markov Chains(MC)**\n",
        "\n",
        "Future is Independent of the past, knowing the present (present state at time $t$) makes the future (future state at time $t+1$) independent of the past (all past states at time $0 , 1, …, t-1$), which means The state at time ($t+1$) has no dependence on the states before time ($t$).\n",
        "\n",
        "Here we consider transition probability from one state to another not thier rewards.\n",
        "\n",
        "2. **Markov Reward Processes(MRP)**\n",
        "We can calculate the value of a state $v(s)$, which is the cumulative reward that the agent gets when it is in state $S=s$ at time $t$ and it follows the dynamics of the system.\n",
        "\n",
        "$$\n",
        "    \\large v(s) = \\mathbb{E} \\left[G_t | S_t=s \\right]\n",
        "$$\n",
        "\n",
        "> Where $G_t =R_{t+1}+γR_{t+2}+γ^{2}R_{t+3}+...$\n",
        "\n",
        "The expectation operator $E[•]$ is used to derive formulae and to prove theoretical results. However, in practice, it is replaced by averages over many sample simulations and is also known as **Monte Carlo simulation**.\n",
        "\n",
        "<br>\n",
        "\n",
        "3. **Markov decision processes(MDP)**\n",
        "Now we add “action.” In MRP. the agent had no control on the outcome, it learns everything by interacting with environment. However, under the MDP regime, the agent can choose actions based on the current state/observation. over time the agent can learn to take actions that maximize the cumulative reward ($G_t$).\n",
        "\n",
        "&nbsp; &nbsp; In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming.\n",
        "\n",
        "**Transition** : Moving from one state to another is called Transition.\n",
        "\n",
        "**Transition Probability (T)**: The probability that the agent will move from one state to another.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img align='right' width='400' src=\"https://miro.medium.com/v2/resize:fit:860/format:webp/1*MBcie302iU3qbQPbhU0psw.png\">\n",
        "\n",
        "&nbsp; &nbsp; The edges of the tree denote transition probability. From this chain let’s take some sample.\n",
        "\n",
        "&nbsp; &nbsp; Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream.\n",
        "\n",
        "- you can see transition matrix below (each row for one state):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0.2 & 0.6 & 0.2\\\\\n",
        "0.1 & 0.6 & 0.3 \\\\\n",
        "0.2 & 0.7 & 0.1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "- **The return Gt is the total discounted reward from time-step $t$ (discounting the rewards from the future to present)**\n",
        "\n",
        "$\\large G_t =R_{t+1}+γR_{t+2}+...=\\sum_{k=0}^{\\infty}γ^kR_{t+k+1}$\n",
        "\n",
        "**Note:** discounting is always introduced in continuing tasks and is optional in episodic tasks.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img align='left' width='350' src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*p5KQnP1rwTcXFooMF0n-TA.png\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Suppose our start state is Class 2,\n",
        "Class 2 > Class 3 > Pass > Sleep.\n",
        "<br>\n",
        "$γ = 0.5 \\\\\n",
        "G_t = -2 + (-2 * 0.5) + (10 * 0.25) +0 = -0.5$"
      ],
      "metadata": {
        "id": "I4fPPCpw22D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Policies and Value Functions**\n",
        "\n",
        "the agent gets feedback from the environment by rewards. The dynamics of MDP are defined as $p(s’ ,r | s, a)$. alongisde the $G_t$, which is the sum total of all rewards received from time.\n",
        "\n",
        "<br>\n",
        "\n",
        "The transition dynamics is outside the agent control. The agent, however, can control the decision, which means take action in a particular state. The agent does so with an objective to maximize the $G_t$ for each state $S_t$.\n",
        "\n",
        "The mapping of states to actions is known as **policy**:\n",
        "\n",
        "$$\n",
        "    \\pi (a|s)\n",
        "$$\n",
        "\n",
        "So policy is the probability of taking action $a$ at time $t$ when the agent is in state $s$ at time t. The agent objective is to learn the mapping function from the state to actions to maximize $G_t$.\n",
        "\n",
        "- **stochastic** policies: there are multiple actions that the agent can take, and the probability of taking each such action is defined by $π(a| s)$.\n",
        "- **deterministic** policies where there is only one unique action for the state\n",
        "\n",
        "<br>\n",
        "\n",
        "The value function is always defined in the context of the policy the agent is following. It is also referred to as the agent’s behavior.\n",
        "\n",
        "$$\n",
        "    \\large v_{\\pi}(s) = \\mathbb{E_{\\pi}} \\left[G_t | S_t=s \\right]\n",
        "$$\n",
        "\n",
        "> Where $v_π(s)$ specifies the “state value” of state $s$ when the agent is following a policy $π$. The value of $G_t$ is dependent on the trajectory of states that the agent will see after time $t$.\n",
        "\n",
        "in practice we usually calculate these values using simulation. We do so over multiple iterations and then average the value, which converge to expectations $\\mathbb{E}$. This method call **Monte Carlo simulations**.\n",
        "\n",
        "- action value functions: The expected return that the agent gets at time t is now known as action value function $q_π(s, a)$\n",
        "\n",
        "$$\n",
        "    \\large q_{\\pi}(s, a) = \\mathbb{E_{\\pi}} \\left[G_t | S_t=s, A_t=a \\right]\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gCSa55mNfEIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bellman Equation for Value Function**\n",
        "\n",
        "\n",
        "First let's write down $G_t$ in a recursive form:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    G_t & = R_{t+1}+γR_{t+2}+γ^{2}R_{t+3}+...+γ^{T-t+1}R_{T} \\\\\n",
        "    G_t & = R_{t+1}+γ\\left[R_{t+2}+γR_{t+3}+γ^{2}R_{t+4}+...+γ^{T-t}R_{T} \\right] \\\\\n",
        "    G_t & = R_{t+1} + γ G_{t+1}\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "So:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\large v_{\\pi}(s) & = \\mathbb{E_{\\pi}} \\left[G_t | S_t=s \\right] \\\\\n",
        "    \\large v_{\\pi}(s) & = \\mathbb{E_{\\pi}} \\left[R_{t+1} + γ G_{t+1} | S_t=s \\right]\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The Bellman equation for $V_\\pi(s)$:\n",
        "---\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "$$\n",
        "    V_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s{'}} P(s{'}| s, a) \\left[ R(s, a, s{'}) + \\gamma V_\\pi(s{'}) \\right]\n",
        "$$\n",
        "\n",
        "> where: <br>\n",
        "- $\\pi(a \\mid s)$ : Probability of taking action $a$ in state $s$ under policy $\\pi$,\n",
        "- $P(s{'} \\mid s, a)$ : Probability of transitioning to state $s{'}$ after taking action $a$ in state $s$,\n",
        "- $R(s, a, s{'})$ : Reward received when transitioning from $s$ to $s{'}$ using action $a$.\n",
        "\n",
        "<br>\n",
        "\n",
        "The action-value function $Q_\\pi(s, a)$\n",
        "---\n",
        "\n",
        "---\n",
        "The action-value function $ Q_\\pi(s, a)$  represents the expected cumulative reward starting from state $s$, taking action $a$, and then following policy  $\\pi$:\n",
        "\n",
        "$$\n",
        "    Q_\\pi(s, a)  = \\mathbb{E_\\pi} \\left[ R_{t+1} + γ G_{t+1} | s, a \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "The q-value is the value of the paired $(s, a)$, and the state value is the value for a state $(s)$. The policy links the state to the possible set of actions through a probability distribution:\n",
        "\n",
        "$$\n",
        "    \\large v_{\\pi}(s) = \\sum_{a} \\pi(a| s) Q_\\pi(s, a)\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "- The Bellman equation for $Q_\\pi(s, a)$ is:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    Q_\\pi(s, a) & = \\sum_{s{'}} P(s{'}| s, a) \\left[ R(s, a, s{'}) + \\gamma \\sum_{a{'}} \\pi(a{'} | s{'}) Q_\\pi(s{'}, a{'}) \\right] \\\\\n",
        "    Q_\\pi(s, a) & = \\sum_{s{'}} P(s{'}| s, a) \\left[ R(s, a, s{'}) + \\gamma v_{\\pi}(s^{'}) \\right] \\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Optimal State-Value Function:\n",
        "---\n",
        "\n",
        "---\n",
        "The optimal state-value function $V(s) $ is the maximum value achievable in state $s$ under any policy, which is the objective of  a reinforcement learning problem:\n",
        "\n",
        "$$\n",
        "V_*(s) = \\max_\\pi v_{\\pi}(s)\n",
        "$$\n",
        "\n",
        "> the optimal state value is the maximum one that can be obtained across all possible policies $π$.\n",
        "\n",
        "If an agent is following the optimal policy, then the agent in state $(s)$ will take the action that maximizes the $Q(s, a)$.\n",
        "\n",
        "$$\n",
        "V_*(s) = \\max_a Q(s, a)\n",
        "$$\n",
        "\n",
        "The Bellman optimality equation for $V(s)$ is:\n",
        "\n",
        "$$\n",
        "V_*(s) = \\max_a \\sum_{s{'}} P(s{'} | s, a) \\left[ R(s, a, s{'}) + \\gamma V_*(s{'}) \\right]\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Optimal Action-Value Function:\n",
        "---\n",
        "\n",
        "---\n",
        "The optimal action-value function $Q(s, a)$ is the maximum value achievable starting from state $s$, taking action $a$, and then following the optimal policy:\n",
        "\n",
        "$$\n",
        "Q_*(s, a) = \\sum_{s{'}} P(s{'} | s, a) \\left[ R(s, a, s{'}) + \\gamma \\max_{a{'}} Q_*(s{'}, a{'}) \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "Bellman Equation helps us to find optimal policies and value functions.\n",
        "\n",
        "$ V_{t+1} = R + \\gamma \\times T \\times V_{t} $\n",
        "\n",
        "<br>\n",
        "\n",
        "when the value converges, which means $ V_{t+1} = V_{t}$:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$   \n",
        "V - \\gamma \\times T \\times V = R \\\\\n",
        "V(I - \\gamma \\times T) = R \\\\\n",
        "V = (I - \\gamma \\times T)^{-1} \\times R\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Create and MDP:**\n",
        "> we have an environment consists of: <br>\n",
        "$\n",
        "S = [s_0, s_1, s_2], \\\\ A = [a_0, a_1], \\\\\n",
        "\\text{transition matrix}: T(s, a, s^{'}), \\\\\n",
        "\\text{discount factor}: \\gamma\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "- In the beginning we don't want to implement a complex model, so we assume optimal policy select first action in all circumstances. $ \\hspace{1mm}{action = a_0} $ which means it would be 0 all the time.\n",
        "- define reward function and discount factor\n",
        "\n"
      ],
      "metadata": {
        "id": "iqKC7xn0Rjfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_value_matrix_inv(gamma, trans_matrix, rewards):\n",
        "    inv = torch.inverse(torch.eye(rewards.shape[0]) - gamma * trans_matrix)\n",
        "    v = torch.mm(inv, rewards.reshape(-1, 1))\n",
        "    return v"
      ],
      "metadata": {
        "id": "OyrooHagE0WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = torch.tensor(\n",
        "    [[[0.8, 0.1, 0.1],\n",
        "      [0.1, 0.6, 0.3]],\n",
        "     [[0.7, 0.2, 0.1],\n",
        "      [0.1, 0.8, 0.1]],\n",
        "     [[0.6, 0.2, 0.2],\n",
        "      [0.1, 0.4, 0.5]]]\n",
        ")\n",
        "\n",
        "R = torch.tensor([1., 0., -1.])\n",
        "gammas = [0, 0.5, 0.99]\n",
        "action = 0\n",
        "\n",
        "Trans_matrix = T[:, action]\n",
        "for gamma in gammas:\n",
        "    v = calc_value_matrix_inv(gamma, Trans_matrix, R)\n",
        "    print(f\"The value function under he optimal policy and discount factor = {gamma} is: \\n{v.numpy()} \\n\")"
      ],
      "metadata": {
        "id": "OFWc2hAjV0Ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d4a029-2b6e-4a0f-8d2d-08b9998ba390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value function under he optimal policy and discount factor = 0 is: \n",
            "[[ 1.]\n",
            " [ 0.]\n",
            " [-1.]] \n",
            "\n",
            "The value function under he optimal policy and discount factor = 0.5 is: \n",
            "[[ 1.6786704 ]\n",
            " [ 0.62603873]\n",
            " [-0.48199445]] \n",
            "\n",
            "The value function under he optimal policy and discount factor = 0.99 is: \n",
            "[[65.8293 ]\n",
            " [64.71942]\n",
            " [63.4876 ]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problems**\n",
        "\n",
        "There are two main problem;\n",
        "1. the environment transition is unknown\n",
        "2. calculate matrix inversion is not an easy task\n",
        "\n",
        "So in reality we use different methods."
      ],
      "metadata": {
        "id": "OlEr8_QpL4yy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vIbWUNnUc6PM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}