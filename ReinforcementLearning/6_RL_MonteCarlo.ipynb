{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zm7cpKOfwiP"
      },
      "source": [
        "# **Reinforcement Learning**\n",
        "<img align=\"right\" src=\"https://vitalflux.com/wp-content/uploads/2020/12/Reinforcement-learning-real-world-example.png\">\n",
        "\n",
        "- In reinforcement learning, your system learns how to interact intuitively with the environment by basically doing stuff and watching what happens.\n",
        "\n",
        "if you need the last version of gym use block of code below:\n",
        "\n",
        "```sh\n",
        "!pip uninstall gym -y\n",
        "!pip install gym\n",
        "```\n",
        "<br>\n",
        "\n",
        "And here is gymnasium version:\n",
        "\n",
        "```python\n",
        "gymnasium.__version__\n",
        "```\n",
        "1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMm-R68qfqwz"
      },
      "outputs": [],
      "source": [
        "# !pip install -U gym==0.25.2\n",
        "!pip install swig\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "# !pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MJI0LUTUf0di"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from IPython.core.display import HTML\n",
        "from base64 import b64encode\n",
        "from gym.wrappers import record_video, record_episode_statistics\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d_YOHy86f4Fj"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0, video_width=600):\n",
        "    \"\"\"\n",
        "    Displays a video from a specified episode with customizable width.\n",
        "\n",
        "    Args:\n",
        "        episode (int): The episode number to load the video for. Defaults to 0.\n",
        "        video_width (int): The width of the video player in pixels. Defaults to 600.\n",
        "\n",
        "    Returns:\n",
        "        IPython.display.HTML: An HTML video element that can be rendered in Jupyter notebooks.\n",
        "\n",
        "    Note:\n",
        "        - The function expects video files to be in './video/' directory with naming format 'rl-video-episode-{N}.mp4'\n",
        "        - Videos are base64 encoded and embedded directly in the HTML for display\n",
        "    \"\"\"\n",
        "    # Construct the path to the video file based on episode number\n",
        "    video_path = f\"./video/rl-video-episode-{episode}.mp4\"\n",
        "\n",
        "    # Read the video file as binary data\n",
        "    video_file = open(video_path, \"rb\").read()\n",
        "\n",
        "    # Encode the binary video data as base64 string\n",
        "    decoded = b64encode(video_file).decode()\n",
        "\n",
        "    # Create a data URL for the video\n",
        "    video_url = f\"data:video/mp4;base64,{decoded}\"\n",
        "\n",
        "    # Return an HTML video element with the embedded video\n",
        "    return HTML(f\"\"\"<video width=\"{video_width}\"\" controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "def create_env(name, render_mode=\"rgb_array\", record=False, eps_record=50, video_folder='./video'):\n",
        "    \"\"\"\n",
        "    Creates and configures a Gym environment with optional video recording and statistics tracking.\n",
        "\n",
        "    Args:\n",
        "        name (str): Name of the Gym environment to create (e.g., 'CartPole-v1')\n",
        "        render_mode (str): Rendering mode - \"human\", \"rgb_array\", or \"ansi\". Defaults to \"rgb_array\"\n",
        "        record (bool): Whether to record videos of the environment. Defaults to False\n",
        "        eps_record (int): Record a video every N episodes (when record=True). Defaults to 50\n",
        "        video_folder (str): Directory to save recorded videos. Defaults to './video'\n",
        "\n",
        "    Returns:\n",
        "        gym.Env: Configured Gym environment wrapped with recording and statistics tracking\n",
        "\n",
        "    Note:\n",
        "        - When record=True, videos will be saved in the specified folder with automatic naming\n",
        "        - The environment is always wrapped with episode statistics tracking\n",
        "    \"\"\"\n",
        "    # Create base Gym environment with specified render mode\n",
        "    env = gym.make(name, render_mode=render_mode)\n",
        "\n",
        "    # Optionally wrap environment with video recorder\n",
        "    if record:\n",
        "        # Record video every eps_record episodes (trigger function)\n",
        "        env = RecordVideo(env, video_folder=video_folder,\n",
        "                         episode_trigger=lambda x: x % eps_record == 0)\n",
        "\n",
        "    # Always wrap environment with episode statistics tracker\n",
        "    env = RecordEpisodeStatistics(env)\n",
        "\n",
        "    return env\n",
        "\n",
        "def show_reward(total_rewards):\n",
        "    \"\"\"\n",
        "    Plots the progression of rewards across episodes using matplotlib.\n",
        "\n",
        "    Args:\n",
        "        total_rewards (list or array-like): A sequence of reward values obtained per episode.\n",
        "\n",
        "    Displays:\n",
        "        A line plot showing the reward trend over episodes with:\n",
        "        - X-axis: Episode number\n",
        "        - Y-axis: Reward value\n",
        "\n",
        "    Note:\n",
        "        - This function immediately displays the plot using plt.show()\n",
        "        - The plot uses default matplotlib styling\n",
        "        - Useful for visualizing training progress in reinforcement learning\n",
        "    \"\"\"\n",
        "    # Create line plot of reward values\n",
        "    plt.plot(total_rewards)\n",
        "\n",
        "    # Label the x-axis as 'Episode'\n",
        "    plt.xlabel('Episode')\n",
        "\n",
        "    # Label the y-axis as 'Reward'\n",
        "    plt.ylabel('Reward')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYaKD2MWPn06"
      },
      "source": [
        "## **Monte Carlo Learning**\n",
        "\n",
        "In the previous Notebook, we evaluated and solved a **Markov Decision Process (MDP)** using **dynamic programming (DP)**. **Model-based** methods such as DP have some drawbacks. They require the environment to be fully known, including the transition matrix and reward matrix. They also have limited scalability, especially for environments with plenty of states\n",
        "\n",
        "**model-free approach**, the Monte Carlo (MC) methods, which have no requirement of prior knowledge of the environment and are much more scalable than DP.\n",
        "\n",
        "The term **Monte Carlo** is often used more broadly for any estimation method. Monte Carlo methods require only experience, sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.\n",
        "\n",
        "It is a method for estimating Value-action(Value|State, Action) or Value function(Value|State) using some sample runs from the environment for which we are estimating the Value function\n",
        "\n",
        "<br>\n",
        "\n",
        "- **types of Monte Carlo learning:**\n",
        "1. $\\textit{First Visit Monte Carlo}$: First visit estimates (Value|State: S1) as the average of the returns following the first visit to the state S1\n",
        "2. $\\textit{Every Visit Monte Carlo}$: It estimates (Value|State: S1) as the average of returns for every visit to the State S1.\n",
        "- **Example:**\n",
        "\n",
        "$$\n",
        "\\textit{First iteration: }A+3 \\rightarrow A+2 \\rightarrow B-4 \\rightarrow A+4 \\rightarrow\n",
        "B-3 \\rightarrow terminated \\\\\n",
        "\\textit{Second iteration: }B-2 \\rightarrow A+3 \\rightarrow B-3 \\rightarrow terminated\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\textit{First Visit}$ | $\\textit{V(a)}$ | $\\textit{V(b)}$|\n",
        "-----------------------|-----------------|----------------|\n",
        "$\\textit{First iteraion}$ | $3+2-4+4-3=2$ | $-4+4–3=-3$\n",
        "$\\textit{Second iteraion}$ | $3-3=0$ | $-2+3+-3=-2$\n",
        "$\\textit{Sum}$ | $\\frac{2+0}{2}=1$ | $\\frac{-3-2}{2}=-2.5$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$\\textit{Every Visit}$ | $\\textit{V(a)}$ | $\\textit{V(b)}$\n",
        "-----------------------|-----------------|----------------|\n",
        "$\\textit{First iteraion}$ | $(3+2-4+4-3)+(2-4+4-3)+(4-3)=2-1+1$ | $(-4+4-3)+(-3)=-3+-3$\n",
        "$\\textit{Second iteraion}$ | $3-3=0$ | $(-2+3–3)+(-3)=-2+-3$\n",
        "$\\textit{Sum}$ | $\\frac{2+-1+1+0}{4}=0.5$ | $\\frac{-3+-3+-2-3}{4}=-2.75$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note**:As we have been given 2 different iterations, we will be summing all the rewards coming after A (including that of A) after the first visit to ‘A’. It must be noted that if an episode doesn’t have an occurence of ‘A’, it won’t be considered in the average.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWtWNQy6dZ0T"
      },
      "source": [
        "## **Performing Monte Carlo policy evaluation**\n",
        "\n",
        "- **model-based algorithm**: A reinforcement learning algorithm that needs a known MDP is categorized as a model-based algorithm.\n",
        "- **model-free algorithm**: On the other hand, one with no requirement of prior knowledge of transitions and rewards is called a model-free algorithm. Monte Carlo-based reinforcement learning is a model-free approach.\n",
        "\n",
        "we will evaluate the value function using the Monte Carlo method. assuming we don't have access to both of environment transition and reward matrices. You will recall that the returns of a process, which are the total rewards over the long run, are as follows:\n",
        "\n",
        "$$\n",
        "\\large G_t=\\sum_{k}^{\\infty}\\gamma^k R_{t+k+1}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "> $G_t​$:The total discounted reward accumulated from time step t onward.<br>\n",
        "$\\sum_{k=0}^{\\infty}$ (Summation): Monte Carlo methods sum rewards over complete episodes (from current state to termination).<br>\n",
        "$\\gamma$ (Discount factor): $\\gamma$ (gamma) is the discount factor (0 ≤ $\\gamma$ ≤ 1).<br>\n",
        "​$R_{t+k+1}$(Reward sequence): The actual reward received at time $t+k+1$.<br>\n",
        "$t$ is current time step and $k$ is steps into the future\n",
        "\n",
        "<br>\n",
        "\n",
        "MC policy evaluation uses **empirical mean return** instead of **expected return** (as in DP) to estimate the value function.\n",
        "\n",
        "- **Note**: in the Monte Carlo setting, we need to keep track of the states and rewards for all steps, since we don't have access to the full environment, including the transition probabilities and reward matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ1DksAZJIsm"
      },
      "source": [
        "### **First-Visit Monte Carlo Prediction**\n",
        "**Input**: policy ($\\pi$), number of episodes *num_episodes*  \n",
        "**Output**: value function: $V \\approx v_{\\pi}$ if num_episodes is large enough\n",
        "\n",
        "1. Initialize for all states ($s \\in \\mathcal{S}$):  \n",
        "   - $N(s) = 0$\n",
        "   - $\\text{Returns}(s) = 0$  \n",
        "\n",
        "2. **For each episode** ($e \\leftarrow 1$ to $e \\leftarrow \\textit{num_episodes}$):  \n",
        "   - Generate an episode $S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_T$ using $\\pi$\n",
        "   - $G \\leftarrow 0$\n",
        "   - **Loop backwards** from $t = T-1$ to $t=0$:  \n",
        "     - $G \\leftarrow G + R_{t+1}$  \n",
        "     - **If** $S_t$ appears for the **first time** in the episode:  \n",
        "       - $\\text{Returns}(S_t) \\leftarrow \\text{Returns}(S_t) + G$  \n",
        "       - $N(S_t) \\leftarrow N(S_t) + 1$\n",
        "\n",
        "3. **Update value function**:  \n",
        "$$\n",
        "V(s) \\leftarrow \\frac{\\text{Returns}(s)}{N(s)} \\quad \\forall s \\in \\mathcal{S}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Every-Visit Monte Carlo Prediction**\n",
        "**Input**: policy $\\pi$, number of episodes *num_episodes*  \n",
        "**Output**: value function:$V \\approx v_{\\pi}$ if num_episodes is large enough\n",
        "\n",
        "1. Initialize for all states $s \\in \\mathcal{S}$:  \n",
        "   - $N(s) = 0$\n",
        "   - $\\text{Returns}(s) = 0$\n",
        "\n",
        "2.  **For each episode** ($e \\leftarrow 1 $ to $e \\leftarrow \\textit{num_episodes}$): :  \n",
        "   - Generate an episode $S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_T$ using $\\pi$\n",
        "   - $G \\leftarrow 0$\n",
        "   - **Loop backwards** from $t = T-1$ to $t=0$:  \n",
        "     - $G \\leftarrow G + R_{t+1}$\n",
        "     - **Update for every visit** to $S_t$:\n",
        "       - $\\text{Returns}(S_t) \\leftarrow \\text{Returns}(S_t) + G$\n",
        "       - $N(S_t) \\leftarrow N(S_t) + 1$\n",
        "\n",
        "3. **Update value function**:  \n",
        "$$\n",
        "V(s) = \\frac{\\text{Returns}(s)}{N(s)} \\quad \\forall s \\in \\mathcal{S}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zXF0Wh-OPnQz"
      },
      "outputs": [],
      "source": [
        "env = create_env(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7EZZwLHhe1ek"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Executes one episode in the environment following the given policy.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment to interact with\n",
        "        policy (torch.Tensor): The policy mapping states to actions\n",
        "\n",
        "    Returns:\n",
        "        tuple: (states, rewards) where:\n",
        "            - states (torch.Tensor): Sequence of visited states\n",
        "            - rewards (torch.Tensor): Sequence of received rewards\n",
        "\n",
        "    Note:\n",
        "        - The episode runs until termination (is_done = True)\n",
        "        - States and rewards are converted to PyTorch tensors\n",
        "        - The policy must implement the [] operator for state indexing\n",
        "    \"\"\"\n",
        "    # Initialize the environment and get starting state\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # Initialize lists to store episode data\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "\n",
        "    # Initialize termination flag\n",
        "    is_done = False\n",
        "\n",
        "    # Run the episode until termination\n",
        "    while not is_done:\n",
        "        # Select action according to policy for current state\n",
        "        action = policy[state].item()\n",
        "\n",
        "        # Execute action in environment\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "\n",
        "        # Store the reward\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Store next state if episode continues\n",
        "        if not is_done:\n",
        "            states.append(state)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Convert lists to PyTorch tensors\n",
        "    states = torch.tensor(states)\n",
        "    rewards = torch.tensor(rewards)\n",
        "\n",
        "    return states, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jqBdSrCbfPF3"
      },
      "outputs": [],
      "source": [
        "def mc_first_visit(env, policy, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Monte Carlo (First-Visit) method for estimating the value function under a given policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        policy: A policy to follow, which maps states to actions.\n",
        "        gamma: Discount factor (0 <= gamma < 1) to weigh future rewards.\n",
        "        n_episode: Number of episodes to sample.\n",
        "\n",
        "    Returns:\n",
        "        V (torch.Tensor): Estimated value function for each state.\n",
        "    \"\"\"\n",
        "    # Number of states in the environment\n",
        "    n_state = env.observation_space.n\n",
        "\n",
        "    # Initialize the value function and visit counts to zero\n",
        "    V = torch.zeros(n_state)  # Value function\n",
        "    N = torch.zeros(n_state)  # Number of first visits to each state\n",
        "\n",
        "    # Iterate over the specified number of episodes\n",
        "    pbar = tqdm(range(n_episode), desc=\"Episode\")\n",
        "    for episode in pbar:\n",
        "        # Generate a single episode following the policy\n",
        "        states, rewards = run_episode(env, policy)\n",
        "\n",
        "        # Initialize return and other variables\n",
        "        G = torch.zeros(n_state)  # To store return for each state\n",
        "        first_visit = torch.zeros(n_state)  # Marks the first visit to a state in the episode\n",
        "        return_t = 0\n",
        "\n",
        "        # Process the episode in reverse (from terminal state to the initial state)\n",
        "        for state_t, reward_t in zip(reversed(states), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t  # Calculate return\n",
        "            G[state_t] = return_t  # Store the return for the state\n",
        "            first_visit[state_t] = 1  # Mark the state as visited for the first time\n",
        "\n",
        "        # Update value function and visit counts for first-visit states\n",
        "        for state in range(n_state):\n",
        "            if first_visit[state] > 0:  # Only update for first-visit states\n",
        "                V[state] += G[state]\n",
        "                N[state] += 1\n",
        "\n",
        "        pbar.set_description(f\"G: {G.sum()}\")\n",
        "    # Finalize the value function by averaging returns for each state\n",
        "    for state in range(n_state):\n",
        "        if N[state] > 0:\n",
        "            V[state] = V[state] / N[state]\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oz4lLNcR7eU0"
      },
      "outputs": [],
      "source": [
        "def mc_every_visit(env, policy, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Monte Carlo (Every-Visit) method for estimating the value function under a given policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        policy: A policy to follow, which maps states to actions.\n",
        "        gamma: Discount factor (0 <= gamma < 1) to weigh future rewards.\n",
        "        n_episode: Number of episodes to sample.\n",
        "\n",
        "    Returns:\n",
        "        V (torch.Tensor): Estimated value function for each state.\n",
        "    \"\"\"\n",
        "    # Number of states in the environment\n",
        "    n_state = env.observation_space.n\n",
        "\n",
        "    # Initialize the value function, visit counts, and cumulative returns to zero\n",
        "    V = torch.zeros(n_state)  # Value function\n",
        "    N = torch.zeros(n_state)  # Visit counts for every visit\n",
        "    G = torch.zeros(n_state)  # Cumulative return for each state\n",
        "\n",
        "    # Iterate over the specified number of episodes\n",
        "    pbar = tqdm(range(n_episode), desc=\"Episode\")\n",
        "    for episode in pbar:\n",
        "        # Generate a single episode following the policy\n",
        "        states, rewards = run_episode(env, policy)\n",
        "\n",
        "        # Initialize return\n",
        "        return_t = 0\n",
        "\n",
        "        # Process the episode in reverse (from terminal state to the initial state)\n",
        "        for state_t, reward_t in zip(reversed(states), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t  # Calculate return\n",
        "            G[state_t] += return_t  # Add the return to the cumulative return\n",
        "            N[state_t] += 1  # Increment visit count for the state\n",
        "\n",
        "        pbar.set_description(f\"G: {G.sum()}\")\n",
        "\n",
        "    # Finalize the value function by averaging returns for each state\n",
        "    for state in range(n_state):\n",
        "        if N[state] > 0:\n",
        "            V[state] = G[state] / N[state]\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbCno4Js-gv"
      },
      "source": [
        "## **Another approach**\n",
        "\n",
        "We store the cumulative total, the count of first visits and the average:\n",
        "\n",
        "$$\n",
        "    N(s) = N(s) + 1;\\hspace{5mm} S(s) = S(s) + G;\\hspace{5mm} V(s) = \\frac{S(s)}{N(s)}\n",
        "$$\n",
        "\n",
        "> $N(s)$: visit count;<br>\n",
        "$S(s)$: cumulative state values;<br>\n",
        "$V(s)$: estimate state values\n",
        "\n",
        "<br>\n",
        "\n",
        "We can update $V(s)$ in another way\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\\\\n",
        "    V(s)_{n+1} & = \\frac{S(s)_n + G}{N(s)_{n+1}}; \\hspace{1cm} S(s) = V(s) * N(s) \\\\\n",
        "    \\\\\n",
        "    V(s)_{n+1} & = \\frac{V(s)_n * N(s)_n + G}{N(s)_{n+1}}; \\hspace{1cm} N(s)_{n+1} = N(s)_n + 1 \\\\\n",
        "    \\\\\n",
        "    V(s)_{n+1} & = \\frac{V(s)_n * N(s)_{n+1} - V(s)_n + G}{N(s)_{n+1}} \\\\\n",
        "    \\\\\n",
        "    V(s)_{n+1} & = V(s)_n + \\frac{1}{N(s)_{n+1}}\\bigl[ G - V(s)_n\\bigr]\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "$\\bigl[ G - V(s)_n\\bigr]$ can be viewed as an error, and $\\frac{1}{N}$ reduce to zero as $N$ becomes very large. we can use constant $\\alpha$ as a factor instead of $\\frac{1}{N}$, which is better for nonstationary problems:\n",
        "\n",
        "\n",
        "$$\n",
        "    V(s)_{n+1} = V(s)_n + \\alpha\\bigl( G - V(s)_n\\bigr)\\\\\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EWx08IYpzjwQ"
      },
      "outputs": [],
      "source": [
        "def mc_first_visit2(env, policy, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Monte Carlo (First-Visit) method for estimating the value function under a given policy.\n",
        "    This version updates the value function incrementally using an online formula.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        policy: A policy to follow, which maps states to actions.\n",
        "        gamma: Discount factor (0 <= gamma < 1) to weigh future rewards.\n",
        "        n_episode: Number of episodes to sample.\n",
        "\n",
        "    Returns:\n",
        "        V (torch.Tensor): Estimated value function for each state.\n",
        "    \"\"\"\n",
        "    # Number of states in the environment\n",
        "    n_state = env.observation_space.n\n",
        "\n",
        "    # Initialize the value function and visit counts to zero\n",
        "    V = torch.zeros(n_state)  # Value function\n",
        "    N = torch.zeros(n_state)  # Number of first visits to each state\n",
        "\n",
        "    # Iterate over the specified number of episodes\n",
        "    pbar = tqdm(range(n_episode), desc=\"Episode\")\n",
        "    for episode in pbar:\n",
        "        # Generate a single episode following the policy\n",
        "        states, rewards = run_episode(env, policy)\n",
        "\n",
        "        # Initialize return and other variables\n",
        "        G = torch.zeros(n_state)  # To store return for each state\n",
        "        first_visit = torch.zeros(n_state)  # Marks the first visit to a state in the episode\n",
        "        return_t = 0\n",
        "        count = len(states)  # Total number of states visited in the episode\n",
        "\n",
        "        # Process the episode in reverse (from terminal state to the initial state)\n",
        "        for t in range(count - 1, -1, -1):  # Traverse the states in reverse order\n",
        "            state_t, reward_t = states[t], rewards[t]\n",
        "            return_t = reward_t + gamma * return_t  # Calculate return\n",
        "            G[state_t] = return_t  # Store the return for the state\n",
        "            first_visit[state_t] = 1  # Mark the state as visited for the first time\n",
        "\n",
        "            # Check if this is the first visit to the state\n",
        "            if state_t not in states[:t]:\n",
        "                N[state_t] += 1  # Increment the visit count for the state\n",
        "                # Incrementally update the value function using the online formula\n",
        "                V[state_t] = V[state_t] + (1 / N[state_t]) * (G[state_t] - V[state_t])\n",
        "\n",
        "        pbar.set_description(f\"G: {G.sum()}\")\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9QFWUg48AaE"
      },
      "source": [
        "|Aspect | mc_first_visit | mc_first_visit2|\n",
        "|-------|----------------|----------------|\n",
        "|Update Method |\tUses a batch update after all episodes.\t| Updates incrementally after each visit.|\n",
        "|Return Storage\t|Stores and sums all returns before averaging.\t|Directly computes averages during updates.|\n",
        "|Visit Check |Uses a first_visit flag to track first visits.| Checks if the state is in states[:t].|\n",
        "|Memory Usage| Requires additional memory for storing G and first_visit.| Lower memory usage due to incremental updates.|\n",
        "|Convergence Speed| May converge slightly slower due to batch updates.| Potentially faster convergence due to online updates.|\n",
        "|Code Complexity| Straightforward with clear separation of steps.| Slightly more complex due to conditional checks and incremental updates.|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "e8be5940db2542f9ad2c0aa85172d5da",
            "9b530820922043f4b0546c9771fb6247",
            "1163bed94fe646bab2465ffbfb5f1f22",
            "6a9da55690df4f51ad19aeffd472a00a",
            "752a2cf9fefc42b1b1596102eab7ef68",
            "253c3be0c3994bec9be578eddd27fc11",
            "c0490f59afbf4c32ba3e4f6e0a0476c9",
            "ba04cccd65bf4bf596dfcf7d1b3c037b",
            "a815c0113dff410b91b5abc9d65af2f4",
            "f520efc9350445599b0f38e31c4f2d5a",
            "8b10280a0ee94f8bbb57741a72ac67d8"
          ]
        },
        "id": "XHrIphHSltA-",
        "outputId": "613484ab-7480-4854-bb1d-bd668e2fda10"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8be5940db2542f9ad2c0aa85172d5da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The value function calculated by first visit MC:\n",
            "tensor([0.7460, 0.4887, 0.4815, 0.4286, 0.7460, 0.0000, 0.3833, 0.0000, 0.7460,\n",
            "        0.7467, 0.6756, 0.0000, 0.0000, 0.8065, 0.8881, 0.0000])\n",
            " \n"
          ]
        }
      ],
      "source": [
        "gamma = 1\n",
        "# use policy from previous notebook\n",
        "policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.]).long()\n",
        "n_episode = 1000\n",
        "\n",
        "first_visit = mc_first_visit(env, policy, gamma, n_episode)\n",
        "print(f\"The value function calculated by first visit MC:\\n{first_visit}\\n \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "c86f6ed09a974c23b0100762f40894db",
            "ba260737007244e3b58a4035a7d99a2d",
            "3e10abad4b834be191b90568cbe0871e",
            "7a92acbd1a2b4185bd73302e888c053f",
            "b1e5dfcf3c8143409ce66c754da6336c",
            "06dc8e4945f64a628a91e23e6a657fd7",
            "efd4411f4c4f44b1b73034aeede80689",
            "4b5ec25eda8e48d4b21e1ee71921103c",
            "350a72eb10974190ae5fc98c707a222d",
            "9ef887133ee1490e80fb5aa9571045a6",
            "a3eba0f0268a48629342180491dc8b1a"
          ]
        },
        "id": "Xna34oRmnd42",
        "outputId": "8e5aafcf-7f71-4aca-aabf-32d4d260b03a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c86f6ed09a974c23b0100762f40894db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The value function calculated by every visit MC:\n",
            "tensor([0.5969, 0.4928, 0.4410, 0.4389, 0.6098, 0.0000, 0.3775, 0.0000, 0.6398,\n",
            "        0.6725, 0.6420, 0.0000, 0.0000, 0.7616, 0.8794, 0.0000])\n",
            " \n"
          ]
        }
      ],
      "source": [
        "every_visit = mc_every_visit(env, policy, gamma, n_episode)\n",
        "print(f\"The value function calculated by every visit MC:\\n{every_visit}\\n \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "1c6b5171c9844aabaaff371eb245ff5d",
            "9acb9d0cd8ec419288f6a4e994df19ae",
            "650b3642d3d84577a5cc8575d0ea979c",
            "438ec4ca892d4b8594eab0bf45440d77",
            "760fb063dc454543ab1774fc87d14d07",
            "132d146574fa4f599675f591ff74317d",
            "91263a74534e45d984e108b8c285f373",
            "e9b68e3ec729458f977b9b64ebb8db34",
            "4c6d1a8cab5543cea01c74a0bef08e17",
            "8200282c6b5a4055952f2336d167ce33",
            "ac605efae33247c8b2fa46ba4d4aad78"
          ]
        },
        "id": "Gxxb_Gi41MGg",
        "outputId": "dc869263-c810-45a5-f067-955237938bf2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c6b5171c9844aabaaff371eb245ff5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The value function calculated by first visit MC:\n",
            "tensor([0.7570, 0.4841, 0.4766, 0.4023, 0.7570, 0.0000, 0.3902, 0.0000, 0.7570,\n",
            "        0.7570, 0.6839, 0.0000, 0.0000, 0.8086, 0.9044, 0.0000])\n",
            " \n"
          ]
        }
      ],
      "source": [
        "first_visit2 = mc_first_visit2(env, policy, gamma, n_episode)\n",
        "print(f\"The value function calculated by first visit MC:\\n{first_visit2}\\n \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxt8G_Ex09z-"
      },
      "source": [
        "## **Bias and Variance**\n",
        "\n",
        "\n",
        "**Bias** refers to the property of the model to converge to the true  value. Some estimators are biased, meaning they are not able to converge to the true value due to lack of flexibility.\n",
        "\n",
        "**Variance** refers to the model estimate being sensitive to the specific sample data being used. This means the estimate value may fluctuate a lot and hence may require a large data set or trials for the estimate average to converge to a stable value.\n",
        "\n",
        "<br>\n",
        "\n",
        "- **bias-variance trade-off**\n",
        "Flexible models have low bias; however, they can overfit to the data, making the estimates vary a lot as the training data changes. On the contrary, simple models have high bias. So they may not be able to represent the true underlying model. But they will also have low variance as they do not overfit.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **first visit** is unbiased but has high variance. **Every visit** has bias that goes down to zero, and it has low variance, which usually converges to the true value estimates faster than first visit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ICzxLVuOZ4"
      },
      "source": [
        "## **BlackJack**\n",
        "<img align='right' width='400' src=\"https://www.gymlibrary.dev/_images/blackjack.gif\">\n",
        "\n",
        "\n",
        "|              |            |\n",
        "|--------------|------------|\n",
        "| Action Space | Discrete(2)|\n",
        "| Observation Space | Tuple(Discrete(32), Discrete(11), Discrete(2)) |\n",
        "| Import | gym.make(\"Blackjack-v1\") |\n",
        "\n",
        "Card Values:\n",
        "\n",
        "- Face cards (Jack, Queen, King) have a point value of 10.\n",
        "- Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
        "- Numerical cards (2-9) have a value equal to their number.\n",
        "\n",
        "Action Space:\n",
        "\n",
        "- There are two actions: stick (0), and hit (1).\n",
        "\n",
        "Observation Space:\n",
        "\n",
        "- The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MxExVnWEtY8A"
      },
      "outputs": [],
      "source": [
        "env = create_env(\"Blackjack-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fFn66GSPt9Hc"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def run_episode(env, hold_score):\n",
        "    state, _ = env.reset()\n",
        "    rewards = []\n",
        "    states = [state]\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        action = 1 if state[0] < hold_score else 0\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if is_done:\n",
        "            break\n",
        "        else:\n",
        "            states.append(state)\n",
        "\n",
        "    states = torch.tensor(states)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    return states, rewards\n",
        "\n",
        "def mc_first_visit_blackjack(env, hold_score, gamma, n_episode):\n",
        "    V = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "    pbar = tqdm(range(n_episode), desc=\"Epsiode\")\n",
        "    for episode in pbar:\n",
        "        states, rewards = run_episode(env, hold_score)\n",
        "        G = {}\n",
        "        return_t = 0\n",
        "        for state_t, reward_t in zip(reversed(states), reversed(rewards)):\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t] = return_t\n",
        "\n",
        "        for state, return_t in G.items():\n",
        "            if state[0] <= 21:\n",
        "                V[state] += return_t\n",
        "                N[state] += 1\n",
        "        pbar.set_description(f\"G: {sum(G.values())}\")\n",
        "\n",
        "    for state in V:\n",
        "        V[state] = V[state] / N[state]\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7539850d958147ce974c380512bb3523",
            "a6599d403ec14671b4500eca0b5b1cf3",
            "cec32f68bb324be79b156b80ca8d0232",
            "340043b39bb7415bbb0e88febac78dd2",
            "b5fbf5cbde6e4b6d9f00e62ae8272ee4",
            "b658e42dc4ac402288466f11ce3b37f4",
            "b98fb5a75f5c4c9786ce94ca5c1138d6",
            "88474f2504794091b6dd06b07dee2c58",
            "4a93101dac714d859c4b09d20211615d",
            "4982670b150b4daea7ea5552fcbe04cb",
            "a0120135aed84939bd23daedb1ee1ec6"
          ]
        },
        "id": "XklmJAoquGFs",
        "outputId": "dcffecbe-d61d-470f-edff-5a3bdd9250af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7539850d958147ce974c380512bb3523",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epsiode:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = create_env(\"Blackjack-v1\")\n",
        "hold_score = 18\n",
        "gamma = 1\n",
        "n_episode = 500\n",
        "value = mc_first_visit_blackjack(env, hold_score, gamma, n_episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciiYFhQavxcO",
        "outputId": "4b45fa06-3a7a-4137-d670-483086518086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "868"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIOnX54h8s-O"
      },
      "source": [
        "## **Performing on-policy Monte Carlo control**\n",
        "\n",
        "**Monte Carlo prediction** is used to evaluate the value for a given policy, while **Monte Carlo control (MC control)** is for finding the optimal policy when such a policy is not given. There are basically categories of MC control: **on-policy** and **off-policy**.\n",
        "- On-policy methods learn about the optimal policy by executing the policy and evaluating and improving it\n",
        "- off-policy methods learn about the optimal policy using data generated by another policy.\n",
        "\n",
        "**Note:** The way on-policy MC control works is quite similar to policy iteration in dynamic programming, which has two phases, evaluation and improvement:\n",
        "\n",
        "- **In the evaluation phase**, instead of evaluating the value function (also called the state value, or utility), it evaluates the action-value. The action-value is more frequently called the **Q-function**, which is the utility of a state-action pair $(s, a)$ by taking action a in state s under a given policy. Again, the evaluation can be conducted in a first-visit manner or an every-visit manner.\n",
        "\n",
        "- **In the improvement phase**, the policy is updated by assigning the optimal action to each state:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\large\\pi(s) = \\underset{a}{argmax}Q(s, a)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "F8g-vYMT4x9C"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Function to simulate a single episode based on the given Q-table and environment\n",
        "def run_episode(env, Q, n_action):\n",
        "    \"\"\"\n",
        "    Simulates one episode of the environment using the given Q-values.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        Q (defaultdict): Q-table storing state-action value estimates.\n",
        "        n_action (int): The number of possible actions in the environment.\n",
        "\n",
        "    Returns:\n",
        "        states (list): A list of states visited during the episode.\n",
        "        actions (list): A list of actions taken during the episode.\n",
        "        rewards (list): A list of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()  # Reset the environment to its initial state\n",
        "    rewards = []         # List to store rewards for the episode\n",
        "    actions = []         # List to store actions taken\n",
        "    states = []          # List to store states visited\n",
        "    is_done = False      # Variable to track if the episode is finished\n",
        "\n",
        "    # Take a random initial action\n",
        "    action = torch.randint(0, n_action, [1]).item()\n",
        "\n",
        "    while not is_done:\n",
        "        actions.append(action)    # Store the current action\n",
        "        states.append(state)      # Store the current state\n",
        "\n",
        "        # Perform the action and observe the next state, reward, and termination flag\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        rewards.append(reward)    # Store the reward for the current step\n",
        "\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "        # Select the next action using the current policy (greedy policy derived from Q)\n",
        "        action = torch.argmax(Q[state]).item()\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "KYa7G2RWANON"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo On-Policy Control with First-Visit MC Prediction\n",
        "def mc_on_policy(env, gamma, n_episode):\n",
        "    \"\"\"\n",
        "    Implements on-policy first-visit Monte Carlo control to learn the optimal policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        gamma (float): Discount factor for future rewards (0 <= gamma < 1).\n",
        "        n_episode (int): The number of episodes to run for training.\n",
        "\n",
        "    Returns:\n",
        "        Q (defaultdict): The learned Q-table with state-action values.\n",
        "        policy (dict): The learned policy derived from the Q-table.\n",
        "    \"\"\"\n",
        "    n_action = env.action_space.n  # Number of possible actions\n",
        "    G_sum = defaultdict(float)    # Cumulative sum of returns for each state-action pair\n",
        "    N = defaultdict(int)          # Count of visits to each state-action pair\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))  # Q-table initialized with empty tensors\n",
        "\n",
        "    pbar = tqdm(range(n_episode), desc = \"Episode\")\n",
        "    for episode in pbar:\n",
        "\n",
        "        # Generate an episode using the current Q-table\n",
        "        states, actions, rewards = run_episode(env, Q, n_action)\n",
        "\n",
        "        G = {}  # Dictionary to store the return G for each state-action pair\n",
        "        return_t = 0  # Initialize the return for the episode\n",
        "\n",
        "        # Iterate over the episode in reverse order (to calculate returns)\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            return_t = reward_t + gamma * return_t  # Calculate the discounted return\n",
        "            G[state_t, action_t] = return_t         # Store the return for the state-action pair\n",
        "\n",
        "        # Update the Q-table for each state-action pair in the episode\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            # Update only for states with valid indices\n",
        "            if state[0] <= 21:\n",
        "                G_sum[state_action] += return_t        # Update cumulative return\n",
        "                N[state_action] += 1                  # Increment visit count\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]  # Update Q-value\n",
        "\n",
        "        pbar.set_description(f\"G: {sum(G.values())}\")\n",
        "\n",
        "    # Derive the policy from the Q-table by taking the action with the highest value in each state\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        policy[state] = torch.argmax(actions).item()\n",
        "\n",
        "    return Q, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8edddca5cfad466780c12eac913e1bd2",
            "2f4cdcd0256c43ff87e78389953eeb2f",
            "cc0f45c4c3b746b9b1c0edee084a6ee4",
            "8a4ad6668cb843b288787bc330eaf4c0",
            "3470231dc80d4b909cea3a227ede339c",
            "70f4b53928254ec9b7fb2ed58688e279",
            "79f6478222ce4258ac66696c8ff58e3d",
            "b08b9b38f751411694d0e07c4cbfe420",
            "33814653129949838fdfa55833a12f1d",
            "122f622f6b204fe6a0a4878aa4d7bbb8",
            "59a1cdd9cfdc433b806ee4328edfe9e7"
          ]
        },
        "id": "EpwbzRXUAyVL",
        "outputId": "f0391298-c4b5-4fd8-fc60-6b57240c3d8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8edddca5cfad466780c12eac913e1bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = create_env(\"Blackjack-v1\")\n",
        "gamma = 1\n",
        "n_episode = 500\n",
        "optimal_Q, optimal_policy = mc_on_policy(env, gamma, n_episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LhRfthkBGHI",
        "outputId": "75fd43dc-71ed-498d-8e45-0cf89278488a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(13, 7, 0): 0.20000000298023224\n",
            "(20, 6, 0): 0.4000000059604645\n",
            "(11, 10, 0): 0.20000000298023224\n",
            "(9, 10, 0): -0.5\n",
            "(21, 8, 1): 1.0\n",
            "(5, 7, 0): 0.0\n",
            "(20, 5, 0): 1.0\n",
            "(20, 10, 0): 0.25\n",
            "(14, 10, 0): -0.27272728085517883\n",
            "(17, 10, 0): -0.3333333432674408\n",
            "(15, 10, 0): -0.5\n"
          ]
        }
      ],
      "source": [
        "# Initialize a dictionary to store the optimal state values\n",
        "optimal_value = defaultdict(float)\n",
        "\n",
        "# Iterate through the optimal Q-table to extract the maximum value for each state\n",
        "for state, action_values in optimal_Q.items():\n",
        "    \"\"\"\n",
        "    For each state in the optimal Q-table:\n",
        "        - action_values: Tensor containing the Q-values for all possible actions in the state.\n",
        "        - torch.max(action_values).item(): Finds the maximum Q-value (optimal value) for the state.\n",
        "        - Store this maximum value in the `optimal_value` dictionary.\n",
        "    \"\"\"\n",
        "    optimal_value[state] = torch.max(action_values).item()\n",
        "\n",
        "# Print the dictionary containing the optimal state values\n",
        "# print(optimal_value)\n",
        "for i, (k, v) in enumerate(optimal_value.items()):\n",
        "    print(f\"{k}: {v}\")\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SlmCjlLgDRYq"
      },
      "outputs": [],
      "source": [
        "def simulate_episode(env, policy):\n",
        "    \"\"\"\n",
        "    Simulates an episode in the given environment following a specific policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (assumed to follow OpenAI Gym's structure).\n",
        "        policy (dict): A dictionary mapping states to actions. If a state is not in the policy,\n",
        "                       a random action is chosen.\n",
        "\n",
        "    Returns:\n",
        "        reward (float): The reward received at the end of the episode.\n",
        "    \"\"\"\n",
        "    # Reset the environment to the initial state\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # Flag to track if the episode is finished\n",
        "    done = False\n",
        "\n",
        "    # Simulate the episode\n",
        "    while not done:\n",
        "        # Check if the state exists in the policy\n",
        "        if state in policy:\n",
        "            # Follow the policy to select the action\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            # Choose a random action if the state is not in the policy\n",
        "            action = torch.randint(2, [1]).item()\n",
        "\n",
        "        # Perform the action in the environment and observe the new state and reward\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        # If the episode ends, return the final reward\n",
        "        if done:\n",
        "            return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6f87bd6909f54983a45f7ca14f2715fa",
            "07ce8e4e45534fd18ce2cc9ce30e8300",
            "d51f2522f5424d2c852a76b90c9c4247",
            "4d9554d6110043378ba635557ff6e5c3",
            "48f3a813612f4fb2869d27ee623d058b",
            "cb35381dd1f749f1b3f078594092ce3a",
            "54d5b0bf4c044f5190c78f531dea4b11",
            "5f05583cb6934393ad3da25980632b53",
            "4f5f14f29be843f2b37218dc112399dd",
            "f624283051ac422c870ba884059fabf8",
            "32985ddfe4b843a7a3b19c0039b935cf"
          ]
        },
        "id": "fVBuWkl9IX8a",
        "outputId": "573ab1ab-3077-45ae-b64a-b3af00bd1599"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f87bd6909f54983a45f7ca14f2715fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "win = 0\n",
        "pbar = tqdm(range(100))\n",
        "for i in pbar:\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)\n",
        "    pbar.set_description(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjYDmV-pMCYN"
      },
      "source": [
        "### **Developing MC control with epsilon-greedy policy**\n",
        "\n",
        "In MC control with **epsilon-greedy** policy, we no longer exploit the best action all the time, but choose an action randomly under certain probabilities. As the name implies, the algorithm has two folds:\n",
        "\n",
        "Epsilon: given a parameter, $ε$, with a value from 0 to 1, each action is taken with a probability calculated as follows:\n",
        "\n",
        "$$\n",
        "\\large \\pi(s, a) = \\frac{ε}{|A|}\n",
        "$$\n",
        "- Here, |A| is the number of possible actions.\n",
        "\n",
        "Greedy: the action with the highest state-action value is favored, and its probability of being chosen is increased by $1-ε$:\n",
        "\n",
        "$$\n",
        "\\large \\pi(s, a) = 1 - ε + \\frac{ε}{|A|}\n",
        "$$\n",
        "\n",
        "Epsilon-greedy policy exploits the best action most of the time and also keeps exploring different actions from time to time.\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img width=\"600\" src=\"https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-5b10393cf0c6395ae5fb22260220c574_l3.svg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uSopckQvMD7f"
      },
      "outputs": [],
      "source": [
        "def take_action(state, Q, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Selects an action for a given state using an epsilon-greedy policy.\n",
        "\n",
        "    Parameters:\n",
        "        state (int): The current state for which an action is to be chosen.\n",
        "        Q (dict): A dictionary where Q[state][action] represents the estimated value of taking\n",
        "                  'action' in 'state'.\n",
        "        epsilon (float): The probability of choosing a random action for exploration (0 <= epsilon <= 1).\n",
        "        n_action (int): The total number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        action (int): The chosen action.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a random number to decide between exploration or exploitation\n",
        "    if np.random.random() < epsilon:\n",
        "        # Exploration: Choose a random action with uniform probability\n",
        "        return torch.randint(0, n_action, (1,)).item()\n",
        "    else:\n",
        "        # Exploitation: Choose the action with the highest Q-value for the current state\n",
        "        return torch.argmax(Q[state]).item()\n",
        "\n",
        "\n",
        "def take_action2(state, Q, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Selects an action for a given state using an epsilon-greedy policy.\n",
        "\n",
        "    Parameters:\n",
        "        state (int): The current state for which an action is to be chosen.\n",
        "        Q (dict): A dictionary where Q[state][action] represents the estimated value of taking\n",
        "                  'action' in 'state'.\n",
        "        epsilon (float): The probability of choosing a random action for exploration (0 <= epsilon <= 1).\n",
        "        n_action (int): The total number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        action (int): The chosen action.\n",
        "    \"\"\"\n",
        "    # Initialize a probability distribution for all actions, with each action having an equal probability\n",
        "    # of being selected (epsilon / n_action).\n",
        "    probs = torch.ones(n_action) * epsilon / n_action\n",
        "\n",
        "    # Find the action with the highest Q-value for the current state (exploitation).\n",
        "    best_action = np.argmax(Q[state])\n",
        "\n",
        "    # Increase the probability of selecting the best action by the remaining probability (1 - epsilon).\n",
        "    probs[best_action] += (1.0 - epsilon)\n",
        "\n",
        "    # Sample an action based on the computed probability distribution.\n",
        "    action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "C9Z-51rDC_uJ"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, Q, epsilon, n_action):\n",
        "    \"\"\"\n",
        "    Simulates an episode in the environment using an epsilon-greedy policy for action selection.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (e.g., OpenAI Gym environment).\n",
        "        Q (dict): A dictionary where Q[state][action] represents the estimated value of taking\n",
        "                  'action' in 'state'.\n",
        "        epsilon (float): The probability of choosing a random action for exploration (0 <= epsilon <= 1).\n",
        "        n_action (int): The total number of possible actions.\n",
        "\n",
        "    Returns:\n",
        "        states (list): A list of states visited during the episode.\n",
        "        actions (list): A list of actions taken during the episode.\n",
        "        rewards (list): A list of rewards received during the episode.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the environment and get the starting state\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # Initialize lists to track states, actions, and rewards\n",
        "    rewards = []  # Rewards received during the episode\n",
        "    actions = []  # Actions taken during the episode\n",
        "    states = []   # States visited during the episode\n",
        "\n",
        "    # Flag to indicate whether the episode is done\n",
        "    is_done = False\n",
        "\n",
        "    # Loop until the episode ends\n",
        "    while not is_done:\n",
        "        # Select an action using the epsilon-greedy policy\n",
        "        action = take_action2(state, Q, epsilon, n_action)\n",
        "\n",
        "        # Record the chosen action and the current state\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "\n",
        "        # Perform the action in the environment and observe the outcome\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "\n",
        "        # Record the reward received for the action\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Check if the episode has ended\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "        # If the episode continues, select the next action based on the current Q-values\n",
        "        action = torch.argmax(Q[state]).item()\n",
        "\n",
        "    # Return the recorded states, actions, and rewards\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "t0PHmRRToGl_"
      },
      "outputs": [],
      "source": [
        "def mc_epsilon_greedy(env, gamma, n_episode, epsilon):\n",
        "    \"\"\"\n",
        "    Monte Carlo Control using the epsilon-greedy method to estimate the optimal policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (e.g., OpenAI Gym environment).\n",
        "        gamma (float): Discount factor (0 <= gamma <= 1), determines the importance of future rewards.\n",
        "        n_episode (int): Number of episodes to run for learning.\n",
        "        epsilon (float): Exploration probability for the epsilon-greedy policy (0 <= epsilon <= 1).\n",
        "\n",
        "    Returns:\n",
        "        Q (defaultdict): A dictionary mapping state-action pairs to their estimated Q-values.\n",
        "        policy (dict): The learned policy, mapping states to optimal actions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of possible actions in the environment\n",
        "    n_action = env.action_space.n\n",
        "\n",
        "    # Initialize accumulators for state-action returns\n",
        "    G_sum = defaultdict(float)  # Sum of returns for each state-action pair\n",
        "    N = defaultdict(int)        # Count of visits to each state-action pair\n",
        "\n",
        "    # Initialize Q-values for all state-action pairs\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
        "\n",
        "    # Loop through episodes\n",
        "    pbar = tqdm(range(n_episode), desc = \"Episode\")\n",
        "    for episode in pbar:\n",
        "\n",
        "        # Generate an episode using epsilon-greedy policy\n",
        "        states, actions, rewards = run_episode(env, Q, epsilon, n_action)\n",
        "\n",
        "        # Dictionary to store returns for state-action pairs in this episode\n",
        "        G = {}\n",
        "        return_t = 0  # Initialize cumulative return\n",
        "\n",
        "        # Loop through the episode in reverse to calculate returns\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            # Calculate the cumulative return for the state-action pair\n",
        "            return_t = reward_t + gamma * return_t\n",
        "            G[state_t, action_t] = return_t\n",
        "\n",
        "        # Update Q-values based on the observed returns\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "\n",
        "            # Ensure only valid states are updated (e.g., some states may be terminal)\n",
        "            if state[0] <= 21:  # Example condition specific to the problem\n",
        "                # Update cumulative return and visit count\n",
        "                G_sum[state_action] += return_t\n",
        "                N[state_action] += 1\n",
        "\n",
        "                # Update Q-value for the state-action pair using the average return\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
        "\n",
        "        pbar.set_description(f\"G: {sum(G.values())}\")\n",
        "\n",
        "    # Derive the policy from the Q-values\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        # Choose the action with the highest Q-value for each state\n",
        "        policy[state] = torch.argmax(actions).item()\n",
        "\n",
        "    # Return the learned Q-values and policy\n",
        "    return Q, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "24c7782cf8fc434baa95bfd1f40ac94c",
            "961974dca2db4a4a912e895713b15594",
            "e8082f0c80ab4855b2b0595ba11874fe",
            "04be545a65744cbb8cdb0151650adb4c",
            "c5ae5406133d4bf7a1911020e9e0b0e3",
            "e3d58517e1e3406c917b3db7434e2990",
            "5b1440101baa4436b97276d4ec583d3a",
            "377912dd434a4eba9a0e63a663f30ffb",
            "94b2c6187a2b49b18782e5c348e51ed9",
            "3a88bd6699d14985984d14023e61e17f",
            "5c882e61a0c24da6b7f17bea1fd797b5"
          ]
        },
        "id": "i0Vjy-UTos7F",
        "outputId": "f190279e-d5a7-484e-8b6f-45075c4aac4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24c7782cf8fc434baa95bfd1f40ac94c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = create_env(\"Blackjack-v1\")\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n_episode = 500\n",
        "optimal_Q, optimal_policy = mc_epsilon_greedy(env, gamma, n_episode, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3f9a4d85735149148ed285dbb01957e5",
            "7897152ea1f545d2b7dcd31b1465bd40",
            "bcef1e5ab2e5485a9f08a93b8c37832c",
            "81c8c7ac357547da856c16856ca93685",
            "6af2c70c54334f878cd367757315042c",
            "8cee04c93d004e57a470455c12396665",
            "576f078dea584f1281ccd28e3fb800c4",
            "6dac6b0e727d4284aab03bf830b13321",
            "da4cbd6b29df4d25bc50976a897b64fa",
            "df379c78c24f4be3bf9bfe76c1f098df",
            "7b6c6609d2bc4edc932fe0de13355acf"
          ]
        },
        "id": "pyZ0Cky8ooQu",
        "outputId": "f432d6e7-ab2c-4a21-bcca-80aa83958c84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f9a4d85735149148ed285dbb01957e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def simulate_episode(env, policy):\n",
        "     state, _ = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "        if state in policy:\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            action = torch.randint(2, [1]).item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            return reward\n",
        "\n",
        "win = 0\n",
        "pbar = tqdm(range(100))\n",
        "for i in pbar:\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)\n",
        "    pbar.set_description(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_vvvsb1qzhi"
      },
      "source": [
        "## **Performing off-policy Monte Carlo control**\n",
        "\n",
        "The off-policy method optimizes the **target policy** ($\\pi$), using data generated by another policy, called the **behavior policy** ($b$). The target policy performs **exploitation** all the time while the behavior policy is for **exploration** purposes. This means that the target policy is greedy with respect to its current Q-function, and the behavior policy generates behavior so that the target policy has data to learn from.\n",
        "\n",
        "We start with the latest step whose action taken under the behavior policy is different from the action taken under the greedy policy. And to learn about the target policy with another policy, we use a technique called **importance sampling**, which is commonly used to estimate the expected value under a distribution, given samples generated from a different distribution. The weighted importance for a state-action pair is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\omega_t= \\sum_{k=t}\\frac{\\pi(a_k|s_k)}{b{(a_k|s_k)}}$$\n",
        "\n",
        "> Here, $π(a_k | s_k)$ is the probability of taking action $a_k$ in state $s_k$ under the target policy; <br> $b(a_k | s_k)$ is the probability under the behavior policy; <br> the weight, $w_t$, is the multiplication of ratios between those two probabilities from step $t$ to the end of the episode. The weight, $w_t$, is applied to the return at step $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qG6C-Ph7zkJs"
      },
      "outputs": [],
      "source": [
        "def creat_random_policy(n_action):\n",
        "    \"\"\"\n",
        "    Creates a random policy where each action has an equal probability of being chosen.\n",
        "\n",
        "    Parameters:\n",
        "        n_action (int): The number of possible actions in the environment.\n",
        "\n",
        "    Returns:\n",
        "        policy_fn (function): A function that returns a random probability distribution over the actions.\n",
        "    \"\"\"\n",
        "    # Generate equal probabilities for each action\n",
        "    probs = torch.ones(n_action) / n_action\n",
        "\n",
        "    def policy_fn(observation):\n",
        "        \"\"\"\n",
        "        This is the policy function that, given an observation (state), returns the action probabilities.\n",
        "\n",
        "        Parameters:\n",
        "            observation: The current state of the environment (not used in this random policy, but included for consistency).\n",
        "\n",
        "        Returns:\n",
        "            probs (torch.Tensor): The probability distribution over the actions, which is uniform.\n",
        "        \"\"\"\n",
        "        return probs\n",
        "\n",
        "    return policy_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZPB1kGUiF5g2"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, random_policy):\n",
        "    \"\"\"\n",
        "    Runs a single episode of interaction with the environment using a random policy.\n",
        "\n",
        "    Parameters:\n",
        "        env: The environment object (e.g., OpenAI Gym environment).\n",
        "        random_policy: The random policy function that generates action probabilities for each state.\n",
        "\n",
        "    Returns:\n",
        "        states (list): List of states encountered during the episode.\n",
        "        actions (list): List of actions taken during the episode.\n",
        "        rewards (list): List of rewards received during the episode.\n",
        "    \"\"\"\n",
        "    # Reset the environment to get the initial state\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    # Lists to store states, actions, and rewards\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "\n",
        "    # Initialize the done flag to control the termination of the episode\n",
        "    is_done = False\n",
        "\n",
        "    # Run the episode until termination (done)\n",
        "    while not is_done:\n",
        "        # Get the action probabilities from the random policy\n",
        "        probs = random_policy(state)\n",
        "\n",
        "        # Sample an action from the probability distribution\n",
        "        action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        # Append the action and state to the lists\n",
        "        actions.append(action)\n",
        "        states.append(state)\n",
        "\n",
        "        # Take a step in the environment using the action\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "\n",
        "        # Append the reward to the rewards list\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Break the loop if the episode is done\n",
        "        if is_done:\n",
        "            break\n",
        "\n",
        "    # Return the states, actions, and rewards collected during the episode\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "eplUPMxHq1Ra"
      },
      "outputs": [],
      "source": [
        "def mc_off_policy(env, gamma, n_episode, epsilon, behavior_policy):\n",
        "    \"\"\"\n",
        "    Performs Monte Carlo off-policy control using the importance sampling technique.\n",
        "\n",
        "    Parameters:\n",
        "        env (gym.Env): The environment.\n",
        "        gamma (float): The discount factor.\n",
        "        n_episode (int): The number of episodes to run.\n",
        "        epsilon (float): The epsilon value for epsilon-greedy behavior policy.\n",
        "        behavior_policy (function): The behavior policy used to generate episodes.\n",
        "\n",
        "    Returns:\n",
        "        Q (dict): The action-value function learned through off-policy Monte Carlo control.\n",
        "        policy (dict): The optimal policy derived from Q.\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of possible actions in the environment\n",
        "    n_action = env.action_space.n\n",
        "\n",
        "    # Initialize dictionaries to accumulate the returns and counts\n",
        "    G_sum = defaultdict(float)  # Sum of returns for state-action pairs\n",
        "    N = defaultdict(int)        # Count of visits to state-action pairs\n",
        "    Q = defaultdict(lambda: torch.empty(env.action_space.n))  # Action-value function (Q)\n",
        "\n",
        "    # Loop through episodes\n",
        "    pbar = tqdm(range(n_episode), desc=\"Episode\")\n",
        "    for episode in pbar:\n",
        "        # Initialize importance sampling weight and dictionary to store weights\n",
        "        w = 1\n",
        "        W = {}\n",
        "\n",
        "        # Generate the episode using the behavior policy\n",
        "        states, actions, rewards = run_episode(env, behavior_policy)\n",
        "        G = {}  # Dictionary to store return for each state-action pair\n",
        "        return_t = 0  # The return (sum of discounted rewards) at each step\n",
        "\n",
        "        # Iterate over the episode in reverse order (bootstrapping)\n",
        "        for state_t, action_t, reward_t in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "            return_t = reward_t + gamma * return_t  # Update the return\n",
        "            G[state_t, action_t] = return_t        # Store the return for the state-action pair\n",
        "            W[state_t, action_t] = w              # Store the weight for the state-action pair\n",
        "\n",
        "            # If the action taken is not the greedy action, break the loop (off-policy condition)\n",
        "            if action_t != torch.argmax(Q[state_t]).item():\n",
        "                break\n",
        "\n",
        "            # Update the importance sampling weight (only if the behavior policy action was taken)\n",
        "            w *= 1. / behavior_policy(state_t)[action_t]\n",
        "\n",
        "        # Update Q-values based on the weighted returns\n",
        "        for state_action, return_t in G.items():\n",
        "            state, action = state_action\n",
        "            if state[0] <= 21:  # This condition restricts to certain states (perhaps for environment constraints)\n",
        "                G_sum[state_action] += return_t * W[state_action]  # Weighted sum of returns\n",
        "                N[state_action] += 1  # Count the visits\n",
        "                Q[state][action] = G_sum[state_action] / N[state_action]  # Update Q-value with average return\n",
        "\n",
        "        pbar.set_description(f\"G: {sum(G.values())}\")\n",
        "\n",
        "    # Derive the policy from the action-value function Q\n",
        "    policy = {}\n",
        "    for state, actions in Q.items():\n",
        "        policy[state] = torch.argmax(actions).item()  # Choose the action with the highest Q-value for each state\n",
        "\n",
        "    return Q, policy  # Return the learned action-value function and the optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6438ffe373b94a50a04ccec154fc746e",
            "827338f2d16e43259fc029ae9e7e0ca5",
            "eef099208fd54016b4d943fafa1eeaf6",
            "d7f6b9ef5f6f40fb8c064f836d61f646",
            "ebafe57d2bc84fd1b7ad944b2d7f2fe5",
            "6a111a0efa7647cda0dea8db0aab528b",
            "4245952411aa41f0936c12bf005781cb",
            "b259d84990394871bdcfa4049eda77dd",
            "33ff820ce9d7462bafd4420760f345f3",
            "a871003019f64a55afd2a17ba8bdf80a",
            "622975160c5844d39cb8c2d3f60e11be"
          ]
        },
        "id": "riOlcFho3yI0",
        "outputId": "1882b102-f1f4-4de2-9ed0-4c69faf2c356"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6438ffe373b94a50a04ccec154fc746e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Episode:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = create_env(\"Blackjack-v1\")\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "n_episode = 500\n",
        "random_policy = creat_random_policy(env.action_space.n)\n",
        "optimal_Q, optimal_policy = mc_off_policy(env, gamma, n_episode, epsilon, random_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "14cd03c9626549d7be259b1d29a12404",
            "48d23f35fcc04970a69a44a235a35eb4",
            "e496185300834a9c98a14eef6b86a5cb",
            "e13a1de7a2434c1892984176f2e21a3a",
            "64f51312fc81427eadb7e2e3c494d161",
            "fd3771dcdf554395be94f7944e093bf6",
            "a21a4e3c26ca44f0b10a069338f25f3e",
            "2b7d55969e2d4804a748553917a44974",
            "4963cba303864866b1e83131c2714f3b",
            "85ad41c9881847679b0d482e223d238a",
            "601116d588144088a78fb467c6a9c04f"
          ]
        },
        "id": "BKJiz1_43-Wx",
        "outputId": "6d4c2ca9-1e5c-4f68-839a-42b4398e7928"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14cd03c9626549d7be259b1d29a12404",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def simulate_episode(env, policy):\n",
        "     state, _ = env.reset()\n",
        "     done = False\n",
        "     while not done:\n",
        "        if state in policy:\n",
        "            action = policy[state]\n",
        "        else:\n",
        "            action = torch.randint(2, [1]).item()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            return reward\n",
        "\n",
        "win = 0\n",
        "pbar = tqdm(range(100))\n",
        "for i in pbar:\n",
        "    R = simulate_episode(env, optimal_policy)\n",
        "    if R > 0:\n",
        "        win += int(R)\n",
        "    pbar.set_description(f\"{win} times win which means {win:.2f} % winning chanse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa_UNvpN69hd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
